---
title: "Assignment4_Data621"
author: "Puja Roy, Mubashira Qari, Marco Castro, Zach Rose"
date: "2025-03-23"
format: pdf
---


```{r, include=FALSE, warning = FALSE, message = FALSE}
# Load required packages
library(htmltools)
library(caret)
library(pROC)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(broom)
library(skimr)
require(DataExplorer)
require(miscTools)
require(MASS)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 

# Load required packages
library(htmltools)
library(caret)
library(pROC)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(skimr)
require(DataExplorer)
require(miscTools)
require(MASS)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 
library(patchwork)
library(ggpubr)
library(e1071)
library(car)

```



```{r include=FALSE}

# Load datasets from GitHub
training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment4/insurance_training_data.csv")

testing_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment4/insurance-evaluation-data.csv")


```

## DATA EXPLORATION 

We checked the overview of the dataset's structure while, summary() provided statistical summaries for each variable. The dataset contains 26 columns and 8161 rows. 

```{r check-structure}
# Check the structure of the dataset
str(training_df)
```

### Preliminary Data Cleaning

In this section, we reformat variable values into numeric and factor formats. Currency values are converted from a string format to a numeric format by removing `$` and commas.


```{r clean-vars}
# Remove $ and , from currency columns
# Function to clean dollar values
clean_money <- function(x) {
  as.numeric(gsub("[$,]", "", x))
}

money_vars <- c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM", "TARGET_AMT")
factor_vars <- c("TARGET_FLAG", "SEX",  "PARENT1", "MSTATUS", "URBANICITY", "REVOKED",  "RED_CAR", "JOB", "EDUCATION", "CAR_TYPE",  "CAR_USE")
numeric_vars <- c("TARGET_AMT", "OLDCLAIM", "CLM_FREQ", "MVR_PTS", "CAR_AGE", "BLUEBOOK", "HOME_VAL", "INCOME", "YOJ",  "HOMEKIDS", "KIDSDRIV", "AGE")

# Apply to relevant columns
clean_df <-  function(df) {
  
  df <- df |>
    mutate( 
      JOB = as.factor(ifelse(is.na(JOB), "Unknown", as.character(JOB))),
      RED_CAR = if_else(RED_CAR == "yes", "Yes", "No"),
      URBANICITY = if_else(URBANICITY == "Highly Urban/ Urban", "Rural", "Urban")
    )
  
  df[money_vars] <- lapply(df[money_vars], clean_money)
  df[factor_vars] <- lapply(df[factor_vars], function(x) { as.factor(x) })
  df[numeric_vars] <- lapply(df[numeric_vars], function(x) { as.numeric(x) })
   
  return (df |>
    mutate(across(
      .cols = where(is.factor),
      .fns = ~ factor(sub("^z_", "", .))
    ))
  )
}
  
training_df <- clean_df(training_df) |>
  subset(select=-c(INDEX))
testing_df <- clean_df(testing_df) 
```

### Summary Statistics

We review the mean, standard deviation, and median for all numeric variables, helping us understand the data distribution and central tendencies.

```{r summary-stats, warning=FALSE, echo=FALSE}

cat('Summary Stats for Numeric Variables')

summary(training_df[numeric_vars])

cat('\nStandard Deviviations for Numeric Variables')
# Calculate SD for numerical variables
#summary_stats <- training_df %>% 
#  summarise(across(where(is.numeric), list(sd = sd), na.rm = TRUE))
#summary_stats[1:7]
#summary_stats[8:14]


cat('\nSummary Stats for Categorical Variables')
summary(training_df[factor_vars])
```




### Visualizing Our Data 

The first visualization is a bar chart displaying the distribution of TARGET_FLAG, helping to assess the proportion of crashes vs. non-crashes. The second visualization is a box plot for BLUEBOOK, showing the distribution and identifying potential outliers in vehicle values. TARGET_FLAG ratio of having more 0s than 1s indicates that most of the customers in the dataset did not have a crash. It's class imbalance, and it impacts the accuracy of classification models. The BLUEBOOK boxplot indicates that the variable has a strongly dominant majority of values or is long-tailed by outliers. There could be a chance that the data is non-representative, i.e., the majority of the cars have low values with few cars of high value on the tail end of the distribution.

```{r distribution-viz,  fig.width=6, fig.height=2.5, echo=FALSE}
# Bar chart of TARGET_FLAG
viz1 <-ggplot(training_df, aes(x = as.factor(TARGET_FLAG))) +
  geom_bar(fill = "blue") +
  labs(title = "Distribution of TARGET_FLAG", x = "Crash (1 = Yes, 0 = No)", y = "Count")

  
# Box plot of BLUEBOOK values
viz2 <- ggplot(training_df, aes(x = BLUEBOOK)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Box Plot of BLUEBOOK", y = "Vehicle Value")

(viz1 | viz2 )
```

\newpage
#### Visualize Distributions

Next we will use DataExplorer for visualize the distributions for our numberic variables. This gives mini-histograms for all numerical variables, so we can quickly spot skewness, check distribution and value ranges, and identify variables with spikes or unusual spread.

```{r distribution-viz-all,  fig.width=6, fig.height=6, echo=FALSE}
# Show distributions
plots <- plot_histogram(training_df |> subset(select=-c(TARGET_AMT)))
layout(1)
par(mfrow = c(1, 1))
```

\newpage

Here we focus on four variables that show skewness from their histograms that may benefit from a log transforation for when we apply a linear model. Skewness distorts relationships in regression. Log transformations may reduce skewness by compressing extreme values.


```{r skewness-plots, echo=FALSE, warning=FALSE}
# INCOME plot
p1 <- ggplot(training_df, aes(x = INCOME)) +
  geom_histogram(fill = "#2c7fb8", bins = 30) +
  labs(title = "Distribution of INCOME")

# HOME_VAL plot
p2 <- ggplot(training_df, aes(x = HOME_VAL)) +
  geom_histogram(fill = "#41ab5d", bins = 30) +
  labs(title = "Distribution of HOME_VAL")

# OLDCLAIM plot
p3 <- ggplot(training_df, aes(x = OLDCLAIM)) +
  geom_histogram(fill = "#f03b20", bins = 30) +
  labs(title = "Distribution of OLDCLAIM")

# BLUEBOOK plot
p4 <- ggplot(training_df, aes(x = BLUEBOOK)) +
  geom_histogram(fill = "#ffbf00", bins = 30) +
  labs(title = "Distribution of BLUEBOOK")

# Combine all four using patchwork
(p1 | p2) / (p3 | p4)


# Values > 1 or < -1 are highly skewed
cat("Skewness value for INCOME:", skewness(training_df$INCOME, na.rm = TRUE))
cat("Skewness value for HOME_VAL:", skewness(training_df$HOME_VAL, na.rm = TRUE)) 
cat("Skewness value for OLDCLAIM:", skewness(training_df$OLDCLAIM, na.rm = TRUE))
cat("Skewness value for BLUEBOOK:", skewness(training_df$BLUEBOOK, na.rm = TRUE))

```

\newpage

#### Visualizing Distributions for Numberical Variables

The boxplots on the following pages show the Interquartile Ranges for our numerical variables vs TARGET_FLAG. The IQR helps us visualize how similar/disimilar the ranges are between people who crashed (1) vs those who didn't crash (0) for each given parameter. The plots also gives us a sense of potential outliers for each parameter; we will want to look out for observations beyond the whiskers or address transform our data.  

---

```{r explore-boxplot, fig.width=6, fig.height=6, echo=FALSE, warning=FALSE}
plot_boxplot(training_df |> subset(select = -c(TARGET_AMT, CAR_AGE, CLM_FREQ, MVR_PTS, OLDCLAIM)), by = "TARGET_FLAG", title="Boxplots of Target vs Param", ncol = 3)
```

\newpage

Our boxplots show that there is good difference between the medians for MVR_PTS (motor vehicle record points), HOME_VAL (home value), and TIF (time in force) for crashers and non-crashes and moderate differences for INCOME, YOJ (years on Job) and CAR_AGE. CLM_FREQ (claim frequency) how high variation but this is likely due to the disproportionate number of claims from crashers versus non-crashers who may be processing other claim types outside of automotive vehicle accidents.

---


```{r explore-boxplot-p2, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}
plot_boxplot(training_df |> subset(select = c(TARGET_FLAG, CAR_AGE, CLM_FREQ, MVR_PTS, OLDCLAIM)), by = "TARGET_FLAG", title="Boxplots of Target vs Param", ncol = 2)
```


#### Visualizing Distributions for Categorical Variables

Visualizing the distributions of out categorical variables helps ensure variables are treated as discrete categories, not continuous numbers.

```{r factor-charts, fig.width=6, fig.height=6, echo=FALSE}

# Show bar charts for factors
factor_cols <- sapply(training_df, is.factor)

plots <- list()  
i <- 1

for (colname in names(training_df)[factor_cols]) {
  
  if (colname != 'TARGET_FLAG') {
    #cat("\nCounts for", colname, ":\n")
    #print(table(training_df[[colname]]))

  plot_data <- training_df %>%
    group_by(TARGET_FLAG, group_var = .data[[colname]]) %>%
    summarise(count = n(), .groups = "drop") %>%
    group_by(group_var) %>%
    mutate(
      percent = 100 * count / sum(count),
      label = paste0(round(percent), "%")
    )
  
  # Plot
  plots[[i]]  <- ggplot(plot_data, aes(x = group_var, y = count, fill = TARGET_FLAG, label = label)) +
    geom_col(position = "stack") +
    geom_text(position = position_stack(0.5), size = 3) +
    labs(
      title = paste("Distribution of", colname, "by TARGET_FLAG"),
      x = colname,
      y = "Count"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_blank(),
      axis.title.x = element_text(size = 8),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
  
    i <- i + 1
  }
}

combined <- (
  wrap_plots(plots[1:3], ncol = 3, guides = "collect") /
  wrap_plots(plots[8:10], ncol = 3, guides = "collect")
) & theme(legend.position = "bottom")

combined
```


_Key takeaways:_

PARENT1 - greater percentage of parents crashed their cars vs. non-parents (though smaler total number)

REVOKED - greater percentage of people whose license was revoked in the last 7 years crashed their cars vs. others (though smaler total number)

EDUCATION - High School graduates had the highest number of crashes among all educational groups; a higher percentage of high school graduates were also in a crash when compared to the breakdown of other groups.


JOB - Blue collar workers had the highest number of crashes among all job groups; a higher percentage of blue collar workers were also in a crash when compared to the breakdown of other groups.

CAR_USE - A similar number of commerical and private policy holders that were involved in a crash, though private policies accounted for a smaller percentage of crashes within that group.

CARTYPE - SUV's had highest total number of crashes among all vehicle types and the second highest percentage when compared to other groups


```{r factor-charts-2, fig.width=6, fig.height=6, echo=FALSE}
wrap_plots(plots[4:7], ncol = 2, guides = "collect") & 
  theme(legend.position = "bottom")
```

\newpage

### Correlation Analysis

The correlation plot provides a graphical impression of the relationship between different numerical variables of the data set. The size and color of the circles convey the strength of the relationship. Dark blue circles indicate strong positive relationships, which predict that if one variable is increasing, so will the other. Dark red circles on the other hand produce negative high correlations, where the rise of one variable is accompanied by a fall in the other. Light-colored circles show weak or no correlation.

```{r corr-analysis, echo=FALSE}
layout(1)
par(mfrow = c(1, 1))
# Compute correlation matrix for numeric variables
numeric_vars <- training_df %>% 
  select_if(is.numeric) 
corr_matrix <- cor(numeric_vars, use = "complete.obs")
corrplot(corr_matrix, method = "circle")
```

Plots suggest that TARGET_FLAG is not strongly correlated with most of the numeric features. Hence, no numeric feature is particularly useful to a customer to make a claim. What it says is perhaps claims will be predicted more accurately with a more advanced method, i.e., interactions, categorical features, or non-linear models.

Moreover, some variables such as MVR_PTS (Motor Vehicle Record Points) and CLM_FREQ (Claim Frequency) show high positive correlation. As would be expected—more holders of violations file more claims. Also, correlations such as AGE and HOMEKIDS or KIDSDRIV reflect older people with children at home, a typical demographic pattern.

Knowledge of such relationships is helpful in model building and data preparation. Correlated variables cause multicollinearity in regression models, necessitating extra work in the form of variance inflation factor (VIF) testing to eliminate redundant information. Target_FLAG correlations also imply that other transformations, engineered features, or other sources of data might be helpful in enhancing predictiveness.


### Missing Value Analysis

Missing values show that the data have missing values in relatively high counts of major variables, and most missing values exist in JOB (526 missing), CAR_AGE (510 missing), HOME_VAL (464 missing), INCOME (445 missing), YOJ (454 missing), and AGE (6 missing). Missing values may produce bias or weaken the ability of the model to predict unless appropriately handled.


```{r missing-values, echo=FALSE}
# Check for missing values
missing_values <- colSums(is.na(training_df))

missing_values

# Visualize missing data as a heatmap
plot_missing(training_df)

```

Missing values for large numbers for variables like JOB and HOME_VAL mean the work details weren't reported for certain customers or home values weren't reported. Missing values for YOJ and INCOME may be for self-employed or the ones having the wrong job history. For CAR_AGE, missing values may be for new acquisitions or lease of automobiles where the age wasn't reported.

Depending on the count of missing values, different imputation methods will be needed. Quantitative features like AGE, INCOME, and CAR_AGE can be imputed with median to remove the impact of outliers, while features like JOB may require one extra "Unknown" value. Additionally, making missing value indicators for features like HOME_VAL will enable the model to learn missing value patterns and enhance the predictive power. Closing gaps like these in an efficient way is vital to achieve model reliability and precision.




## DATA PREPARATION

### Create Flags
```{r prep-create-flags}
# Create binary flags for missing values (1 = missing, 0 = not missing)
training_df <- training_df %>%
  mutate(
    YOJ_MISSING = ifelse(is.na(YOJ), 1, 0),
    INCOME_MISSING = ifelse(is.na(INCOME), 1, 0),
    HOME_VAL_MISSING = ifelse(is.na(HOME_VAL), 1, 0),
    CAR_AGE_MISSING = ifelse(is.na(CAR_AGE), 1, 0),
    JOB_MISSING = ifelse(JOB == "Unknown", 1, 0)
  )

```

### Handling Missing Values

Missing values in key numeric columns (AGE, YOJ, INCOME, HOME_VAL, and CAR_AGE) are replaced with their respective medians. Data is preserved and no biased. We then convert YOJ, INCOME, HOME_VAL, and CAR_AGE into numeric to make it easier for managing in the list of upcoming transformations.

In addition, binary flags (YOJ_MISSING, INCOME_MISSING, HOME_VAL_MISSING, CAR_AGE_MISSING, JOB_MISSING) are built to indicate missing values in variables. This allows for models to detect missingness as a predictor.

```{r impute-missing-values}
training_df$AGE[is.na(training_df$AGE)] <- median(training_df$AGE, na.rm = TRUE)
training_df$YOJ[is.na(training_df$YOJ)] <- median(training_df$YOJ, na.rm = TRUE)
training_df$INCOME[is.na(training_df$INCOME)] <- median(training_df$INCOME, na.rm = TRUE)
training_df$HOME_VAL[is.na(training_df$HOME_VAL)] <- median(training_df$HOME_VAL, na.rm = TRUE)
training_df$CAR_AGE[is.na(training_df$CAR_AGE)] <- median(training_df$CAR_AGE, na.rm = TRUE)

colSums(is.na(training_df))
```

### Feature Engineering

Some of the new features aim to capture meaningful relationships in the data:

AVG_CLAIM: This is the feature that is created by dividing OLDCLAIM by CLM_FREQ so as to be able to interpret average payout per claim.

HIGH_RISK_CAR: Certain types of cars (e.g., "Sports Car", "Luxury SUV") are designated as high-risk by encoding them with a binary value of 1.

```{r feature-engineering-deprecate, include=FALSE}


# CLAIMS_PER_YEAR: This is the feature that is created by dividing OLDCLAIM by CAR_AGE so as to be able to interpret frequency of claims over a car's lifetime. A safety factor is included so that division by zero will not be encountered.

# TOTAL_KIDS: This is a combination of HOMEKIDS and KIDSDRIV to create a feature of total kids number of the household.
# NOTE: OLDCLAIM appears to be $ amount of the payout
# not the total number of claims
# Convert CAR_AGE and OLDCLAIM to numeric to avoid errors
training_df$CAR_AGE <- as.numeric(training_df$CAR_AGE)
training_df$OLDCLAIM <- as.numeric(training_df$OLDCLAIM)

# Ensure no division by zero when creating CLAIMS_PER_YEAR
training_df <- training_df %>%
  mutate(
    CLAIMS_PER_YEAR = ifelse(!is.na(CAR_AGE) & CAR_AGE > 0, OLDCLAIM / CAR_AGE, OLDCLAIM)
  )


# NOTE: This may double count kids.

# Convert HOMEKIDS and KIDSDRIV to numeric if they are not already
training_df$HOMEKIDS <- as.numeric(training_df$HOMEKIDS)
training_df$KIDSDRIV <- as.numeric(training_df$KIDSDRIV)

# Create a new feature: Total number of kids (home + driving)
training_df <- training_df %>%
  mutate(
    TOTAL_KIDS = ifelse(!is.na(HOMEKIDS) & !is.na(KIDSDRIV), HOMEKIDS + KIDSDRIV, NA)
  )
```

```{r feature-engineering}
  
# Convert CAR_AGE and OLDCLAIM to numeric to avoid errors
training_df$CLM_FREQ <- as.numeric(training_df$CLM_FREQ)
training_df$OLDCLAIM <- as.numeric(training_df$OLDCLAIM)

# Ensure no division by zero when creating CLAIMS_PER_YEAR
training_df <- training_df %>%
  mutate(
    AVG_CLAIM = ifelse(!is.na(CLM_FREQ) & CLM_FREQ > 0, OLDCLAIM / CAR_AGE, OLDCLAIM)
  )

# Create a binary flag for high-risk car types
training_df <- training_df %>%
  mutate(
    HIGH_RISK_CAR = ifelse(CAR_TYPE %in% c("Sports Car", "Luxury SUV"), 1, 0)
  )

glimpse(training_df$AVG_CLAIM)
```



```{r new-features-charts, fig.width=6, fig.height=2, echo=FALSE}

# Show bar charts for factors
new_factor_cols <- c('HIGH_RISK_CAR')


en_plots1 <- ggplot(training_df, aes(x = AVG_CLAIM)) +
  geom_histogram()


plot_data <- training_df %>%
  group_by(TARGET_FLAG, group_var = .data[['HIGH_RISK_CAR']]) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(group_var) %>%
  mutate(
    percent = 100 * count / sum(count),
    label = paste0(round(percent), "%")
  )

en_plots2  <- ggplot(plot_data, aes(x = group_var, y = count, fill = TARGET_FLAG, label = label)) +
    geom_col(position = "stack") +
    geom_text(position = position_stack(0.5), size = 3) +
    labs(
      title = paste("Distribution of HIGH_RISK_CAR by TARGET_FLAG"),
      x = colname,
      y = "Count"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_blank(),
      axis.title.x = element_text(size = 8),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
  
 
(en_plots1 | en_plots2)
```


### Transform data

Skewed numerical attributes are log-transformed to stabilize their distributions:

LOG_BLUEBOOK (log-transformed value of the vehicle),

LOG_OLDCLAIM (log-transformed old claims),

LOG_INCOME (log-transformed income).
NA values are substituted with a small positive constant (0.01) before applying the log transformation to prevent computation errors.

LOG_HOME_VAL (log-transformed home value)

```{r log-transform}
# Handle NA values before log transformation
training_df <- training_df %>%
  mutate(
    BLUEBOOK = ifelse(is.na(BLUEBOOK), 0.01, BLUEBOOK),
    OLDCLAIM = ifelse(is.na(OLDCLAIM), 0.01, OLDCLAIM),
    INCOME = ifelse(is.na(INCOME), 0.01, INCOME),
    HOME_VAL = ifelse(is.na(INCOME), 0.01, HOME_VAL),
    LOG_BLUEBOOK = log1p(BLUEBOOK),
    LOG_OLDCLAIM = log1p(OLDCLAIM),
    LOG_INCOME = log1p(INCOME),
    LOG_HOME_VAL = log1p(HOME_VAL)
  )

```


#### Compare Before and After Transformation

The charts below show the distributions for the values of the INCOME, HOME_VAL, OLDCLAIM, and BLUEBOOK parameters before and after applying a log transformation.

```{r compare-log-transformations,  fig.width=6, fig.height=5, echo=FALSE}
# Set layout to plot side-by-side
par(mfrow = c(2, 2))

# INCOME
hist(training_df$INCOME, 
     main = "Original INCOME", 
     col = "salmon", 
     xlab = "training_df$INCOME")

hist(training_df$LOG_INCOME, 
     main = "Log Transformed INCOME", 
     col = "seagreen", 
     xlab = "training_df$LOG_INCOME")


hist(training_df$HOME_VAL, 
     main = "Original HOME_VAL", 
     col = "tomato", 
     xlab = "training_df$HOME_VAL")

hist(training_df$LOG_HOME_VAL, 
     main = "Log Transformed HOME_VAL", 
     col = "forestgreen", 
     xlab = "training_df$LOG_HOME_VAL")



```

```{r  compare-log-transformations-2, fig.width=6, fig.height=5, echo=FALSE}

# BLUEBOOK
par(mfrow = c(2, 2))

hist(training_df$OLDCLAIM, 
     main = "Original OLDCLAIM", 
     col = "lightskyblue", 
     xlab = "training_df$OLDCLAIM")

hist(training_df$LOG_OLDCLAIM, 
     main = "Log Transformed OLDCLAIM", 
     col = "darkgreen", 
     xlab = "training_df$LOG_OLDCLAIM")

hist(training_df$BLUEBOOK, 
     main = "Original BLUEBOOK", 
     col = "khaki3", 
     xlab = "training_df$BLUEBOOK")

hist(training_df$LOG_BLUEBOOK, 
     main = "Log Transformed BLUEBOOK", 
     col = "darkolivegreen4", 
     xlab = "training_df$LOG_BLUEBOOK")

```

#### Check Variable Correlations (Multicollinearity Insight)

This updated Correlation Matrix shows correlation values for all numerical values including our log transformed values and our new variables. The matrix below shows that there is high correlation between LOG_OLDCLAIM and CLM_FREQ (Claim frequency) and LOG_INCOME and YOJ (Years on Job). There is moderate correlation between our HOMEKIDS, KIDSDRV (Kids driving) and AGE.


```{r correlation-2, fig.width=4, fig.height=4, echo=FALSE}
# Select only numeric columns
numeric_vars <- training_df %>% 
  dplyr::select(where(is.numeric))  %>% 
  subset(select = -c(AVG_CLAIM, YOJ_MISSING, INCOME_MISSING, HOME_VAL_MISSING, CAR_AGE_MISSING, JOB_MISSING)) %>% 
  subset(select = -c(INCOME, OLDCLAIM, HOME_VAL, BLUEBOOK))

# Remove rows with NAs
# numeric_vars <- na.omit(numeric_vars) 

# Compute correlation matrix
cor_matrix <- cor(numeric_vars)

# Visualize correlation matrix
layout(1)
par(mfrow = c(1, 1))
corrplot(cor_matrix, method = "color", type = "lower", tl.cex = 0.8)

```



### Categorical Groupings

Categorical groupings are created to reduce numerical variables:
AGE_GROUP: Four age groups—Young (≤25), Adult (26-40), Middle-Aged (41-60), and Senior (>60)—categorize individuals.

INCOME_GROUP: Income is divided into four quantile-based groups (Low, Medium, High, Very High) such that there is an equal share of income levels.

```{r bin-age}
# Transform data by putting it into buckets using case_when
training_df <- training_df %>%
  mutate(
    AGE_GROUP = case_when(
      AGE <= 25 ~ "Young",
      AGE > 25 & AGE <= 40 ~ "Adult",
      AGE > 40 & AGE <= 60 ~ "Middle Aged",
      AGE > 60 ~ "Senior",
      TRUE ~ NA_character_  # Handle missing values
    )
  )

```

```{r bin-income}
# Fix Income Grouping - Ensure proper quantile calculation
income_quantiles <- quantile(training_df$INCOME, probs = seq(0, 1, 0.25), na.rm = TRUE)

training_df <- training_df %>%
  mutate(
    INCOME_GROUP = case_when(
      INCOME <= income_quantiles[2] ~ "Low",
      INCOME > income_quantiles[2] & INCOME <= income_quantiles[3] ~ "Medium",
      INCOME > income_quantiles[3] & INCOME <= income_quantiles[4] ~ "High",
      INCOME > income_quantiles[4] ~ "Very High",
      TRUE ~ NA_character_
    )
  )
```


```{r new-factor-charts, fig.width=6, fig.height=3, echo=FALSE}

# Show bar charts for factors
new_factor_cols <- c('AGE_GROUP', 'INCOME_GROUP')

plots <- list()  
i <- 1

for (colname in new_factor_cols) {
  
  if (colname != 'TARGET_FLAG') {
    #cat("\nCounts for", colname, ":\n")
    #print(table(training_df[[colname]]))

  plot_data <- training_df %>%
    group_by(TARGET_FLAG, group_var = .data[[colname]]) %>%
    summarise(count = n(), .groups = "drop") %>%
    group_by(group_var) %>%
    mutate(
      percent = 100 * count / sum(count),
      label = paste0(round(percent), "%")
    )
  
  # Plot
  plots[[i]]  <- ggplot(plot_data, aes(x = group_var, y = count, fill = TARGET_FLAG, label = label)) +
    geom_col(position = "stack") +
    geom_text(position = position_stack(0.5), size = 3) +
    labs(
      title = paste("Distribution of", colname, "by TARGET_FLAG"),
      x = colname,
      y = "Count"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_blank(),
      axis.title.x = element_text(size = 8),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
  
    i <- i + 1
  }
}

wrap_plots(plots, ncol = 2, guides = "collect") & theme(legend.position = "bottom")

```

### Creating New Features:

To capture relationships between features more effectively, new ratio-based features are formed:

CLAIMS_INCOME_RATIO: Indicates the proportion of old claims relative to income.

VEHICLE_VALUE_TO_INCOME: Indicates how cheap the vehicle is relative to income.

CLAIMS_TO_MVR_PTS: Prior claims quantity per vehicle record to indicate the severity of prior offenses.

Also an estimated risk score by averaging important risk indicators (MVR_PTS, CLM_FREQ, HIGH_RISK_CAR, and REVOKED). The risk score forms a composite measure of the subject's risk level.

```{r new-features, warning=FALSE}

# Ensure no division by zero when creating ratios
training_df <- training_df %>%
  mutate(
    CLAIMS_INCOME_RATIO = ifelse(INCOME > 0, OLDCLAIM / INCOME, NA),
    VEHICLE_VALUE_TO_INCOME = ifelse(INCOME > 0, BLUEBOOK / INCOME, NA),
    CLAIMS_TO_MVR_PTS = ifelse(MVR_PTS > 0, OLDCLAIM / MVR_PTS, NA)
  )

# Create a risk score by combining various indicators
training_df <- training_df %>%
  mutate(
    RISK_SCORE = (as.numeric(MVR_PTS) * 0.4) + (as.numeric(CLM_FREQ) * 0.3) + (as.numeric(HIGH_RISK_CAR) * 0.2) + (as.numeric(REVOKED) * 0.1)
  )

```

#### Checking for Outliers

We will fit a base model that includes all 23 original variables without any transformations. We will use this model to check for outliers using the Cook's Distance test. Below we see the output of the 10 observations with the highest Cook's Distance values.

```{r base_model, echo=FALSE}

model_full <- glm(TARGET_FLAG ~ AGE + BLUEBOOK + CAR_AGE + CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + HOMEKIDS + HOME_VAL + INCOME + JOB + KIDSDRIV  + MSTATUS + MVR_PTS + OLDCLAIM + PARENT1  + RED_CAR + REVOKED + SEX + TIF + TRAVTIME + URBANICITY + YOJ,
  data = training_df,
  family = binomial
)

potential_outliers <- augment(model_full) |>
  mutate(
    index = 1:n(),
    TARGET_FLAG = if_else(TARGET_FLAG == 0, 'Yes', 'No')
  ) |>
  relocate(index, .before=TARGET_FLAG) |>
  top_n(10, .cooksd) |>
  arrange(desc(.cooksd))

glimpse(potential_outliers |> subset(select=c(index, .fitted, .resid, .hat, .sigma, .cooksd, .std.resid)))
```

The diagnostic plots on the following page show that point 5101 stands out in our Residuals vs. Fitted and Scale Location plots. However, when take a closer look at some our other diagnostic plots and  we don't see this point within the 10 highest Cook's distance values. Our Cook's distance values and respective plot shows that none of our points is an extreme outlier. Examining our Cook's distance vs Leverage plot doesn't present evidence of the presence of outliers with high leverage.


```{r base_model-diagnostic-plots, fig.width=6, fig.height=8, echo=FALSE}
layout(matrix(1:6, nrow = 3, byrow = TRUE))
plot(model_full, which = c(4, 6, 1, 2, 3, 5), col=training_df$TARGET_FLAG,  id.n = 5)
par(mfrow = c(1, 1))

```



## MODEL BUILDING

### Binary Logistic Regression Models for TARGET_FLAG 

We use glm() with family = binomial for logistic regression. This model predicts the probability of a crash (1). We are predicting TARGET_FLAG (binary: 1 = made a claim, 0 = no claim).


#### Model 1: Log Transformed Model

The log transformed model includes 19 our original variables and uses four log transformed variables: LOG_BLUEBOOK, LOG_HOME_VAL, LOG_INCOME, LOG_OLDCLAIM.

```{r log_transformed_model, echo=FALSE}

model1 <- glm(TARGET_FLAG ~ AGE + LOG_BLUEBOOK + CAR_AGE + CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + HOMEKIDS + LOG_HOME_VAL + LOG_INCOME + JOB + KIDSDRIV  + MSTATUS + MVR_PTS + LOG_OLDCLAIM + PARENT1  + RED_CAR + REVOKED + SEX + TIF + TRAVTIME + URBANICITY + YOJ,
  data = training_df,
  family = binomial
)

summary(model1)
```

\newpage
##### Diagnostics Plots

```{r model_1_diagnostics, fig.width=6, fig.height=7, echo=FALSE}
layout(matrix(1:6, nrow = 3, byrow = TRUE))
plot(model1, col=training_df$TARGET_FLAG,  id.n = 5)
par(mfrow = c(1, 1))

```

#### Model 2: Stepwise Selected Log Transformed Model

We applied backward selection to the Log Transformed Model (model 1) to obtain a simplifled model that removes the majority of the variables with little statistical significance. while trying to find a low AIC value. The AIC value for model 2 is 7357.7 while our original log transformed model had an AIC value of 7364.97. AIC is just one measure that we will use to evaluate and select our final model in section 4 of this work, but it is worth noting here as a preliminary point of comparison as we build our models.

```{r stepwise-model, echo=FALSE}

# Logistic Regression Model 2 (Stepwise Selection)
model2 <- stepAIC(glm(TARGET_FLAG ~ AGE + LOG_BLUEBOOK + CAR_AGE + CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + HOMEKIDS + LOG_HOME_VAL + LOG_INCOME + JOB + KIDSDRIV  + MSTATUS + MVR_PTS + LOG_OLDCLAIM + PARENT1  + RED_CAR + REVOKED + SEX + TIF + TRAVTIME + URBANICITY + YOJ,
  data = training_df,
  family = binomial), direction="backward")

summary(model2)

AIC(model2)  # Compare AIC

```

#### Model 3: Using Categorical Groupings

For model 3, we substituted AGE and INCOME with our categorically grouped variables AGE_GROUP and INCOME_GROUP and used all of the other predictors from our Log Transformed model. We again used backward selection to obtaina simplified model. This model yields an AIC value of 7327.1

```{r model3, echo=FALSE}
#

model3 <- stepAIC(glm(TARGET_FLAG ~ AGE_GROUP + LOG_BLUEBOOK + CAR_AGE + CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + HOMEKIDS + LOG_HOME_VAL + INCOME_GROUP + JOB + KIDSDRIV  + MSTATUS + MVR_PTS + LOG_OLDCLAIM + PARENT1  + RED_CAR + REVOKED + SEX + TIF + TRAVTIME + URBANICITY + YOJ,
  data = training_df,
  family = binomial), direction="backward")

summary(model3)
AIC(model3) 
```

#### Model 4: Using One-Hot Encoded Parameters

Building off of the predictors we obtained in model 3, uses one-hot encoded predictors for AGE_GROUP, INCOME_GROUP, EDUCATION, and JOB to remove predictors with little statistical significance from our model. Our AIC value has increased a bit to 7354.6 but our model is slightly simpler.


```{r one-hot-encoded-model, echo=FALSE}

# one-hot encode values
age_one_hot <- model.matrix(~ AGE_GROUP - 1, data = training_df)
income_one_hot <- model.matrix(~ INCOME_GROUP - 1, data = training_df)
education_one_hot <- model.matrix(~ EDUCATION - 1, data = training_df)
job_one_hot <- model.matrix(~ JOB - 1, data = training_df)


df_training_one_hot <- cbind(training_df[ , !names(training_df) %in% "AGE_GROUP"], age_one_hot) 

df_training_one_hot <-
  cbind(df_training_one_hot[ , !names(df_training_one_hot) %in% "INCOME_GROUP"], income_one_hot) 

df_training_one_hot <-
  cbind(df_training_one_hot[ , !names(df_training_one_hot) %in% "EDUCATION"], education_one_hot) 

df_training_one_hot <-
  cbind(df_training_one_hot[ , !names(df_training_one_hot) %in% "JOB"], job_one_hot) 
 

# Adapted from stepwise backwards selection
model4 <- glm(TARGET_FLAG ~ `AGE_GROUPMiddle Aged` + LOG_BLUEBOOK + CAR_AGE + CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATIONBachelors  + LOG_HOME_VAL + `INCOME_GROUPVery High` + JOBManager + KIDSDRIV  + MSTATUS + MVR_PTS + LOG_OLDCLAIM + PARENT1 + REVOKED + TIF + TRAVTIME + URBANICITY + YOJ,
  data = df_training_one_hot,
  family = binomial)

summary(model4)
AIC(model4) 

```


#### Model 5: Using Selected Features

This model uses a selection of features based on our exploratory charts. In particular, categorical demographic information was selected based on the single group with the highest numerical number of crashes. Additionally, we have added LOG_HOME_VAL, TIF, URBANCITY, HIGH_RISK_CAR, REVOKED and an interaction term between CLM_FREQ and MVR_PTS. The latter four parameters are used in our weighted RISK_SCORE param, but replacing these paramters with the former results in a higher AIC value.


```{r model5, echo=FALSE}

model5 <- glm(TARGET_FLAG ~ HIGH_RISK_CAR + CLM_FREQ * MVR_PTS  + REVOKED + LOG_HOME_VAL   + TIF + URBANICITY + `AGE_GROUPMiddle Aged` + `INCOME_GROUPLow` + `EDUCATIONHigh School` + `JOBBlue Collar` + PARENT1,
  data = df_training_one_hot,
  family = binomial)

summary(model5)
AIC(model5) 

```

#### Model : MQ's Logit model

```{r logit_model, echo=FALSE}
# Clone dfs and rename to match 
# Mubashira's Work
insurance_training_clean <- training_df %>%
  rename(
    INCOME_LOG = LOG_INCOME,
    HOME_VAL_LOG = LOG_HOME_VAL,
    OLDCLAIM_LOG = LOG_OLDCLAIM,
    BLUEBOOK_LOG = LOG_BLUEBOOK
  )

insurance_evaluation_df <- testing_df 

logit_model <- glm(TARGET_FLAG ~ CAR_TYPE + CAR_USE + EDUCATION + INCOME_LOG + JOB + KIDSDRIV  + MSTATUS + MVR_PTS + OLDCLAIM_LOG + PARENT1  + REVOKED  + TIF + TRAVTIME + URBANICITY + YOJ,
  data = insurance_training_clean,
  family = binomial
)

summary(logit_model)

```


Interpreting the Output: Significant p-values (\< 0.05) indicate variables that are strong predictors of making a claim.

##### Model Significance with Deviance Test

```{r}
null_dev <- logit_model$null.deviance
resid_dev <- logit_model$deviance
df_diff <- logit_model$df.null - logit_model$df.residual
p_val <- 1 - pchisq(null_dev - resid_dev, df_diff)

cat("Model significance p-value:", p_val)

# Check each variable
anova(logit_model, test = "Chisq")

```

##### Run Assumption Tests Using Residuals

##### Assumption Testing – Logistic

##### Multicollinearity: Variance Inflation Factor

```{r}
vif(logit_model)

```

Explanation: VIF \> 5 or 10 indicates multicollinearity. Remove or combine correlated variables.

##### ROC Curve + AUC

```{r mq-roc}
# Step 1: Load the library
library(pROC)

# Step 2: Generate predicted probabilities from your logistic model
pred_probs <- predict(logit_model, newdata = insurance_training_clean, type = "response")

# Step 3: Create the ROC object
roc_obj <- roc(response = insurance_training_clean$TARGET_FLAG,
               predictor = pred_probs)

# Step 4: Plot the ROC Curve
plot(roc_obj, main = "ROC Curve")

# Step 5: Get the AUC
pROC::auc(roc_obj)


identical(length(pred_probs), length(insurance_training_clean$TARGET_FLAG))  # Should return TRUE

```

Explanation: Measures model's ability to distinguish crashers vs. non-crashers. AUC closer to 1 = better.

##### Confusion Matrix

```{r mq-confusion-matrix}
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
confusionMatrix(as.factor(pred_class), as.factor(insurance_training_clean$TARGET_FLAG))

```

Explanation: Helps evaluate accuracy, sensitivity, specificity using 0.5 as threshold.

#### Crash Model comparison
```{r model-comp-1}

library(vcdExtra)
library(pscl)

stats <- LRstats(model1, model2, model3, model4, model5, logit_model)

stats$McFaddenR2 <- NA 
stats$Accuracy <- NA 
stats$Precision <- NA 
#stats$Recall <- NA 
stats$Sensitivity <- NA 
stats$Specificity <- NA 
stats$F1_score <- NA 
stats$AUC <- NA 
stats$CV_est_predict_err <- NA 
stats$CV_adj_est <- NA 

enhanceEvaluationMetrics <- function(df, model_name) {
  model <- get(model_name)
  
  if (model_name == 'logit_model') {
    model_data <- insurance_training_clean
  } else if (model_name == 'model4') {
    model_data <- df_training_one_hot
  } else  {
    model_data <- training_df
  }
  
  df[model_name, "McFaddenR2"] <- pR2(model)["McFadden"]
  #pred_probs <- predict(model, type = "response")
  #pred_probs_factor <- as.factor(ifelse(pred_probs > 0.5, 1, 0))
  #conf_matrix <- confusionMatrix(pred_probs_factor, as.factor(model_data$TARGET_FLAG))
  #df[model_name, "Accuracy"] <- conf_matrix$overall['Accuracy']
  #df[model_name, "Precision"] <- conf_matrix$byClass['Precision']
  #df[model_name, "Recall"] <- conf_matrix$byClass['Recall']
  #df[model_name, "F1_score"] <- conf_matrix$byClass['F1']
  #df[model_name, "Sensitivity"] <- conf_matrix$byClass["Sensitivity"] 
  #df[model_name, "Specificity"] <- conf_matrix$byClass["Specificity"]

  #df[model_name, "AUC"] <- MLmetrics::AUC(y_true = model_data$target, y_pred = pred_probs) 
  
  # Cross-Validation using 10 folsds
  cv_result <- boot::cv.glm(model_data, model, K= 10)
  df[model_name, "CV_est_predict_err"] <- cv_result$delta[1]
  df[model_name, "CV_adj_est"] <- cv_result$delta[2]
  
  return(df) 
}


# Loop through the list of models and update the dataframe for each
for (model_name in rownames(stats)) {
  
 #stats <- enhanceEvaluationMetrics(stats, model_name)
}

stats 
```

### Models for TARGET_AMT

#### Subset of Crashers

Since payouts can't happen without a crash event, we will create a subset where TARGET_FLAG = 1 to avoid biasing our payout predictions towards zero. We will also look at the distribution and review our IQR values. We see that the payouts are highly skewed to the right with a long tail to the right. The median payout is $4,104 and the maximum payout is $107,586. Applying a log transformation to TARGET_AMT gives a normal distribution.

```{r subset_claims_only, echo=FALSE}
claims_only <- insurance_training_clean %>% 
  filter(TARGET_FLAG == 1) %>%
  subset(select = -c(TARGET_FLAG, YOJ_MISSING, INCOME_MISSING, HOME_VAL_MISSING, CAR_AGE_MISSING, JOB_MISSING, CAR_AGE_MISSING, HIGH_RISK_CAR, RISK_SCORE, TOTAL_KIDS)) %>%
  mutate(
    TARGET_AMT_LOG = log1p(TARGET_AMT)
  )

# Histogram of TARGET_AMT
p3 <- ggplot(claims_only, aes(x = TARGET_AMT)) +
  geom_histogram(bins = 25)

p4 <- ggplot(claims_only, aes(x = TARGET_AMT_LOG)) +
  geom_histogram(bins = 25)

summary(claims_only$TARGET_AMT)
(p3 | p4)
```


#### Payout Model 1

Since Blue Book values are used to determine the market rate value of a used car, we can assume that it will affect payout amount. Blue Book values are derived from various attributes, including car age, type, mileage and condition. We will use the logged transformed variable BLUEBOOK_LOG since we had identified that the distribution for original BLUEBOOK was right-skewed.


```{r amt_model1, echo=FALSE}

amt_model1 <- lm(TARGET_AMT ~ BLUEBOOK_LOG, claims_only)
summary(amt_model1)
AIC(amt_model1)
```

Our model summary indicates that BLUEBOOK_LOG is statistically significant.

##### Diagnostic Plots

Out diagnostic plots show that our model fails our assumptions for linear regression. In particular, our Residuals vs. Fitted plot shows possible funneling in the positive values, suggesting that our model fails the linearity assumption. Our Q-Q plot shows clear high skewing on the right tail, suggesting that our model fails the normality assumption. The  Scale-Location plot does not show evenly scattered points, suggesting we fail our assumption of Homoscedasticity. Our Residual vs. Leverage plot shows some extreme values but they don't appear to have high leverage. A Durbin-Watson value of 1.9844 suggests that our model meets our independence assumption.


```{r diagnose_amt_model1, echo=FALSE}
par(mfrow = c(2,2))
plot(amt_model1)

#library(lmtest)
#dwtest(amt_model1)
```

#### Payout Model 2

As our Target Amount value is highly skewed to the right, we will update our model with a log transformed dependent variable (TARGET_AMT_LOG). 
 
```{r amt_model2, echo=FALSE}

amt_model2 <- lm(TARGET_AMT_LOG ~ BLUEBOOK_LOG, claims_only)
summary(amt_model2)

par(mfrow = c(2,2))
plot(amt_model2)
dwtest(amt_model2)
bptest(amt_model2)
shapiro.test(residuals(amt_model2))
AIC(amt_model2, amt_model1)
```

Using the log transformed dependent variable improves our diagnostic plots. Our Residuals vs. Fitted plot shows points fairly evenly scattered around the y-axis, suggesting that our model meets the linearity assumption. The Scale-Location plot shows a fairly even cloud of points around the horizontal line; while the points above the line have a greater spread than those below, the plot suggests that our model is mostly homoscedastic. A Breusch-Pagan test shows that there is evidence of constant variance, supporting that we meet homoscedasticity assumption. A Durbin-Watson value of 1.9844 suggests that our model meets our independence assumption. 

 Our Q-Q plot shows skewing at the tails suggesting that our model may not meet our normality assumption. A Shapiro-Wilk normality test with a very low p-value supports this assessment. However, this violation may be acceptable, as we know that our payout values will be right skewed due to lower payouts for lesser accidents such as minor "fender-benders;" there will be some edge cases with higher payouts for accidents with greater damage, such as those involving multiple-vehicles or personal injuries. This is evident in the Residual vs. Leverage plot which shows the presence of extreme values although they don't appear to have high leverage.


#### Payout Model 3

This model introduces MVR_PTS (Motor Vehicle Record Points) as a predictor under the assumption that drivers with MVR_PTS may be involved in accidents resulting in greater damage or may face a greater share of the financial liability if insurance companies have policies that restrict payout amounts for drivers with higher MVR_PTS. Our corresponding diagnostic plots show similar results as amt_model2.

```{r amt_model3, echo=FALSE}

amt_model3 <- lm(TARGET_AMT_LOG ~ BLUEBOOK_LOG + MVR_PTS, claims_only)
summary(amt_model3)
par(mfrow = c(2,2))
plot(amt_model3)
AIC(amt_model3, amt_model2)
```


#### Payout Model 4

This Model introduces CAR_TYPE as an interaction predictor for BLUEBOOK_LOG to identify if type of car impacts payout amount. Our corresponding diagnostic plots show similar results as amt_model2.

```{r amt_model4, echo=FALSE}

amt_model4 <- lm(TARGET_AMT_LOG ~ BLUEBOOK_LOG * CAR_TYPE + MVR_PTS, claims_only)
summary(amt_model4)
par(mfrow = c(2,2))
plot(amt_model4)
AIC(amt_model4, amt_model3)
```

#### Payout Model 5: Gamma GLM

Given that our dependent payout variable TARGET_AMT is highly right skewed and that out OLS models fail the linearity assumption, we can attempt a Gamma GLM with a Log link. This will also mean that we won't need to retransform the predicted payout amounts if we use a Gamma GLM model as our selected model.


```{r gamma-model1, fig.width=6, fig.height=8, echo=FALSE}

library(boot)
library(faraway)

amt_model5 <- glm(TARGET_AMT ~ BLUEBOOK_LOG,
                     family = Gamma(link = "log"),
                     data = claims_only)
summary(amt_model5)


par(mfrow = c(3,2))
#glm.diag.plots(amt_model5, glm.diag(amt_model5))
plot(amt_model5, which = c(1,3,4,5))
plot(residuals(amt_model5, type = "deviance"))
halfnorm(residuals(amt_model5, type = "deviance"))
abline(0, 1, col = "red", lty = 2) 
AIC(amt_model5, amt_model2)
```


Our Residual vs fitted plot shows some funneling suggesting that our data is heteroschedastic; however, this may be acceptable for our payout predictions when using GLM's. Our Scale-Location plot shows a roughly horizontal plot. Additionally, three observations are moderately influential (2038, 2063, and 1430). Our Residuals vs Leverage highlights the same points but they don't appear to exert much leverage on our model. Our deviance residuals show a random scatter around the 0, which suggests the gamma model is a good fit for our data. Our half-normal plot of the absolute deviance residuals shows points generally following the slope line, though we do see flaring on the right tail and note points 2038 and 1430 deviating from the line.

```{r echo=FALSE}

influence <- influence.measures(amt_model5)
cooks_d <- influence$infmat[, "cook.d"]
print(claims_only[which.max(cooks_d), ])
```

Looking closer at point 2038 shows that the policy holder had unusually high payout amount of 107586. However, it is not possible to tell if this is due to a data entry error or is representative of a true payout amount for an accident with greater damage.



#### MQ: Fit Linear Model

```{r}
lm_model <- lm(
  TARGET_AMT ~ KIDSDRIV + YOJ + INCOME_LOG + PARENT1 +
    MSTATUS + EDUCATION + JOB + TRAVTIME + CAR_USE + TIF +
    CAR_TYPE + OLDCLAIM_LOG + REVOKED + MVR_PTS + URBANICITY + BLUEBOOK_LOG,
  data = claims_only
)
summary(lm_model)

```

##### Assumption Testing – Linear

##### Linearity & Residual Plots

Explanation: p \> 0.05 = residuals are normally distributed (good!)

```{r}

par(mfrow = c(2,2))
plot(lm_model)

```

Explanation:

Residuals vs. Fitted: should show no pattern (linearity)

Q-Q Plot: points should fall along diagonal (normality)

##### Normality of Residuals

```{r}
shapiro.test(residuals(lm_model))

```

##### Multicollinearity

```{r}

vif(lm_model)

```




#### Payout Model comparison
```{r full-model-lrstats}

library(vcdExtra)
library(pscl)

stats <- LRstats(amt_model1, amt_model2, amt_model3, amt_model4, amt_model5, lm_model)

stats$McFaddenR2 <- NA 
stats$CV_est_predict_err <- NA 
stats$CV_adj_est <- NA 

enhanceEvaluationMetrics <- function(df, model_name) {
  model <- get(model_name)
  model_data <- claims_only
  print(model_name)
  head(model)
  
  df[model_name, "McFaddenR2"] <- pR2(model)["McFadden"]
  # Cross-Validation using 10 folsds
  cv_result <- boot::cv.glm(model_data, model, K= 10)
  df[model_name, "CV_est_predict_err"] <- cv_result$delta[1]
  df[model_name, "CV_adj_est"] <- cv_result$delta[2]
  
  return(df) 
}


# Loop through the list of models and update the dataframe for each
for (model_name in rownames(stats)) {
  
  #stats <- enhanceEvaluationMetrics(stats, model_name)
}

stats 
```

## MODEL SELECTION


### Make Predictions on Evaluation Data

```{r}
# Apply same transformations to test dataset
testing_df <- testing_df %>%
  mutate(
    BLUEBOOK = ifelse(is.na(BLUEBOOK), 0.01, BLUEBOOK),
    OLDCLAIM = ifelse(is.na(OLDCLAIM), 0.01, OLDCLAIM),
    INCOME = ifelse(is.na(INCOME), 0.01, INCOME),
    HOME_VAL = ifelse(is.na(INCOME), 0.01, HOME_VAL),
    LOG_BLUEBOOK = log1p(BLUEBOOK),
    LOG_OLDCLAIM = log1p(OLDCLAIM),
    LOG_INCOME = log1p(INCOME),
    LOG_HOME_VAL = log1p(HOME_VAL)
  )


insurance_evaluation_df <- testing_df  %>%
  rename(
    INCOME_LOG = LOG_INCOME,
    HOME_VAL_LOG = LOG_HOME_VAL,
    OLDCLAIM_LOG = LOG_OLDCLAIM,
    BLUEBOOK_LOG = LOG_BLUEBOOK
  )

```

Predict Crash Probability & Classification


```{r}
eval_probs <- predict(logit_model, newdata = insurance_evaluation_df, type = "response")
eval_flag_pred <- ifelse(eval_probs > 0.5, 1, 0)

```

### Logistic Regression Model Comparison

```{r}
model_list <- list(
  model1 = model1,
  model2 = model2,
  model3 = model3,
  model4 = model4,
  model5 = model5,
  logit_model = logit_model
)
```

```{r}
logit_eval_df <- data.frame(
  Model = character(),
  AIC = numeric(),
  McFaddenR2 = numeric(),
  Accuracy = numeric(),
  Precision = numeric(),
  Recall = numeric(),
  Specificity = numeric(),
  F1 = numeric(),
  AUC = numeric(),
  stringsAsFactors = FALSE
)

for (model_name in names(model_list)) {
  model <- model_list[[model_name]]
  
if (model_name == "logit_model") {
  df <- insurance_training_clean
} else if (model_name %in% c("model4", "model5")) {
  df <- df_training_one_hot
} else {
  df <- training_df
}
  
  probs <- predict(model, newdata = df, type = "response")
  preds <- ifelse(probs > 0.5, 1, 0)
  
  cm <- confusionMatrix(as.factor(preds), as.factor(df$TARGET_FLAG))
  
  roc_obj <- roc(df$TARGET_FLAG, probs)
  auc_val <- as.numeric(roc_obj$auc)
  
  logit_eval_df <- rbind(logit_eval_df, data.frame(
    Model = model_name,
    AIC = AIC(model),
    McFaddenR2 = pR2(model)["McFadden"],
    Accuracy = cm$overall["Accuracy"],
    Precision = cm$byClass["Precision"],
    Recall = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    F1 = cm$byClass["F1"],
    AUC = auc_val
  ))
}

knitr::kable(logit_eval_df, caption = "Logistic Regression Model Comparison")
```

After evaluating six logistic regression models, we selected model3 as the best binary classification model for predicting TARGET_FLAG. This model achieved the highest AUC (0.816) and highest accuracy (79.3%), while also maintaining a strong balance between precision (0.818) and recall (0.925). It demonstrated the highest McFadden R² (0.2299) and the lowest AIC (7327.13) among all models evaluated. Although model4 had a slightly lower AIC, its predictive metrics were marginally weaker, so model3 was prioritized for its superior performance and interpretability. For predicting TARGET_AMT, we compared six regression models. These included both standard linear models and a gamma GLM. We found that amt_model3, which includes BLUEBOOK_LOG and MVR_PTS as predictors for a log-transformed payout amount (TARGET_AMT_LOG), was the strongest overall. It achieved an AIC of 5179.6, the lowest among all linear models, and showed improved explanatory power over simpler versions. Both predictors were statistically significant, and diagnostic plots indicated acceptable performance across key assumptions. While the gamma GLM (amt_model5) handled skewness well, it had a much higher AIC (41282), and thus was not chosen.

Overall, model3 for classification and amt_model3 for payout prediction provide the most reliable, interpretable, and statistically sound results. These models are recommended for deployment, with model3 used to first classify crashers, and amt_model3 used to estimate payout amounts for those predicted to crash.



#### ROC Curve Comparison

```{r}
# Compare ROC curves visually
plot(roc(training_df$TARGET_FLAG, predict(model1, type="response")), col = "blue", main = "ROC Curves for Logistic Models")
lines(roc(training_df$TARGET_FLAG, predict(model2, type="response")), col = "green")
lines(roc(training_df$TARGET_FLAG, predict(model3, type="response")), col = "orange")
lines(roc(df_training_one_hot$TARGET_FLAG, predict(model4, type="response")), col = "purple")
lines(roc(training_df$TARGET_FLAG, predict(model5, type="response")), col = "red")
lines(roc(insurance_training_clean$TARGET_FLAG, predict(logit_model, type="response")), col = "black")
legend("bottomright", legend = c("model1", "model2", "model3", "model4", "model5", "logit_model"),
       col = c("blue", "green", "orange", "purple", "red", "black"), lty = 1, cex = 0.8)

```


### Predict Cost for Crashers Only

```{
#eval_amt_pred <- rep(0, nrow(eval))
#eval_crash_idx <- which(eval_flag_pred == 1)
#eval_amt_pred[eval_crash_idx] <- predict(lm_model, newdata = #insurance_evaluation_df[eval_crash_idx, ])

```

### Final Combined Output
AGE_GROUP + LOG_BLUEBOOK + CAR_AGE + CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + HOMEKIDS + LOG_HOME_VAL + INCOME_GROUP + JOB + KIDSDRIV  + MSTATUS + MVR_PTS + LOG_OLDCLAIM + PARENT1  + RED_CAR + REVOKED + SEX + TIF + TRAVTIME + URBANICITY + YOJ
```{r bin-age}
# Transform data by putting it into buckets using case_when
testing_df_clean <- testing_df %>%
  mutate(
    AGE_GROUP = case_when(
      AGE <= 25 ~ "Young",
      AGE > 25 & AGE <= 40 ~ "Adult",
      AGE > 40 & AGE <= 60 ~ "Middle Aged",
      AGE > 60 ~ "Senior",
      TRUE ~ NA_character_  # Handle missing values
    )
  )

```

```{r bin-income}
# Fix Income Grouping - Ensure proper quantile calculation
income_quantiles <- quantile(testing_df$INCOME, probs = seq(0, 1, 0.25), na.rm = TRUE)

testing_df_clean <- testing_df_clean %>%
  mutate(
    INCOME_GROUP = case_when(
      INCOME <= income_quantiles[2] ~ "Low",
      INCOME > income_quantiles[2] & INCOME <= income_quantiles[3] ~ "Medium",
      INCOME > income_quantiles[3] & INCOME <= income_quantiles[4] ~ "High",
      INCOME > income_quantiles[4] ~ "Very High",
      TRUE ~ NA_character_
    )
  )
```

```{r}
#final_predictions <- eval %>%
#  select(INDEX) %>%
#  mutate(
#    PRED_TARGET_FLAG = eval_flag_pred,
#    PRED_TARGET_AMT = eval_amt_pred
#  )

#head(final_predictions)

prediction <- predict(model3, testing_df_clean)

```

Explanation:

Everyone gets a binary prediction (crash or not).

Only predicted crashers get a dollar amount predicted.

Others remain at zero.


