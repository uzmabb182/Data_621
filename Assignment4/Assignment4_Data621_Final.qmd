---
title: "Assignment4_Data621"
author: "Mubashira Qari, Puja Roy"
date: "2025-03-23"
output: html_document
---

### Load Libraries

```{r, warning = FALSE, message = FALSE}
# Load required packages
library(htmltools)
library(caret)
library(pROC)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(skimr)
require(DataExplorer)
require(miscTools)
require(MASS)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 

# Load required packages
library(htmltools)
library(caret)
library(pROC)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(skimr)
require(DataExplorer)
require(miscTools)
require(MASS)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 
library(patchwork)  # for combining ggplots
library(e1071)
library(car)

```

### DATA EXPLORATION 

### Variables in the Dataset:


###  load the dataset and understand its structure.
### Load Data and Check Structure

```{r}

# Load datasets from GitHub
training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment4/insurance_training_data.csv")

testing_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment4/insurance-evaluation-data.csv")

# Check the structure of the dataset
str(training_df)
summary(training_df)

```

```{r}
insurance_training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment4/insurance_training_data.csv")

insurance_evaluation_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment4/insurance-evaluation-data.csv")

head(insurance_training_df)
#head(insurance_evaluation_df)
```

### Approach 1: (Puja's Work)

We checked the overview of the dataset's structure while, summary() provided statistical summaries for each variable. The dataset contains 26 columns and 8161 rows. 


### Summary Statistics

```{r}
# Calculate summary statistics for numerical variables
summary_stats <- training_df %>% 
  summarise(across(where(is.numeric), list(mean = mean, sd = sd, median = median), na.rm = TRUE))
print(summary_stats)
```

We computed the mean, standard deviation, and median for all numeric variables, helping us understand the data distribution and central tendencies.

### Visualizing Data Distributions

```{r}
# Bar chart of TARGET_FLAG
ggplot(training_df, aes(x = as.factor(TARGET_FLAG))) +
  geom_bar(fill = "blue") +
  labs(title = "Distribution of TARGET_FLAG", x = "Crash (1 = Yes, 0 = No)", y = "Count")

# Box plot of BLUEBOOK values
ggplot(training_df, aes(y = BLUEBOOK)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Box Plot of BLUEBOOK", y = "Vehicle Value")
```


The first visualization is a bar chart displaying the distribution of TARGET_FLAG, helping to assess the proportion of crashes vs. non-crashes. The second visualization is a box plot for BLUEBOOK, showing the distribution and identifying potential outliers in vehicle values. TARGET_FLAG ratio of having more 0s than 1s indicates that most of the customers in the dataset did not have a crash. It's class imbalance, and it impacts the accuracy of classification models. The BLUEBOOK boxplot indicates that the variable has a strongly dominant majority of values or is long-tailed by outliers. There could be a chance that the data is non-representative, i.e., the majority of the cars have low values with few cars of high value on the tail end of the distribution. 


### Correlation Analysis

```{r}
# Compute correlation matrix for numeric variables
numeric_vars <- training_df %>% select_if(is.numeric)
corr_matrix <- cor(numeric_vars, use = "complete.obs")
corrplot(corr_matrix, method = "circle")
```

The correlation plot provides a graphical impression of the relationship between different numerical variables of the data set. The size and color of the circles convey the strength of the relationship. Dark blue circles indicate strong positive relationships, which predict that if one variable is increasing, so will the other. Dark red circles on the other hand produce negative high correlations, where the rise of one variable is accompanied by a fall in the other. Light-colored circles show weak or no correlation.

Plots suggest that TARGET_FLAG is not strongly correlated with most of the numeric features. Hence, no numeric feature is particularly useful to a customer to make a claim. What it says is perhaps claims will be predicted more accurately with a more advanced method, i.e., interactions, categorical features, or non-linear models.

Moreover, some variables such as MVR_PTS (Motor Vehicle Record Points) and CLM_FREQ (Claim Frequency) show high positive correlation. As would be expected—more holders of violations file more claims. Also, correlations such as AGE and HOMEKIDS or KIDSDRIV reflect older people with children at home, a typical demographic pattern.

Knowledge of such relationships is helpful in model building and data preparation. Correlated variables cause multicollinearity in regression models, necessitating extra work in the form of variance inflation factor (VIF) testing to eliminate redundant information. Target_FLAG correlations also imply that other transformations, engineered features, or other sources of data might be helpful in enhancing predictiveness.


### Missing Value Analysis

```{r}
# Check for missing values
missing_values <- colSums(is.na(training_df))
print(missing_values[missing_values > 0])
```

Missing values show that the data have missing values in relatively high counts of major variables, and most missing values exist in JOB (526 missing), CAR_AGE (510 missing), HOME_VAL (464 missing), INCOME (445 missing), YOJ (454 missing), and AGE (6 missing). Missing values may produce bias or weaken the ability of the model to predict unless appropriately handled.

Missing values for large numbers for variables like JOB and HOME_VAL mean the work details weren't reported for certain customers or home values weren't reported. Missing values for YOJ and INCOME may be for self-employed or the ones having the wrong job history. For CAR_AGE, missing values may be for new acquisitions or lease of automobiles where the age wasn't reported.

Depending on the count of missing values, different imputation methods will be needed. Quantitative features like AGE, INCOME, and CAR_AGE can be imputed with median to remove the impact of outliers, while features like JOB may require one extra "Unknown" value. Additionally, making missing value indicators for features like HOME_VAL will enable the model to learn missing value patterns and enhance the predictive power. Closing gaps like these in an efficient way is vital to achieve model reliability and precision.


### DATA PREPARATION

### Handling Missing Values

Missing values in key numeric columns (AGE, YOJ, INCOME, HOME_VAL, and CAR_AGE) are replaced with their respective medians. Data is preserved and no biased. We then convert YOJ, INCOME, HOME_VAL, and CAR_AGE into numeric to make it easier for managing in the list of upcoming transformations.

In addition, binary flags (YOJ_MISSING, INCOME_MISSING, HOME_VAL_MISSING, CAR_AGE_MISSING, JOB_MISSING) are built to indicate missing values in variables. This allows for models to detect missingness as a predictor.

```{r}
training_df$AGE[is.na(training_df$AGE)] <- median(training_df$AGE, na.rm = TRUE)
training_df$YOJ[is.na(training_df$YOJ)] <- median(training_df$YOJ, na.rm = TRUE)
training_df$INCOME[is.na(training_df$INCOME)] <- median(training_df$INCOME, na.rm = TRUE)
training_df$HOME_VAL[is.na(training_df$HOME_VAL)] <- median(training_df$HOME_VAL, na.rm = TRUE)
training_df$CAR_AGE[is.na(training_df$CAR_AGE)] <- median(training_df$CAR_AGE, na.rm = TRUE)

```

```{r}
str(training_df)
```

```{r}
training_df$YOJ <- as.numeric(training_df$YOJ)
training_df$INCOME <- as.numeric(training_df$INCOME)
training_df$HOME_VAL <- as.numeric(training_df$HOME_VAL)
training_df$CAR_AGE <- as.numeric(training_df$CAR_AGE)
```

```{r}
training_df$YOJ[is.na(training_df$YOJ)] <- median(training_df$YOJ, na.rm = TRUE)
training_df$INCOME[is.na(training_df$INCOME)] <- median(training_df$INCOME, na.rm = TRUE)
training_df$HOME_VAL[is.na(training_df$HOME_VAL)] <- median(training_df$HOME_VAL, na.rm = TRUE)
training_df$CAR_AGE[is.na(training_df$CAR_AGE)] <- median(training_df$CAR_AGE, na.rm = TRUE)

```


```{r}
colSums(is.na(training_df))

```

### Feature Engineering

Some of the new features aim to capture meaningful relationships in the data:

CLAIMS_PER_YEAR: This is the feature that is created by dividing OLDCLAIM by CAR_AGE so as to be able to interpret frequency of claims over a car's lifetime. A safety factor is included so that division by zero will not be encountered.

TOTAL_KIDS: This is a combination of HOMEKIDS and KIDSDRIV to create a feature of total kids number of the household.

HIGH_RISK_CAR: Certain types of cars (e.g., "Sports Car", "Luxury SUV") are designated as high-risk by encoding them with a binary value of 1.

```{r}
# Convert CAR_AGE and OLDCLAIM to numeric to avoid errors
training_df$CAR_AGE <- as.numeric(training_df$CAR_AGE)
training_df$OLDCLAIM <- as.numeric(training_df$OLDCLAIM)

# Ensure no division by zero when creating CLAIMS_PER_YEAR
training_df <- training_df %>%
  mutate(
    CLAIMS_PER_YEAR = ifelse(!is.na(CAR_AGE) & CAR_AGE > 0, OLDCLAIM / CAR_AGE, OLDCLAIM)
  )

# Convert HOMEKIDS and KIDSDRIV to numeric if they are not already
training_df$HOMEKIDS <- as.numeric(training_df$HOMEKIDS)
training_df$KIDSDRIV <- as.numeric(training_df$KIDSDRIV)

# Create a new feature: Total number of kids (home + driving)
training_df <- training_df %>%
  mutate(
    TOTAL_KIDS = ifelse(!is.na(HOMEKIDS) & !is.na(KIDSDRIV), HOMEKIDS + KIDSDRIV, NA)
  )

# Convert CAR_TYPE to character to ensure proper comparison
training_df$CAR_TYPE <- as.character(training_df$CAR_TYPE)

# Create a binary flag for high-risk car types
training_df <- training_df %>%
  mutate(
    HIGH_RISK_CAR = ifelse(CAR_TYPE %in% c("Sports Car", "Luxury SUV"), 1, 0)
  )
```


### Create Flags
```{r}
# Create binary flags for missing values (1 = missing, 0 = not missing)
training_df <- training_df %>%
  mutate(
    YOJ_MISSING = ifelse(is.na(YOJ), 1, 0),
    INCOME_MISSING = ifelse(is.na(INCOME), 1, 0),
    HOME_VAL_MISSING = ifelse(is.na(HOME_VAL), 1, 0),
    CAR_AGE_MISSING = ifelse(is.na(CAR_AGE), 1, 0),
    JOB_MISSING = ifelse(is.na(JOB), 1, 0)
  )

```


### Transform data

Categorical groupings are created to reduce numerical variables:
AGE_GROUP: Four age groups—Young (≤25), Adult (26-40), Middle-Aged (41-60), and Senior (>60)—categorize individuals.

INCOME_GROUP: Income is divided into four quantile-based groups (Low, Medium, High, Very High) such that there is an equal share of income levels.

```{r}
# Transform data by putting it into buckets using case_when
training_df <- training_df %>%
  mutate(
    AGE_GROUP = case_when(
      AGE <= 25 ~ "Young",
      AGE > 25 & AGE <= 40 ~ "Adult",
      AGE > 40 & AGE <= 60 ~ "Middle-Aged",
      AGE > 60 ~ "Senior",
      TRUE ~ NA_character_  # Handle missing values
    ),
    INCOME_GROUP = case_when(
      INCOME <= quantile(INCOME, 0.25, na.rm = TRUE) ~ "Low",
      INCOME > quantile(INCOME, 0.25, na.rm = TRUE) & INCOME <= quantile(INCOME, 0.50, na.rm = TRUE) ~ "Medium",
      INCOME > quantile(INCOME, 0.50, na.rm = TRUE) & INCOME <= quantile(INCOME, 0.75, na.rm = TRUE) ~ "High",
      INCOME > quantile(INCOME, 0.75, na.rm = TRUE) ~ "Very High",
      TRUE ~ NA_character_
    )
  )

```


Skewed numerical attributes are log-transformed to stabilize their distributions:

LOG_BLUEBOOK (log-transformed value of the vehicle),

LOG_OLDCLAIM (log-transformed old claims),

LOG_INCOME (log-transformed income).
NA values are substituted with a small positive constant (0.01) before applying the log transformation to prevent computation errors.


Creating New Features:

To capture relationships between features more effectively, new ratio-based features are formed:

CLAIMS_INCOME_RATIO: Indicates the proportion of old claims relative to income.

VEHICLE_VALUE_TO_INCOME: Indicates how cheap the vehicle is relative to income.

CLAIMS_TO_MVR_PTS: Prior claims quantity per vehicle record to indicate the severity of prior offenses.

Also an estimated risk score by averaging important risk indicators (MVR_PTS, CLM_FREQ, HIGH_RISK_CAR, and REVOKED). The risk score forms a composite measure of the subject's risk level.

```{r}
# Ensure necessary columns are numeric
training_df <- training_df %>%
  mutate(
    CAR_AGE = as.numeric(CAR_AGE),
    OLDCLAIM = as.numeric(OLDCLAIM),
    HOMEKIDS = as.numeric(HOMEKIDS),
    KIDSDRIV = as.numeric(KIDSDRIV),
    INCOME = as.numeric(INCOME),
    AGE = as.numeric(AGE),
    BLUEBOOK = as.numeric(BLUEBOOK),
    CLM_FREQ = as.numeric(CLM_FREQ),
    MVR_PTS = as.numeric(MVR_PTS),
    REVOKED = as.numeric(as.character(REVOKED)),
    YOJ = as.numeric(YOJ),
    HOME_VAL = as.numeric(HOME_VAL)
  )

# Handle NA values before log transformation
training_df <- training_df %>%
  mutate(
    BLUEBOOK = ifelse(is.na(BLUEBOOK), 0.01, BLUEBOOK),
    OLDCLAIM = ifelse(is.na(OLDCLAIM), 0.01, OLDCLAIM),
    INCOME = ifelse(is.na(INCOME), 0.01, INCOME)
  )

# Apply log transformation
training_df <- training_df %>%
  mutate(
    LOG_BLUEBOOK = log1p(BLUEBOOK),
    LOG_OLDCLAIM = log1p(OLDCLAIM),
    LOG_INCOME = log1p(INCOME)
  )

# Fix Income Grouping - Ensure proper quantile calculation
income_quantiles <- quantile(training_df$INCOME, probs = seq(0, 1, 0.25), na.rm = TRUE)

training_df <- training_df %>%
  mutate(
    INCOME_GROUP = case_when(
      INCOME <= income_quantiles[2] ~ "Low",
      INCOME > income_quantiles[2] & INCOME <= income_quantiles[3] ~ "Medium",
      INCOME > income_quantiles[3] & INCOME <= income_quantiles[4] ~ "High",
      INCOME > income_quantiles[4] ~ "Very High",
      TRUE ~ NA_character_
    )
  )

# Ensure no division by zero when creating ratios
training_df <- training_df %>%
  mutate(
    CLAIMS_INCOME_RATIO = ifelse(INCOME > 0, OLDCLAIM / INCOME, NA),
    VEHICLE_VALUE_TO_INCOME = ifelse(INCOME > 0, BLUEBOOK / INCOME, NA),
    CLAIMS_TO_MVR_PTS = ifelse(MVR_PTS > 0, OLDCLAIM / MVR_PTS, NA)
  )

# Create a risk score by combining various indicators
training_df <- training_df %>%
  mutate(
    RISK_SCORE = (MVR_PTS * 0.4) + (CLM_FREQ * 0.3) + (HIGH_RISK_CAR * 0.2) + (REVOKED * 0.1)
  )

```

### Approach 2: (Mubashira's Work)

### Clean Currency Columns and Prepare Data

Cleaning financial variables (e.g., income, car value) often include symbols

```{r}
#skim(insurance_training_df)
colnames(insurance_training_df)
```

### Check for Missing Values

```{r}
# Visualize missing data as a heatmap
plot_missing(insurance_training_df)

```

### Handling Missing Values

```{r}
# Remove rows where key predictors are missing
insurance_training_df <- insurance_training_df %>% filter(!is.na(INCOME) & !is.na(YOJ))

```


```{r}
# Remove $ and , from currency columns
# Function to clean dollar values
clean_money <- function(x) {
  as.numeric(gsub("[$,]", "", x))
}

# Apply to relevant columns
money_vars <- c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM", "TARGET_AMT")
insurance_training_df[money_vars] <- lapply(insurance_training_df[money_vars], clean_money)
insurance_evaluation_df[money_vars] <- lapply(insurance_evaluation_df[money_vars], clean_money)

head(insurance_training_df)
```

### Convert categorical variables to factors

GLMs require categorical predictors to be in factor form so R knows to create dummy variables internally.

```{r}
# GLMs support categorical predictors through factors. Without converting, R won’t treat these as categories

cat_vars <- c("PARENT1", "MSTATUS", "SEX", "EDUCATION", "JOB",
              "CAR_USE", "CAR_TYPE", "RED_CAR", "REVOKED", "URBANICITY")

insurance_training_df[cat_vars] <- lapply(insurance_training_df[cat_vars], as.factor)
insurance_evaluation_df[cat_vars] <- lapply(insurance_evaluation_df[cat_vars], as.factor)

insurance_training_df[cat_vars]
```

### Check Skewness and Apply Log Transformations

```{r}
# INCOME plot
p1 <- ggplot(insurance_training_df, aes(x = INCOME)) +
  geom_histogram(fill = "#2c7fb8", bins = 30) +
  labs(title = "Distribution of INCOME")

# HOME_VAL plot
p2 <- ggplot(insurance_training_df, aes(x = HOME_VAL)) +
  geom_histogram(fill = "#41ab5d", bins = 30) +
  labs(title = "Distribution of HOME_VAL")

# OLDCLAIM plot
p3 <- ggplot(insurance_training_df, aes(x = OLDCLAIM)) +
  geom_histogram(fill = "#f03b20", bins = 30) +
  labs(title = "Distribution of OLDCLAIM")

# BLUEBOOK plot
p4 <- ggplot(insurance_training_df, aes(x = BLUEBOOK)) +
  geom_histogram(fill = "#ffbf00", bins = 30) +
  labs(title = "Distribution of BLUEBOOK")

# Combine all four using patchwork
(p1 | p2) / (p3 | p4)


```

### Quantify Skewness

```{r}

# Values > 1 or < -1 are highly skewed

skewness(insurance_training_df$INCOME, na.rm = TRUE)  

skewness(insurance_training_df$HOME_VAL, na.rm = TRUE)  

skewness(insurance_training_df$OLDCLAIM, na.rm = TRUE) 

skewness(insurance_training_df$BLUEBOOK, na.rm = TRUE)  

```


Skewness distorts relationships in regression.

Log transformation reduces skewness by compressing extreme values.

We use +1 to avoid log(0) errors.

### Remove or impute missing values + transform variables

GLMs are sensitive to missing values and skewed data. Log transforms help normalize heavily skewed predictors.

```{r}
insurance_training_clean <- insurance_training_df %>%
  filter(!is.na(INCOME) & !is.na(YOJ)) %>%
  mutate(
    INCOME_LOG = log(INCOME + 1),
    HOME_VAL_LOG = log(HOME_VAL + 1),
    OLDCLAIM_LOG = log(OLDCLAIM + 1),
    BLUEBOOK_LOG = log(BLUEBOOK + 1)
  )


```

### Compare Before and After Transformation

Helps ensure variables are treated as discrete categories, not continuous numbers.

```{r}
# Set layout to plot side-by-side
par(mfrow = c(1, 2))

# INCOME
hist(insurance_training_clean$INCOME, 
     main = "Original INCOME", 
     col = "salmon", 
     xlab = "insurance_training_clean$INCOME")

hist(insurance_training_clean$INCOME_LOG, 
     main = "Log Transformed INCOME", 
     col = "seagreen", 
     xlab = "insurance_training_clean$INCOME_LOG")


# HOME_VAL
par(mfrow = c(1, 2))
hist(insurance_training_clean$HOME_VAL, 
     main = "Original HOME_VAL", 
     col = "tomato", 
     xlab = "insurance_training_clean$HOME_VAL")

hist(insurance_training_clean$HOME_VAL_LOG, 
     main = "Log Transformed HOME_VAL", 
     col = "forestgreen", 
     xlab = "insurance_training_clean$HOME_VAL_LOG")


# OLDCLAIM
par(mfrow = c(1, 2))
hist(insurance_training_clean$OLDCLAIM, 
     main = "Original OLDCLAIM", 
     col = "lightskyblue", 
     xlab = "insurance_training_clean$OLDCLAIM")

hist(insurance_training_clean$OLDCLAIM_LOG, 
     main = "Log Transformed OLDCLAIM", 
     col = "darkgreen", 
     xlab = "insurance_training_clean$OLDCLAIM_LOG")


# BLUEBOOK
par(mfrow = c(1, 2))
hist(insurance_training_clean$BLUEBOOK, 
     main = "Original BLUEBOOK", 
     col = "khaki3", 
     xlab = "insurance_training_clean$BLUEBOOK")

hist(insurance_training_clean$BLUEBOOK_LOG, 
     main = "Log Transformed BLUEBOOK", 
     col = "darkolivegreen4", 
     xlab = "insurance_training_clean$BLUEBOOK_LOG")

```

### Visualize Category Distributions

Helps ensure variables are treated as discrete categories, not continuous numbers.

```{r}
# Example: Education levels
ggplot(insurance_training_clean, aes(x = EDUCATION)) +
  geom_bar(fill = "dodgerblue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Distribution of EDUCATION Levels")

```
### Check Variable Correlations (Multicollinearity Insight)

Correlation Matrix for Numeric Variables

```{r}
# Select only numeric columns
numeric_vars <- insurance_training_clean %>% dplyr::select(where(is.numeric))

# Remove rows with NAs
numeric_vars <- na.omit(numeric_vars)

# Compute correlation matrix
cor_matrix <- cor(numeric_vars)

# Visualize correlation matrix
corrplot(cor_matrix, method = "color", type = "lower", tl.cex = 0.8)

```

### Interpretation:

Dark blue/red squares indicate strong positive/negative correlation.

Looking for variable pairs with |correlation| > 0.8 to watch out for multicollinearity.

### Visualize Distributions

Using DataExplorer for All Numeric Distributions

```{r}
plot_histogram(insurance_training_clean)

```

### Purpose:

This gives mini-histograms for all numeric variables, so we can quickly:

Spot skewness

Check ranges

Identify variables with spikes or unusual spread

### Density Plot for a Few Variables

```{r}
# Example: BLUEBOOK
ggplot(insurance_training_clean, aes(x = BLUEBOOK)) +
  geom_density(fill = "lightgreen") +
  labs(title = "Density of BLUEBOOK (Vehicle Value)")

```


### Binary Logistic Regression (TARGET_FLAG)

### Fit the Initial Logistic Regression Model

We use glm() with family = binomial for logistic regression. This model predicts the probability of a crash (1).
We are predicting TARGET_FLAG (binary: 1 = made a claim, 0 = no claim).

```{r}
logit_model <- glm(
  TARGET_FLAG ~ KIDSDRIV + YOJ + INCOME_LOG + PARENT1 +
    MSTATUS + EDUCATION + JOB + TRAVTIME + CAR_USE + TIF +
    CAR_TYPE + OLDCLAIM_LOG + REVOKED + MVR_PTS + URBANICITY,
  data = insurance_training_clean,
  family = binomial
)


summary(logit_model)


```
Interpreting the Output: Significant p-values (< 0.05) indicate variables that are strong predictors of making a claim.

### Model Significance with Deviance Test

```{r}
null_dev <- logit_model$null.deviance
resid_dev <- logit_model$deviance
df_diff <- logit_model$df.null - logit_model$df.residual
p_val <- 1 - pchisq(null_dev - resid_dev, df_diff)

cat("Model significance p-value:", p_val)

# Check each variable
anova(logit_model, test = "Chisq")

```

### Run Assumption Tests Using Residuals

### Assumption Testing – Logistic

### Multicollinearity: Variance Inflation Factor

```{r}
vif(logit_model)

```
Explanation: VIF > 5 or 10 indicates multicollinearity. Remove or combine correlated variables.

### ROC Curve + AUC

```{r}
identical(length(pred_probs), length(insurance_training_clean$TARGET_FLAG))  # Should return TRUE

```


```{r}
# Step 1: Load the library
library(pROC)

# Step 2: Generate predicted probabilities from your logistic model
pred_probs <- predict(logit_model, newdata = insurance_training_clean, type = "response")

# Step 3: Create the ROC object
roc_obj <- roc(response = insurance_training_clean$TARGET_FLAG,
               predictor = pred_probs)

# Step 4: Plot the ROC Curve
plot(roc_obj, main = "ROC Curve")

# Step 5: Get the AUC
pROC::auc(roc_obj)




```

Explanation: Measures model's ability to distinguish crashers vs. non-crashers. AUC closer to 1 = better.

### Confusion Matrix

```{r}
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
confusionMatrix(as.factor(pred_class), as.factor(insurance_training_clean$TARGET_FLAG))

```
Explanation: Helps evaluate accuracy, sensitivity, specificity using 0.5 as threshold.

### Multiple Linear Regression (TARGET_AMT)

### Subset Only Crashers

Explanation: Only those who crashed (TARGET_FLAG = 1) have claim amounts > 0. Others are always 0.

```{r}
claims_only <- insurance_training_clean %>% filter(TARGET_FLAG == 1)

```

### Fit Linear Model

```{r}
lm_model <- lm(
  TARGET_AMT ~ KIDSDRIV + YOJ + INCOME_LOG + PARENT1 +
    MSTATUS + EDUCATION + JOB + TRAVTIME + CAR_USE + TIF +
    CAR_TYPE + OLDCLAIM_LOG + REVOKED + MVR_PTS + URBANICITY + BLUEBOOK_LOG,
  data = claims_only
)
summary(lm_model)

```

### Assumption Testing – Linear

### Linearity & Residual Plots

Explanation: p > 0.05 = residuals are normally distributed (good!)

```{r}

par(mfrow = c(2,2))
plot(lm_model)

```
Explanation:

Residuals vs. Fitted: should show no pattern (linearity)

Q-Q Plot: points should fall along diagonal (normality)

### Normality of Residuals

```{r}
shapiro.test(residuals(lm_model))

```
### Multicollinearity

```{r}

vif(lm_model)

```
### Make Predictions on Evaluation Data

Predict Crash Probability & Classification

```{r}
eval_probs <- predict(logit_model, newdata = insurance_evaluation_df, type = "response")
eval_flag_pred <- ifelse(eval_probs > 0.5, 1, 0)

```
### Predict Cost for Crashers Only

```{r}
eval_amt_pred <- rep(0, nrow(eval))
eval_crash_idx <- which(eval_flag_pred == 1)
eval_amt_pred[eval_crash_idx] <- predict(lm_model, newdata = insurance_evaluation_df[eval_crash_idx, ])

```

### Final Combined Output

```{r}
final_predictions <- eval %>%
  select(INDEX) %>%
  mutate(
    PRED_TARGET_FLAG = eval_flag_pred,
    PRED_TARGET_AMT = eval_amt_pred
  )

head(final_predictions)
write_csv(final_predictions, "insurance_predictions.csv")

```
Explanation:

Everyone gets a binary prediction (crash or not).

Only predicted crashers get a dollar amount predicted.

Others remain at zero.



























































