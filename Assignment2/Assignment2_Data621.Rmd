---
title: "Assignment2_Data621"
author: "Mubashira Qari"
date: "2025-03-02"
output: html_document
---

### Load Libraries

```{r global_options, include=FALSE}
# Load required packages
#install.packages("htmltools", dependencies = TRUE)
library(htmltools)
library(caret)
library(pROC)
library(tidyverse)
library(ggplot2)
library(skimr)
require(car)
require(DataExplorer)
require(miscTools)
require(MASS)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 
```

### Loading Datasets

-   load the dataset and understand its structure.

```{r}


classification_df <- read.csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment2/classification-output-data.csv")

head(classification_df)

```
```{r}

# Count occurrences of each class
class_counts <- table(classification_df$class)

# Compute percentages
class_percentages <- prop.table(class_counts) * 100

# Print results
print(class_counts)
print(class_percentages)

```


Class 0 (majority) → 124 samples
Class 1 (minority) → 57 samples

A dataset is considered imbalanced if the class distribution is highly skewed, typically:

Mild imbalance: One class is 60-70%, the other is 30-40%.
Moderate imbalance: One class is 70-85%, the other is 15-30%.
Severe imbalance: One class is 90% or more, the other is 10% or less.
Since Class 1 still makes up 31.5%, this dataset has a mild imbalance, but it's not extreme.


### The dataset has three key columns:

-   class: The actual class (0 = Negative, 1 = Positive)
-   scored.class: The predicted class (0 or 1, based on a 0.5 threshold)
-   scored.probability: The probability score of the prediction



### The goal is to evaluate model performance through various metrics such as accuracy, precision, recall (sensitivity), specificity, F1-score, and ROC-AUC.

### Generating Confusion Matrix

-   The confusion matrix is a fundamental tool for measuring classification performance. 
-   It provides a breakdown of correct and incorrect predictions by comparing the predicted class labels to the actual class labels.
-   For a binary classification problem (e.g., "positive" vs. "negative"), the confusion matrix is structured as:

True Positives (TP) – Correctly predicted positives
True Negatives (TN) – Correctly predicted negatives
False Positives (FP) – Incorrectly predicted positives (Type I error)
False Negatives (FN) – Incorrectly predicted negatives (Type II error)

The confusion matrix structure:

Actual \ Predicted	0 (Negative)	1 (Positive)
0 (Negative)	          TN	        FP
1 (Positive)	          FN	        TP

```{r}

conf_matrix <- table(classification_df$class, classification_df$scored.class)

print(conf_matrix)


```
### Applying Classification Metric Functions

1-  Accuracy:

-   Accuracy measures overall correctness of the classification model.
-   It represents the proportion of correct predictions (both positive and negative) out of all predictions.

-   Accuracy= (TP+TN)/(TP+FP+TN+FN)

-   Measures overall correctness but can be misleading if classes are imbalanced (i.e., one class appears much more frequently than the other).

-   In our results: TP = 27, TN = 119, FP = 5, FN = 30

Accuracy= (27+119)/(27+119+5+30)= 146/181=0.8066(80.66%)
This means 80.66% of the predictions were correct.

```{r}

calculate_accuracy <- function(data) {
  TP <- sum(data$class == 1 & data$scored.class == 1)
  TN <- sum(data$class == 0 & data$scored.class == 0)
  FP <- sum(data$class == 0 & data$scored.class == 1)
  FN <- sum(data$class == 1 & data$scored.class == 0)
  
  accuracy <- (TP + TN) / (TP + TN + FP + FN)
  return(accuracy)
}

# Calling calculate_accuracy function
accuracy <- calculate_accuracy(classification_df)
print(paste("Accuracy:", accuracy))

```
2-  Classification Error Rate

It is the proportion of incorrectly classified cases.
ErrorRate <- (FP + FN) / (TP + TN + FP + FN)

Accuracy measures the proportion of correct predictions
accuracy <- (TP + TN) / (TP + TN + FP + FN)

A prediction is either correct (accuracy) or incorrect (error), the sum of accuracy and error rate must be 1. That is why:
ErrorRate = 1−Accuracy

```{r}

calculate_error_rate <- function(data) {
  error_rate <- 1 - calculate_accuracy(data)
  return(error_rate)
}

# Compute Error Rate
error_rate <- calculate_error_rate(classification_df)
print(paste("Error Rate:", error_rate))

```
### Interpretation:

19.34% of the predictions are incorrect.
This means that 80.66% of the predictions were correct (since Accuracy + Error Rate = 1).
A lower error rate is better, as it indicates fewer mistakes in classification.
If the error rate is high, the model may need improvement through better feature selection, threshold tuning, or model optimization.

### When Is a Low or High Error Rate Acceptable?
In a medical diagnosis model, even a 5% error rate could be dangerous.
In spam filtering, a 20% error rate might be acceptable since false positives (non-spam marked as spam) aren't critical.
For financial fraud detection, a low error rate is crucial to prevent financial losses.

### Verification:

The sum of accuracy + error rate = 1

```{r}
sum(accuracy, error_rate)
```

3-  Precision (Positive Predictive Value)

-   Measures how many of the predicted positive cases were actually positive.

-   Precision= TP/(TP+FP)

TP (True Positives) = Correctly predicted positive cases.
FP (False Positives) = Incorrectly predicted positives (actual class was 0, but predicted as 1).

### Why Do We Calculate Precision (Positive Predictive Value)?

Precision, also known as Positive Predictive Value (PPV), helps us understand:

"When the model predicts a positive case (1), how often is it correct?"

This is crucial in scenarios where false positives (FP) are costly, such as:

Spam detection (flagging important emails as spam is bad).
Fraud detection (incorrectly marking a legitimate transaction as fraud can be inconvenient for customers).
Medical diagnosis (misdiagnosing a healthy person with a disease leads to unnecessary stress and tests).



```{r}
calculate_precision <- function(data) {
  TP <- sum(data$class == 1 & data$scored.class == 1)
  FP <- sum(data$class == 0 & data$scored.class == 1)
  
  precision <- TP / (TP + FP)
  return(precision)
}

# Compute Precision
precision <- calculate_precision(classification_df)
print(paste("Precision:", precision))

```
### What Does "Precision: 0.84375" Mean?

Precision=0.84375

### Interpretation:

84.38% of the predicted positive cases are actually correct.
15.62% of the predicted positives are false positives (wrongly classified as positive).
A higher precision is desirable in cases where false positives have major consequences.


4-  Sensitivity (Recall / True Positive Rate)

-   Measures the proportion of actual positives correctly identified.

-   sensitivity <- TP / (TP + FN)

Sensitivity (also known as Recall or True Positive Rate) helps us understand:

"Out of all actual positive cases (1s), how many did the model correctly identify?"

It focuses on the model’s ability to catch as many actual positive cases as possible.

TP (True Positives) = Cases correctly predicted as positive.
FN (False Negatives) = Cases that were actually positive but the model failed to detect (predicted as 0).

### Why Are We Calculating Sensitivity?

Sensitivity is important when missing a positive case is costly, such as:

Medical Diagnosis
If a model fails to detect a disease (false negative), the patient might not receive treatment.
Fraud Detection
If a fraudulent transaction is missed, money is lost.
Spam Filtering
If an actual spam email is not caught, it could lead to security risks.

```{r}
calculate_sensitivity <- function(data) {
  TP <- sum(data$class == 1 & data$scored.class == 1)
  FN <- sum(data$class == 1 & data$scored.class == 0)
  
  sensitivity <- TP / (TP + FN)
  return(sensitivity)
}

# Compute Sensitivity
sensitivity <- calculate_sensitivity(classification_df)
print(paste("Sensitivity:", sensitivity))

```
### Interpreting "Sensitivity: 0.473684210526316"

Sensitivity=0.4737

### What Does This Mean?

The model correctly identifies about 47.37% of the actual positive cases.
52.63% of the actual positive cases were missed (False Negatives).
This means many positive cases were classified as negative, which could be a big issue in high-risk scenarios (e.g., medical testing).

### Is This a Good Sensitivity Score?

Low Sensitivity (below 50%) means the model misses too many actual positives.
Higher Sensitivity (>80%) is preferred when missing a positive case is unacceptable.

### How to Improve Sensitivity?

Lower the Classification Threshold (e.g., from 0.5 to 0.4)
More cases will be classified as positive, reducing false negatives.
Use Oversampling Techniques (SMOTE) for Imbalanced Data
If the dataset is imbalanced, we may need to generate more positive cases.
Use a Different Model (Ensemble Methods)
Random Forest or Boosting models often improve recall.



5- Specificity (True Negative Rate)

-   Measures the proportion of actual negatives correctly identified.

-   Specificity= TN/(TN+FP)

TN (True Negatives) = Cases correctly predicted as negative (actual 0, predicted 0).
FP (False Positives) = Cases incorrectly predicted as positive (actual 0, predicted 1).

### Why Do We Calculate Specificity (True Negative Rate)?

Specificity (also called the True Negative Rate) measures:

"Out of all actual negative cases (0s), how many did the model correctly classify as negative?"

It tells us how well the model avoids false alarms (false positives).

### Why Is Specificity Important?

Specificity is critical when false positives are costly, such as:

Spam Detection:
If specificity is low, too many non-spam emails are incorrectly marked as spam (false positives).
Fraud Detection:
If specificity is low, many legitimate transactions get blocked, annoying customers.
Medical Testing (e.g., Cancer Screening):
If specificity is low, many healthy people are wrongly told they might have cancer, causing unnecessary stress.


```{r}
calculate_specificity <- function(data) {
  TN <- sum(data$class == 0 & data$scored.class == 0)
  FP <- sum(data$class == 0 & data$scored.class == 1)
  
  specificity <- TN / (TN + FP)
  return(specificity)
}

# Compute Specificity
specificity <- calculate_specificity(classification_df)
print(paste("Specificity:", specificity))

```
### What Does "Specificity: 0.959677419354839" Mean?

Specificity=0.9597

### Interpretation:

95.97% of actual negative cases were correctly classified as negative.
4.03% of the actual negative cases were incorrectly classified as positive (False Positives).
The model is very good at recognizing negative cases and avoids too many false positives.

### Is This a Good Specificity Score?

Specificity above 90% is usually excellent for models that need to avoid false positives.
However, if specificity is very high and sensitivity is low, it means the model avoids false positives but fails to detect real positives.

### Specificity vs. Sensitivity Tradeoff

High Specificity, Low Sensitivity → Model is too strict (avoids false positives but misses true positives).
High Sensitivity, Low Specificity → Model is too lenient (catches all positives but has many false positives).
Balance Depends on the Use Case.

### How to Improve Specificity?

Increase the Classification Threshold (e.g., from 0.5 to 0.6 or higher).
The model will be more cautious in predicting positives.

### Feature Engineering:

Adding more relevant variables can help improve model discrimination.
Use Different Models (SVM, Decision Trees, Random Forest)




6-  F1 Score:

F1 = 2 x (Precision x Sensitivity)/(Precision + Sensitivity)

Precision = How many of the predicted positives were actually correct?
Sensitivity (Recall) = How many of the actual positives were correctly predicted

### Why Do We Calculate F1 Score?

The F1 Score is calculated because it provides a balance between Precision and Sensitivity (Recall), especially when there is an imbalance between classes in the dataset.

It helps us understand:

"How well does the model perform in detecting positives, considering both false positives and false negatives?"

F1 Score is useful when:

Precision and Sensitivity (Recall) give conflicting results (e.g., one is high, and the other is low).
The dataset is imbalanced, meaning one class is much more common than the other.
We want an overall measure of the model’s reliability in detecting positives.

The F1 Score takes the harmonic mean of Precision and Sensitivity, meaning:

If either Precision or Sensitivity is low, the F1 Score will also be low.
It rewards models that have a good balance between these two metrics.

```{r}
calculate_f1_score <- function(data) {
  precision <- calculate_precision(data)
  sensitivity <- calculate_sensitivity(data)
  
  f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)
  return(f1_score)
}

# Compute F1 Score
f1_score <- calculate_f1_score(classification_df)
print(paste("F1 Score:", f1_score))

```
-   Bounds of F1 Score
-   Since 0 ≤ Precision ≤ 1 and 0 ≤ Sensitivity ≤ 1, the F1 score must also be between 0 and 1.

### What Does "F1 Score: 0.606741573033708" Mean?

𝐹1 Score = 0.6067

### Interpretation:

60.67% is the balance between precision and recall.
This means the model has moderate performance in identifying positive cases.
It is neither too biased towards precision (avoiding false positives) nor too biased towards recall (avoiding false negatives).

### Is This a Good F1 Score?

F1 Score = 1 → Perfect model (no false positives or false negatives).
F1 Score = 0 → Worst model (completely wrong predictions).
F1 Score > 0.7 → Good performance in most cases.
F1 Score between 0.5 - 0.7 → Moderate performance, meaning the model needs improvement.
F1 Score < 0.5 → Poor performance, meaning the model is unreliable.

### How to Improve the F1 Score?

Optimize Precision-Recall Balance

If Precision is too low → Reduce false positives.
If Recall is too low → Reduce false negatives.
Adjust the Classification Threshold

Instead of 0.5, experiment with different thresholds (e.g., 0.4 or 0.6).
Use Better Models

Random Forest, Gradient Boosting, or Deep Learning models can improve performance.
Feature Engineering

Adding better features can help the model make more informed decisions.


### Generating ROC Curve and Compute AUC

The ROC Curve is a graphical representation that evaluates the performance of a binary classification model at different probability thresholds.

The ROC curve shows the trade-off between sensitivity and specificity at different probability thresholds.
The AUC (Area Under Curve) measures model performance:
AUC = 1 → Perfect Model
AUC = 0.5 → Random Guessing

the dataset (classification_df) contains:

class → The actual class labels (0 or 1).
scored.probability → The predicted probability for class 1.

```{r}
# Check for missing values in scored.probability
sum(is.na(classification_df$scored.probability))  # Count missing values

# Remove rows with missing values
df <- na.omit(classification_df)

# Verify that missing values are removed
sum(is.na(classification_df$scored.probability))  # Should return 0

# Compute the ROC curve using actual labels and predicted probabilities
roc_curve <- roc(classification_df$class, classification_df$scored.probability)

# Print ROC object details
print(roc_curve)

```

```{r}
# Plot the ROC Curve
plot(roc_curve, col = "blue", main = "ROC Curve")
```
```{r}

# Load the required library
library(pROC)
# Compute ROC curve using pROC
roc_curve <- roc(response = classification_df$class, predictor = classification_df$scored.probability)

# Print ROC details
print(roc_curve)


```

### Investigating caret and pROC Packages

```{r}
# Convert 'class' to factor (caret requires factors for confusion matrix)
classification_df$class <- as.factor(classification_df$class)

# Convert 'scored.class' to factor (caret requires factors for confusion matrix)
classification_df$scored.class <- as.factor(classification_df$scored.class)

# Convert 'scored.probability' to numeric
classification_df$scored.probability <- as.numeric(classification_df$scored.probability)

```

caret requires factors for confusion matrix calculations.
pROC requires numeric probabilities for ROC/AUC calculations.

### Compute Confusion Matrix Using caret

```{r}
library(caret)
# Compute confusion matrix using caret
conf_matrix <- confusionMatrix(classification_df$scored.class, classification_df$class)

# Print confusion matrix results
print(conf_matrix)

```
### Compute Sensitivity and Specificity Using caret

```{r}
# Compute sensitivity (recall)
sensitivity_score <- sensitivity(classification_df$scored.class, classification_df$class)

# Compute specificity
specificity_score <- specificity(classification_df$scored.class, classification_df$class)

# Print results
print(paste("Sensitivity:", sensitivity_score))
print(paste("Specificity:", specificity_score))

```
### Compute ROC Curve and AUC Score Using pROC

```{r}
# Load the required library
library(pROC)
# Compute ROC curve using pROC
roc_curve <- roc(response = classification_df$class, predictor = classification_df$scored.probability)

# Print ROC details
print(roc_curve)

```
### Plot ROC Curve (Receiver Operating Characteristic) Curve

The ROC (Receiver Operating Characteristic) Curve is a graphical representation used to evaluate the performance of a binary classification model. 
It shows the trade-off between True Positive Rate (Sensitivity/Recall) and False Positive Rate (1 - Specificity) at different probability thresholds.

```{r}

# Plot ROC Curve
plot(roc_curve, col = "blue", main = "ROC Curve using pROC")

```

```{r}
library(pROC)

# Function to generate a clear ROC curve
generate_roc_curve <- function(data, actual_col, prob_col) {
  # Ensure correct data types
  data[[actual_col]] <- as.factor(data[[actual_col]])
  data[[prob_col]] <- as.numeric(data[[prob_col]])

  # Compute the ROC curve
  roc_curve <- roc(response = data[[actual_col]], predictor = data[[prob_col]], 
                   percent = FALSE, ci = TRUE, smooth = FALSE)  # No smoothing

  # FIX: Reduce number of threshold labels & adjust text size
  plot(roc_curve, col = "blue", main = "ROC Curve with Optimized Thresholds", 
       print.thres = seq(0, 1, 0.2),  # Show labels only at 0, 0.2, 0.4, 0.6, 0.8, 1
       cex = 0.7)  # Reduce text size
  
  # Compute and print AUC Score
  auc_value <- auc(roc_curve)
  print(paste("AUC Score:", auc_value))

  # Return ROC curve and AUC as a list
  return(list("ROC Curve" = roc_curve, "AUC Score" = auc_value))
}


# Run the function
roc_results <- generate_roc_curve(classification_df, "class", "scored.probability")

```
### Interpretation of AUC Score:

AUC > 0.9 → Excellent Model
AUC 0.8 - 0.9 → Good Model
AUC 0.7 - 0.8 → Fair Model
AUC ≈ 0.5 → Random Guessing (Bad Model)


### Interpretation:

Closer to 1.0 means 	Better model performance.
Closer to 0.5 means Model is guessing randomly (bad performance).

AUC = 0.85 → means model has an 85.03% chance of correctly distinguishing between a positive and negative case.
This is a good model, meaning it has strong predictive performance.
However, there is still room for improvement (e.g., by tuning hyperparameters, adding more features, or balancing the dataset).

ROC Curve	Shows model performance at different thresholds.
AUC Score: 0.85	Model correctly classifies 85.03% of cases.


### Generate a Confusion Matrix Heatmap

```{r}

# Compute Confusion Matrix
conf_matrix <- confusionMatrix(classification_df$scored.class, classification_df$class)

# Extract table for visualization
conf_table <- as.table(conf_matrix$table)

# Convert table to data frame for ggplot2
conf_df <- as.data.frame(conf_table)
colnames(conf_df) <- c("Actual", "Predicted", "Count")

# Plot Confusion Matrix using ggplot2
ggplot(conf_df, aes(x = Actual, y = Predicted, fill = Count)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  geom_text(aes(label = Count), color = "black", size = 5) +
  labs(title = "Confusion Matrix Heatmap") +
  theme_minimal()


```


