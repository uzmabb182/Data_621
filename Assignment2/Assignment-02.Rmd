---
title: "Assignment2"
author: "MQ MC Puja Roy"
date: "2025-02-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(miscTools)
library(ggplot2) 
library(caret)

```

1. Load data
```{r import-data, echo=FALSE}
data_df <- data.frame(read_csv("classification-output-data.csv"))
```

2. The data set has three key columns we will use:
- class: the actual class for the observation
- scored.class: the predicted class for the observation (based on a threshold of 0.5)
- scored.probability: the predicted probability of success for the observation

Use the table() function to get the raw confusion matrix for this scored dataset. Make sure you understand the output. In particular, do the rows represent the actual or predicted class? The columns?

```{r create-confusion-matrix}
# Maybe rewrite confusion matrix?

# Creating the confusion matrix
confusion_matrix <- table(data_df$class, data_df$scored.class)

# Displaying the confusion matrix
print(confusion_matrix)

getParams <- function(df, actual, predicted) {
   # create confusion matrix from dataframe
  confusion_matrix <- table(df[[actual]], df[[predicted]])
  # break out matrix into vars
  tp <- confusion_matrix[2, 2] #True Positives: correctly predicted class=1 (positive class)
  tn <- confusion_matrix[1, 1] #True Negatives: correctly predicted class=0 (negative class)
  fp <- confusion_matrix[1, 2] #False Negatives: incorrectly predicted class=1 but was actually class=0
  fn <- confusion_matrix[2, 1] #False Negatives: incorrectly predicted class=0 but was actually class=1
  
  return(c(tp = tp, tn = tn, fp = fp, fn = fn))
}
```

This can be interpreted as:

True Negatives (TN): 119 – These are the instances where the actual class was 0, and the predicted class was also 0.
False Positives (FP): 5 – These are the instances where the actual class was 0, but the predicted class was 1.
False Negatives (FN): 30 – These are the instances where the actual class was 1, but the predicted class was 0.
True Positives (TP): 27 – These are the instances where the actual class was 1, and the predicted class was also 1.



### Accuracy

Accuracy measures the overall correctness of the model, i.e., the proportion of correct predictions (both true positives and true negatives) out of all predictions made.

_Formula_
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = (𝑇𝑃 + 𝑇𝑁) / (𝑇𝑃 + 𝐹𝑃 + 𝑇𝑁 + 𝐹𝑁)


```{r accuracy-function}

calcAccuracy <- function(df, actual, predicted) {
  # get confusion matrix values
  cm <- getParams(df, actual, predicted)
  # calculate value from formula
  val <- (cm["tp"] + cm["tn"]) / (cm["tp"] + cm["fp"] + cm["tn"] + cm["fn"])
  return(as.numeric(val))
}

```


### Classification Error

Classification error measures the proportion of incorrect predictions (false positives and false negatives) out of all predictions.

_Formula_
𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑐𝑎𝑡𝑖𝑜𝑛 𝐸𝑟𝑟𝑜𝑟 𝑅𝑎𝑡𝑒 = (𝐹𝑃 + 𝐹𝑁) / (𝑇𝑃 + 𝐹𝑃 + 𝑇𝑁 + 𝐹𝑁)


```{r classification-error-function }

calcClassificationError <- function(df, actual, predicted) {
  # get confusion matrix values
  cm <- getParams(df, actual, predicted)
  
  # calculate value from formula
  val <- (cm["fp"] + cm["fn"]) / (cm["tp"] + cm["fp"] + cm["tn"] + cm["fn"])
  return(as.numeric(val))
}

```



5. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions.

### Precision

Precision is the proportion of positive predictions that are actually correct.

_Formula_
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇𝑃 /  (𝑇𝑃 + 𝐹𝑃)


```{r precision-function }

calcPrecision <- function(df, actual, predicted) {
  # get confusion matrix values
  cm <- getParams(df, actual, predicted)
  # calculate value from formula
  val <- cm["tp"] / (cm["tp"] + cm["fp"])
  return(as.numeric(val))
}

```


### Sensitivity

Sensitivity tells you how well the model is at identifying positive cases.  

Sensitivity is true positives properly predicted as positive divided by true positive cases (class = 1). "Class" here is the real diagnosis (for example, having diabetes) and "scored.class" is the diagnosed case. Sensitivity being high implies that most of the real diabetic patients were appropriately identified, keeping false negatives in check. It is particularly vital in medicine where failing to detect may mean belated treatment.
 

_Formula_
𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦 = 𝑇𝑃 / (𝑇𝑃 + 𝐹𝑁)

```{r sensitivity-function }

calcSensitivity <- function(df, actual, predicted) {
  # get confusion matrix values
  cm <- getParams(df, actual, predicted)
  # calculate value from formula
  val <- cm["tp"] / (cm["tp"] + cm["fn"])
  return(as.numeric(val))
}

```




### Specificity

Specificity measures how well the model is at identifying the negative cases. 

Specificity is the ratio of true negative cases (class = 0) that are actually predicted as negative. In that sense, it would be properly predicting those not diabetic. Having high specificity reduces false positives, and in screening for medical issues, avoiding unnecessary intervention or treatment is crucial.

_Formula_
𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦 = 𝑇𝑁 / (𝑇𝑁 + 𝐹𝑃)

```{r specificity-function }

calcSpecificity <- function(df, actual, predicted) {
  # get confusion matrix values
  cm <- getParams(df, actual, predicted)
  # calculate value from formula
  val <- cm["tn"] / (cm["tn"] + cm["fp"])
  return(as.numeric(val))
}

```




### F1 Score 

F1 Score is high when both precision and recall are high. It is particularly useful when the class distribution is imbalanced (i.e., one class is much more frequent than the other), as it takes both false positives and false negatives into account.

F1 score is a trade-off between precision and sensitivity, useful to evaluate general model performance for diabetes detection. Since our dataset can be imbalanced (i.e., fewer positive examples compared to negative examples), the F1 score compromised correct positive detection with minimal false alarms.

_Formula_
𝐹1 𝑆𝑐𝑜𝑟𝑒 = 2 × ( 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 × 𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦) / (𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦)


```{r f1-score-function }

calcF1Score <- function(df, actual, predicted) {
  precision   <- calcPrecision(df, actual, predicted)
  sensitivity <- calcSensitivity(df, actual, predicted)
  
  # calculate value from formula
  val <- 2 * (precision * sensitivity) / (precision + sensitivity) 
  return(as.numeric(val))
}

```


9. Before we move on, let’s consider a question that was asked: What are the bounds on the F1 score? Show 
that the F1 score will always be between 0 and 1. (Hint: If 0 < 𝑎 < 1 and 0 < 𝑏 < 1 then 𝑎𝑏 < 𝑎.)

_The bounds for the F1 score are 0 and 1._ 

The formula for the F1 score is
__2 * (precision * sensitivity) / (precision + sensitivity)__
where precision = TP/(TP + FP) and sensitivity = TP/(TP + FN)

When 100% of our sample observations had a True Positive result the maximum value for TP is 1. If a 100% of the observations are True Positives, then none (0%) of the observations can be False Positive or False Negative. Therefore, TP is 1: 

precision = 1 / (1 - 0) = 1 
-and-
sensitivity = 1 / (1 - 0) = 1
-and-
F1_score = 2 * (1 * 1) / (1 + 1) = 2 * (1/2) = 1

Conversely, when 0% of the observations are True Positive, our numerators for precision and sensitivity will be zero (0) and therefore the numerator for our F1 score will also be zero

precision = 0 / (0 + FP) = 0 
-and-
sensitivity = 0 / (0 + FN) = 0
-and-
F1_score = 
2 * (0 * precision) / (0 + precision) = 2 * (0/2) = 0 
-or-
2 * (0 * sensitivity) / (0 + sensitivity) = 2 * (0/2) = 0 


10. Write a function that generates an ROC curve from a data set with a true classification column (class in our example) and a probability column (scored.probability in our example). Your function should return a list that includes the plot of the ROC curve and a vector that contains the calculated area under the curve (AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals.


or the ROC curve itself, the essential elements are:

True Positive Rate (TPR) = TP / (TP + FN) = sensitivity
False Positive Rate (FPR) = FP / (FP + TN) = 1 - specificity_val

```{r roc-curve}

generate_manual_roc <- function(data, actual_col = "class", prob_col = "scored.probability") {
  thresholds <- seq(0, 1, by = 0.01)
  TPR <- numeric(length(thresholds))
  FPR <- numeric(length(thresholds))
  
  for (i in seq_along(thresholds)) {
    threshold <- thresholds[i]
    predicted <- ifelse(data[[prob_col]] >= threshold, 1, 0)
    
    
    cm <- table(factor(predicted, levels = c(0, 1)), 
                factor(data[[actual_col]], levels = c(0, 1)))
    
    TP <- cm["1", "1"]
    TN <- cm["0", "0"]
    FP <- cm["1", "0"]
    FN <- cm["0", "1"]
    
    TPR[i] <- if ((TP + FN) == 0) 0 else TP / (TP + FN)
    FPR[i] <- if ((FP + TN) == 0) 0 else FP / (FP + TN)
  }
  
  
  plot(FPR, TPR, type = "l", col = "blue", lwd = 2,
       xlab = "False Positive Rate (1 - Specificity)", 
       ylab = "True Positive Rate (Sensitivity)",
       main = "Custom ROC Curve (Manual Method)")
  abline(0, 1, col = "gray", lty = 2)
  
  
  auc_manual <- abs(sum(diff(FPR) * (head(TPR, -1) + tail(TPR, -1)) / 2))
  
  return(list(AUC = auc_manual, TPR = TPR, FPR = FPR))
}

roc_manual <- generate_manual_roc(df)
roc_manual$AUC

```


To demonstrate how an ROC curve is constructed, a manual function was created that calculates the true positive rate and false positive rate across a sequence of thresholds ranging from 0 to 1. At each threshold, the function generates a new set of predicted classes and recalculates the confusion matrix to capture changes in sensitivity and specificity. After collecting all the points, the trapezoidal method was used to estimate the area under the curve (AUC), which provides a single measure of the model’s ability to distinguish between the two classes. The function and model shows how small changes in the threshold can significantly impact the balance between true and false positives.

11. Use your created R functions and the provided classification output data set to produce all of the 
classification metrics discussed above.

```{r}


sensitivity_val <- calcSensitivity(data_df, "class", "scored.class")
print(sensitivity_val)

specificity_val <- calcSpecificity(data_df, "class", "scored.class")
print(specificity_val)

accuracy_val <- calcAccuracy(data_df, "class", "scored.class")
print(accuracy_val)

classification_error <- calcClassificationError(data_df, "class", "scored.class")
print(classification_error)

print(accuracy_val + classification_error)

precision_val <- calcPrecision(data_df, "class", "scored.class")
print(precision_val)

f1_score <- calcF1Score(data_df, "class", "scored.class")
print(f1_score)

getRoc(data_df, "class")
```

12. Investigate the caret package. In particular, consider the functions confusionMatrix, sensitivity, and specificity. Apply the functions to the data set. How do the results compare with your own functions?

```{r caret}

library(caret)

caret_results <- confusionMatrix(as.factor(df$scored.class), as.factor(df$class), positive = "1")

print(caret_results)
```

To compare the manually calculated classification metrics with a built-in method,  the caret package and its confusionMatrix() function were used. This function efficiently returns a detailed summary of model performance, including accuracy, sensitivity, specificity, precision, and more. It also provides additional statistics like Kappa and balanced accuracy, which help to interpret the model’s effectiveness beyond just standard metrics. 

13. Investigate the pROC package. Use it to generate an ROC curve for the data set. How do the results 
compare with your own functions?

```{r}

library(pROC)

roc_pROC <- roc(df$class, df$scored.probability)

plot(roc_pROC, main = "ROC Curve using pROC", col = "red", lwd = 2)
abline(0, 1, col = "gray", lty = 2)

auc(roc_pROC)
```

For this question, the pROC package was used to generate an ROC curve and calculate the AUC using built-in functions. The curve produced by pROC is nearly identical to the one generated by the manual function, and the AUC values were nearly the same. While writing the function by hand was valuable for learning the underlying mechanics of ROC analysis, using pROC is clearly more efficient and practical for real-world work.