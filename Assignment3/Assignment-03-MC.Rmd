---
title: "D621 - Assignment 3"
author: "Marco Castro"
date: "2025-03-07"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


### Variables in the Dataset:

‚Ä¢ zn: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)  
‚Ä¢ indus: proportion of non-retail business acres per suburb (predictor variable)  
‚Ä¢ chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)  
‚Ä¢ nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)  
‚Ä¢ rm: average number of rooms per dwelling (predictor variable)  
‚Ä¢ age: proportion of owner-occupied units built prior to 1940 (predictor variable)  
‚Ä¢ dis: weighted mean of distances to five Boston employment centers (predictor variable)  
‚Ä¢ rad: index of accessibility to radial highways (predictor variable)  
‚Ä¢ tax: full-value property-tax rate per $10,000 (predictor variable)  
‚Ä¢ ptratio: pupil-teacher ratio by town (predictor variable)  
‚Ä¢ lstat: lower status of the population (percent) (predictor variable)  
‚Ä¢ medv: median value of owner-occupied homes in $1000s (predictor variable)  
‚Ä¢ target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)  

```{r global_options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(ggplot2)
library(miscTools)
library(GGally)
library(DataExplorer)
library(lmtest)
#library(car)
library(ggpubr)

library(arm)
library(regclass)
library(corrplot)
library(psych)
library(broom)
library(vcd)
library(vcdExtra)
library(MASS)
library(factoextra)

library(caret)
library(pROC)


# Load required packages
library(htmltools)
library(dplyr)
library(skimr)
require(miscTools)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 

```

```{r import-data, echo=FALSE}

df_training  <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-training-data_modified.csv")
df_eval <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-evaluation-data_modified.csv")

```

## DATA EXPLORATION

### Exploratory Data Analysis

```{r}
#dim(crime_training_df)
skim(crime_training_df)

```

1. Missing Values & Completeness Rate
The n_missing column shows that there are no missing values (0) for any variable, meaning we don‚Äôt need to perform imputation.
The complete_rate column confirms this, as all variables have a completeness rate of 1, meaning every row has a value for these variables.

2. Descriptive Statistics

Mean (mean): The average value of each variable.
Standard Deviation (sd): Measures the spread of values.
Minimum (p0): The lowest observed value (0th percentile).
First Quartile (p25): The 25th percentile, where 25% of the values are below this number.
Median (p50): The 50th percentile (the middle value).
Third Quartile (p75): The 75th percentile, where 75% of the values are below this number.

Key Observations:

Crime Rate Target (target)

The median (p50) is 0, indicating that more than half of the data points fall in the low-crime category (target = 0).

Median Home Value (medv)
Mean = 22.59 ($22,590 in $1000s), Median = 21.2.
The range (p0 = 5, p75 = 25) suggests that most homes are valued between $5,000 and $25,000 (in $1000s).
The standard deviation (9.23) indicates a relatively high spread in home values.

Lower Status Population (lstat)
Mean = 12.63%, Median = 11.93%.
A positively skewed distribution (p0 = 1.73, p75 = 16.93), meaning some areas have much higher lower-status populations than others.

Property Tax Rate (tax)
High variance (Mean = 409.5, SD = 167.9).
Large difference between the 25th percentile (281) and 75th percentile (666), suggesting significant variability in tax rates among neighborhoods.

Average Number of Rooms (rm)
Mean = 6.29, Median = 6.21, with a relatively small spread (SD = 0.70).
Indicates most homes have around 6 rooms.

Distance to Employment Centers (dis)
Median = 3.19, but the 25th percentile is quite low (2.10), meaning some neighborhoods are much closer to employment centers than others.
Higher standard deviation (2.1) suggests some neighborhoods are much more remote.

Industrial Land Proportion (indus)
Mean = 11.10, Median = 9.69, and right-skewed distribution (p0 = 0.46, p75 = 18.1).
Some areas have much higher proportions of industrial land, potentially influencing crime.

Highway Accessibility (rad)
Highly right-skewed: The median is 5, but the 75th percentile is 24, meaning some neighborhoods have much greater access to highways than others.
This might be an important predictor for crime.

Potential Data Transformations
zn, indus, tax, rad, lstat, and medv show skewness, so applying a log transformation might improve normality.
age can be categorized into bins (e.g., young, middle-aged, old) since it ranges from 2.9 to 94.1.
dis has a wide range, so normalization might be needed.

### Checking Binary Logistic Regression Assumptions

Before interpreting results from a binary logistic regression, we must verify three key assumptions:

Independence of Observations
Linearity of the Logit
No Multicollinearity

Now, let's discuss how to check these assumptions and which correlation method is best for your data (which contains both ordinal and continuous variables).

Pearson Correlation, Spearman Rank Correlation, and Kendall‚Äôs Tau Rank Correlation are all methods used to measure the strength and direction of relationships between variables.

However, they differ in terms of their assumptions, use cases, and how they quantify relationships.

Pearson Correlation: Suitable for continuous data when you want to measure linear associations.

Spearman Rank Correlation: Appropriate for both continuous and ordinal data. Particularly useful when the relationship is expected to be monotonic but not necessarily linear.

Kendall‚Äôs Tau Rank Correlation: Suitable for both continuous and ordinal (ranked) data. Useful when the data may not follow a linear relationship.

### Final Choice: Spearman's Rank Correlation
Since your data has both ordinal and continuous variables, Spearman's correlation is the best choice because:

It does not assume normality (unlike Pearson).
It can handle ordinal variables (like chas, rad).
It is more robust against outliers than Pearson.

### Checking Independence Assumption Using Spearman's Correlation in Binary Logistic Regression

#### What is the Independence Assumption?
The independence assumption in binary logistic regression states that each observation (row) in the dataset should be independent of the others. This means:

No duplicated data points (e.g., same neighborhood appearing multiple times).
No clustered observations (e.g., observations grouped by region, time, or other factors).
No strong correlations between residuals of observations, meaning observations do not systematically affect each other.

### Why Use Spearman‚Äôs Correlation?
Spearman's correlation measures monotonic relationships between variables, making it suitable when we have a mix of ordinal and continuous predictors.
If there is high correlation between observations, it suggests possible dependence (e.g., neighborhoods with similar crime rates).

### Compute Spearman's Correlation Between Observations
We calculate the Spearman correlation matrix for all numeric variables (excluding target), which tells us if some variables are highly dependent (correlated).

### 1. Independence (Visual Inspection)

```{r, warning = FALSE, message = FALSE}
library(ggplot2)
library(tidyr)
library(dplyr)

# Exclude categorical columns before pivoting
crime_long <- crime_training_df %>%
  dplyr::select(where(is.numeric)) %>%  # Keep only numeric columns
  pivot_longer(cols = -target, names_to = "Variable", values_to = "Value")

# Create scatter plots for each predictor vs target with a fitted line
ggplot(crime_long, aes(x = Value, y = target)) +
  geom_point(alpha = 0.5, color = "blue") +  # Scatter plot
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear fit
  facet_wrap(~Variable, scales = "free") +  # Multiple plots for each variable
  theme_minimal() +
  labs(title = "Independence Check: Scatter Plots of Predictors vs Target",
       x = "Predictor Value",
       y = "Crime Rate (Binary Target)")



```


```{r}
# Load necessary library
library(dplyr)

# Initialize an empty data frame to store results
independence_results <- data.frame(Variable = character(),
                                   Correlation = numeric(),
                                   P_Value = numeric(),
                                   stringsAsFactors = FALSE)

# Loop through each predictor variable (excluding the target)
for (var in colnames(crime_training_df)[colnames(crime_training_df) != "target"]) {
  
  # Ensure the variable is numeric before computing Spearman correlation
  if (is.numeric(crime_training_df[[var]])) {
    
    # Perform Spearman correlation test
    test_result <- cor.test(crime_training_df[[var]], crime_training_df$target, method = "spearman")
    
    # Store results in a data frame
    independence_results <- rbind(independence_results, 
                                  data.frame(Variable = var, 
                                             Correlation = test_result$estimate, 
                                             P_Value = test_result$p.value))
  }
}

# View results in tabular format
print(independence_results)

```
### Checking for Independence:
In Spearman‚Äôs method, we check the correlation between:

Each independent variable and the dependent variable (target)
If correlation is too low (|ùúå| < 0.1) and p-value > 0.05, the variable might not be useful in predicting the target.

Variables to Consider Removing:

chas (œÅ = 0.0800, p = 0.0843) ‚Üí No meaningful correlation with crime.
Possibly rm (œÅ = -0.1772, p = 0.00012) ‚Üí Weak correlation but could check its importance in the model.

Variables to Keep (for now, but monitor multicollinearity):

nox (œÅ = 0.7547)
age (œÅ = 0.6457)
dis (œÅ = -0.6591)
indus (œÅ = 0.6192)
rad, tax, ptratio (moderate correlation)
Transform / Create Interaction Terms:

Log transform: dis (since it has a strong negative correlation)
Categorize: age into "Young", "Middle-aged", "Old"
Interaction term: tax * rad (both impact crime)


```{r}
# Load necessary library
library(corrplot)
library(dplyr)

crime_training_df <- crime_training_df %>%
  mutate(across(where(is.character), as.numeric))

crime_evaluation_df <- crime_evaluation_df %>%
  mutate(across(where(is.character), as.numeric))

# Verify again
str(crime_training_df)

###  Visualize Correlation Matrix

# Compute Spearman correlation matrix
numeric_data <- crime_training_df %>% dplyr::select(where(is.numeric), -target)
spearman_cor <- cor(numeric_data, method = "spearman", use = "pairwise.complete.obs")

# Plot the upper triangle of the correlation matrix
corrplot::corrplot(
  spearman_cor,
  method = "color",                            # Use colored squares
  type = "upper",                              # Show only upper triangle
  tl.cex = 0.7,                                # Text label size
  tl.col = "black",                            # Text label color
  addCoef.col = "black",                       # Add correlation coefficients in black
  number.cex = 0.6,                            # Coefficient number size
  col = colorRampPalette(c("blue", "white", "red"))(200),  # Custom color scale
  diag = FALSE,                                # Hide diagonal
  order = "hclust"                             # Cluster similar variables
)



```


### Checking for Multicollinearity (High Correlation Between Predictors)

A general rule of thumb is that if |ùúå| > 0.7, it indicates strong correlation between variables, which can lead to multicollinearity in the logistic regression model.

From the matrix:
indus & nox (ùúå = 0.79) ‚Üí Strong positive correlation, meaning they provide redundant information.
tax & rad (ùúå = 0.70) ‚Üí These variables are highly correlated, indicating one may be removed.
lstat & medv (ùúå = -0.85) ‚Üí Very strong negative correlation; keeping both might be problematic.
log_medv & medv (ùúå = 1.00) ‚Üí Perfect correlation (since log transformation was applied), meaning one must be removed to prevent redundancy.

Final Takeaways:

High correlation between predictors (|ùúå| > 0.7) indicates potential multicollinearity. Consider removing indus, rad, or medv to improve model stability.
No evidence of entire rows/columns being highly correlated, suggesting no major independence violations.
Use VIF for confirmation and decide on variable selection accordingly.


### Checking Multicollinearity Using Variance Inflation Factor (VIF)

Variance Inflation Factor (VIF) helps quantify multicollinearity by measuring how much the variance of regression coefficients is inflated due to correlation among predictors. 
A VIF > 5 (or more conservatively, VIF > 10) suggests severe multicollinearity.

### Check VIF (Variance Inflation Factor)

Variance Inflation Factor (VIF) ‚Äì Measures how much variance in regression coefficients is inflated due to multicollinearity
ANd helps confirm whether these high correlations affect regression coefficients.

```{r}
library(car)
vif(glm(target ~ nox + age + rad + tax + dis + zn + medv, family = binomial, data = crime_training_df))

```
Conclusion
 There is NO severe multicollinearity (all VIF values are below 5).
 No immediate need to drop variables based on VIF.
 The variable dis (VIF = 3.58) shows moderate correlation with other predictors, but it's not problematic.
 
 Next Steps:

Keep all predictors in the model.
If we suspect redundancy, check pairwise correlations again or test removing dis to see if model performance improves.

### Checking the Assumption of Linearity of the Logit for Binary Logistic Regression

In logistic regression, we assume that each continuous predictor has a linear relationship with the log-odds (logit) of the target variable. 
If this assumption is violated, the model may be misleading or inaccurate.

```{r, warning = FALSE, message = FALSE}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)

# Fit the logistic regression model
model <- glm(target ~ nox + age + rad + tax + dis + zn + medv, 
             family = binomial, data = crime_training_df)

# Compute predicted probabilities
crime_training_df$predicted_prob <- predict(model, type = "response")

# Compute logit (log-odds) transformation
crime_training_df$logit <- log(crime_training_df$predicted_prob / 
                              (1 - crime_training_df$predicted_prob))

# Reshape data for faceting
plot_data <- crime_training_df %>%
  dplyr::select(logit, nox, age, rad, tax, dis, zn, medv) %>%
  pivot_longer(cols = -logit, names_to = "Predictor", values_to = "Value")

# Create faceted scatter plot
ggplot(plot_data, aes(x = Value, y = logit)) +
  geom_point(alpha = 0.5, color = "blue") + 
  geom_smooth(method = "loess", color = "red", se = FALSE) + 
  facet_wrap(~ Predictor, scales = "free") +
  theme_minimal() +
  labs(title = "Linearity of Logit Check for Binary Logistic Regression",
       x = "Predictor Variable",
       y = "Logit of Predicted Probability")

```
### Interpretation of the Linearity of Logit Check for Binary Logistic Regression

This faceted scatter plot assesses the assumption of linearity of the logit for binary logistic regression. 
The blue dots represent the relationship between each predictor variable and the logit of the predicted probability, 
while the red LOESS (Locally Estimated Scatterplot Smoothing) curve helps visualize patterns.

### Overall Conclusion
Several predictors (e.g., dis, medv, tax, zn) violate the linearity of logit assumption.

### Summary Stats

```{r explore-cols}

column_types <- sapply(df_training, class)
print(column_types)
```

The following three columns were imported as numerical but should be factors: - chas: binomial - rad: ordinal - target: binomial

```{r explore-summary}

# convert to factor
df_training <- df_training |>
  mutate(
    chas = as.factor(chas),
    rad = as.factor(rad),
    target = as.factor(target),
  )

numeric_cols <- c('zn', 'indus', 'nox', 'rm', 'age', 'dis', 'tax', 'ptratio', 'lstat', 'medv')

factor_cols <- c('chas', 'rad', 'target')

glimpse(df_training)

```

A closer examination of the `rad` data shows that that our observations have a rad index value of 1-8 or 24 in this column. Below are the counts: 

```{r count-rads, echo=FALSE}

rad_counts <- table(df_training$rad)

# Print the result
print(rad_counts) 
```

#### Means

We can now calculate the summary statistics for the numeric parameters in our dataframe, including our mean, median, min/max, and standard deviations.

```{r explore-means-medians}

# only show summary stats for numeric values
for (param in numeric_cols) {
  cat("\nSummary for", param, ":\n")
  print(describe(df_training[[param]]))
}
```

### Plots of data

#### Boxplots
Below is series of boxplots for all numeric parameters where target is our dependent variable.

```{r explore-boxplot}
# No dev.off() needed unless you're saving the plot

plot_boxplot(df_training, by = "target", title = "Boxplots of Target vs Param")

```

```{r iqrs}
df_training_hi_crime <- df_training |>
  filter(target == 1) |>
  subset(select = -c(chas, rad, target)) 

df_training_lo_crime <- df_training |>
  filter(target == 0) |>
  subset(select = -c(chas, rad, target)) 

cat("\nIQR for High Crime Neighborhoods\n")
summary(df_training_hi_crime)

cat("\nIQR for Low Crime Neighborhoods\n")
summary(df_training_lo_crime)
```

The boxplots show the distribution numerical parameters grouped by the dependent variable `target`. The plots are useful for getting a sense as to which parameters may be good predictors based on how different the parameter;s IQRs are. Conversely, similar IQRs may provide insight into which may not add much information to our model. Based on these box plots, we see that the IQR for `rm` are very similar where `target` is 0 and 1 and should be flagged for potential removal of our plot.  `ptratio` and `medv` have some overlap  All other variables appear somewhat 

Further, we see that param *zn* has a median value around zero, suggesting that few neighborhoods have residential areas zoned for large plots as shown below. We should also consider omitting this variable from our model down the line


```{r zn-zeros}
count_zeros <- sum(df_training_hi_crime$zn == 0)
cat("\nAbove Median Crime Rate Neighborhoods have ", count_zeros, " rows with a value of 0 for param zn out of ", nrow(df_training_hi_crime), "observations (",  
(count_zeros / nrow(df_training_hi_crime)), "%)\n")

count_zeros <- sum(df_training_lo_crime$zn == 0)
cat("\nBelow Median Crime Rate Neighborhoods have ", count_zeros, " rows with a value of 0 for param zn out of ", nrow(df_training_lo_crime), "observations (",  
(count_zeros / nrow(df_training_lo_crime)), "%)\n")

```


_Categorical Variables_

For our categorical variables, we can use barglaphs to get a sense of the parameter's impact on `target`. 


```{r chas-bargraphs}
df_training |>
  group_by(
    target,chas
  ) |>
  dplyr::summarise(
    count = n()
  ) |>
  ungroup() |>
  group_by(chas) |>
  mutate(
    percent = 100 * count / sum(count),
    label = paste0(round(percent),"%")
  ) |>
  ggplot() +
  aes(x = chas, y = count, label = label, fill=target) +
  geom_col() +
  geom_text(position = position_stack(0.5))
```

The bargraph for `chas` shows fairly equal values for 0 and 1 accross the `chas` values. This suggests that the variable will may have low impact on our model and we should consider removing it.

```{r rad-bargraphs}
### rad
df_training |>
  group_by(
    target,rad
  ) |>
  dplyr::summarise(
    count = n()
  ) |>
  ungroup() |>
  group_by(rad) |>
  mutate(
    percent = 100 * count / sum(count),
    label = paste0(round(percent),"%")
  ) |>
  ggplot() +
  aes(x = rad, y = count, label = label, fill=target) +
  geom_col() +
  geom_text(position = position_stack(0.5))
```

The bargraphs for `rad` are somewhat more revealing. They suggest a strong relationship between low `rad` index values of 1-3 and below median crime rate, while an index value of 24 (the highest rad index) has a strong relationship with above median crime rate.

#### Pairs

Using the pair function, we can print  scatterplots comparing each of the variables to the others. 

```{r pairs-simple}
png("scatterplot_matrix.png", width = 800, height = 800)

pairs(df_training, main="")
dev.off()
```

GGpairs plots take this a step further and show normal distribution and boxplots to get a fuller sense of how the data parameters relate to one another.

```{r ggpairs}

ggpairs(df_training)

```

### Missing data

The training set contains no missing values.

```{r explore-missing}

introduce(df_training)

missing_values_count <- sapply(data, function(x) sum(is.na(x)))
print(missing_values_count)

```


### Distribution
Scatterplots of y=`target` plotted against each of the parameters confirm that the dependent variable is binomial. Therefore, linear regression is not be the best fit for this data and we should explore logistic regression such as logit and probit. 

```{r explore-scatterplot}
# scatter plot doesn't show much
# plot_scatterplot(df_training, by = "target")
# plot_qq(df_training, sampled_rows = 1000L)
```


```{r}
plot_qq(df_training, by="target", sampled_rows = 1000L)

```

### Correlation

```{r correlation-predictors}
df_training |>
  subset(select=-c(target, chas)) |>
  plot_correlation(type = "all")
```


## DATA PREPARATION


### Data Preparation

This step involves cleaning and transforming data to improve model performance.

Tasks:
Handle missing values:
If missing values exist, replace with the mean/median or use imputation methods.

Feature engineering:
Log transformations: medv, lstat, and dis might benefit from log transformations to reduce skewness.
Binning: Convert age into categorical buckets (e.g., young, middle-aged, old).
Interactions: Create new features (e.g., lstat*medv to capture relationships between income and home values).
Standardization: Normalize numerical variables to bring them to the same scale.

```{r}
# Handle missing values (if any)


# Fill missing values with median
crime_training_df$medv[is.na(crime_training_df$medv)] <- median(crime_training_df$medv, na.rm = TRUE)
crime_training_df$lstat[is.na(crime_training_df$lstat)] <- median(crime_training_df$lstat, na.rm = TRUE)
crime_training_df$dis[is.na(crime_training_df$dis)] <- median(crime_training_df$dis, na.rm = TRUE)

# Log transformation


crime_training_df <- crime_training_df %>%
  mutate(
    log_medv = log(medv + 1),  # Avoid log(0)
    log_lstat = log(lstat + 1),
    log_dis = log(dis + 1)
  )



#Standardization (Normalize Continuous Variables)
#Standardization ensures that variables are on the same scale, improving model stability.
crime_training_df <- crime_training_df %>%
  mutate(
    zn_scaled = as.numeric(scale(zn)),
    indus_scaled = as.numeric(scale(indus)),
    nox_scaled = as.numeric(scale(nox)),
    rm_scaled = as.numeric(scale(rm)),
    age_scaled = as.numeric(scale(age)),
    dis_scaled = as.numeric(scale(dis)),
    rad_scaled = as.numeric(scale(rad)),
    tax_scaled = as.numeric(scale(tax)),
    ptratio_scaled = as.numeric(scale(ptratio))
  )


# Create categorical age groups bins
crime_training_df$age_group <- cut(crime_training_df$age, breaks=c(0, 30, 60, 100), labels=c("Young", "Middle-aged", "Old"))

```


```{r}
# Create Interaction Terms
crime_training_df <- crime_training_df %>%
  mutate(
    lstat_medv_interact = log_lstat * log_medv,  # Income & housing price interaction
    tax_rad_interact = tax * rad                # Tax burden & highway accessibility
  )

colnames(crime_training_df)
```
### Building Models

Applying three logistic regression models using different variable selections or transformations.

Approach:
Model 1 (Baseline Model): Use all predictors.
Model 2 (Stepwise Selection): Use stepAIC() to select important variables.
Model 3 (Transformed Variables): Use transformed and interaction variables.

```{r, warning = FALSE, message = FALSE}
# Load library
library(MASS)

# Logistic Regression Model 1 (Baseline)
model1 <- glm(target ~ chas + lstat + medv + log_medv + log_lstat + log_dis +
                        zn_scaled + indus_scaled + nox_scaled + rm_scaled + age_scaled + dis_scaled +
                        rad_scaled + tax_scaled + ptratio_scaled +
                        lstat_medv_interact + tax_rad_interact + age_group,
              data = crime_training_df, family = binomial)

# Summary of models
summary(model1)

AIC(model1)  # Compare AIC
```


```{r}

# Logistic Regression Model 2 (Stepwise Selection)
model2 <- stepAIC(glm(target ~ ., data=crime_training_df, family=binomial), direction="both")

summary(model2)

AIC(model2)  # Compare AIC

```

```{r}

# Logistic Regression Model 3 (With Transformations & Interactions)
model3 <- glm(target ~ log_medv + lstat + nox + ptratio + age_group, data=crime_training_df, family=binomial)

summary(model3)

AIC(model3)  # Compare AIC


```

### Model Evaluation:

Evaluate model performance and select the best model based on multiple criteria.

Evaluation Metrics:
Accuracy: (TP + TN) / (TP + TN + FP + FN)
Precision: TP / (TP + FP)
Recall (Sensitivity): TP / (TP + FN)
Specificity: TN / (TN + FP)
F1 Score: 2 * (Precision * Recall) / (Precision + Recall)
AUC-ROC Curve: Evaluate model discrimination.


```{r}
# Load necessary library
library(caret)
library(pROC)

# Predict on training data
pred_probs <- predict(model1, type="response")
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)

# Confusion Matrix
conf_matrix <- table(Predicted=pred_classes, Actual=crime_training_df$target)
print(conf_matrix)

# Compute accuracy, precision, recall, F1-score
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- conf_matrix[2,2] / sum(conf_matrix[,2])
recall <- conf_matrix[2,2] / sum(conf_matrix[2,])
f1_score <- 2 * (precision * recall) / (precision + recall)
f1_score

# Print performance metrics
cat("Accuracy:", accuracy, "\nPrecision:", precision, "\nRecall:", recall, "\nF1 Score:", f1_score)

```

# Predict Probabilities for Training Data

```{r}
pred_prob_model1 <- predict(model1, type = "response")
pred_prob_model2 <- predict(model2, type = "response")
pred_prob_model3 <- predict(model3, type = "response")

# Convert probabilities into binary classifications (Threshold = 0.5)
pred_class_model1 <- ifelse(pred_prob_model1 > 0.5, 1, 0)
pred_class_model2 <- ifelse(pred_prob_model2 > 0.5, 1, 0)
pred_class_model3 <- ifelse(pred_prob_model3 > 0.5, 1, 0)

```

### Compute Confusion Matrices

```{r}
conf_matrix_model1 <- table(Predicted = pred_class_model1, Actual = crime_training_df$target)
conf_matrix_model2 <- table(Predicted = pred_class_model2, Actual = crime_training_df$target)
conf_matrix_model3 <- table(Predicted = pred_class_model3, Actual = crime_training_df$target)

print(conf_matrix_model1)
print(conf_matrix_model2)
print(conf_matrix_model3)

```
### Evaluate Performance Metrics

```{r}
# Define a function to compute accuracy, precision, recall, and F1-score
evaluate_model <- function(conf_matrix) {
  accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
  precision <- conf_matrix["1", "1"] / sum(conf_matrix[,"1"])
  recall <- conf_matrix["1", "1"] / sum(conf_matrix["1",])

  f1_score <- 2 * ((precision * recall) / (precision + recall))
  
  return(list(Accuracy = accuracy, Precision = precision, Recall = recall, F1_Score = f1_score))
}

eval_model1 <- evaluate_model(conf_matrix_model1)
eval_model2 <- evaluate_model(conf_matrix_model2)
eval_model3 <- evaluate_model(conf_matrix_model3)

print(eval_model1)
print(eval_model2)
print(eval_model3)

```

### Compute ROC & AUC for Models

```{r}
roc_model1 <- roc(crime_training_df$target, pred_prob_model1)
roc_model2 <- roc(crime_training_df$target, pred_prob_model2)
roc_model3 <- roc(crime_training_df$target, pred_prob_model3)
roc_model1
roc_model2
roc_model3

```
```{r}
# Plot ROC curves for comparison
plot(roc_model1, col="red", main="ROC Curves for Logistic Models")
plot(roc_model2, col="blue", add=TRUE)
plot(roc_model3, col="green", add=TRUE)
legend("bottomright", legend=c("Model 1", "Model 2", "Model 3"), col=c("red", "blue", "green"), lwd=2)


```
Model	Accuracy	Precision	Recall	F1 Score	AUC
Model 1	0.933   0.925	    0.938	  0.931	    0.9787
Model 2	0.927	  0.917	    0.933	  0.925	    0.9776
Model 3	0.852	  0.830	    0.864	  0.846	    0.9472

Best Choice: Model 1
Model 1 is the best choice because:

Highest Accuracy (0.933) ‚Üí It makes the fewest classification errors.
Highest F1 Score (0.931) ‚Üí It balances precision and recall effectively.
Highest AUC (0.9787) ‚Üí It has the best ability to distinguish between classes

Model 1 is the most reliable, most precise, and best at distinguishing high-crime neighborhoods. 
Model 2 performs well but has slightly lower Accuracy (0.927) and AUC (0.9776).
Model 3 performs significantly worse, with much lower Accuracy (0.852) and AUC (0.9472).

What Does "Fewest Classification Errors" Mean?
A classification error happens when the model predicts the wrong category:

False Negative (FN) ‚Üí Model predicts 0 (low crime), but actual crime rate is high (1).
False Positive (FP) ‚Üí Model predicts 1 (high crime), but actual crime rate is low (0).
Higher accuracy means fewer FPs & FNs ‚Üí More correct predictions.

Accuracy measures overall correctness of the classification model.
It represents the proportion of correct predictions (both positive and negative) out of all predictions.

Accuracy= (TP+TN)/(TP+FP+TN+FN)

### Why Is This Important in Crime Prediction?

 False Negatives Are Dangerous (Crime exists but is predicted as "safe")
 Consequence: A high-crime area is misclassified as low-crime ‚Üí Authorities don‚Äôt take necessary action.

 Police resources may not be allocated where crime is actually high.
 Residents feel falsely safe but face real crime threats.
 Crime could increase due to lack of intervention.
 False Positives Cause Unnecessary Fear & Resource Waste (Safe area misclassified as high-crime)
 Consequence: A low-crime area is misclassified as high-crime ‚Üí Unnecessary actions are taken.

 Increased policing in safe areas, wasting law enforcement resources.
 Real estate values may drop due to incorrect classification.
 Residents may feel unsafe, even though crime is low.
 
 Bottom Line: Highest accuracy (0.938) = Best model for crime prevention
 
### What Does the Highest F1 Score (0.937) Mean?

F1 Score of 0.931 means that model balances Precision and Recall very well.
 F1 = 2 x (Precision x Sensitivity)/(Precision + Sensitivity)

Precision ("When the model says high crime, is it actually high crime?")
High Precision = Few False Positives (FP) ‚Üí Model doesn‚Äôt falsely label safe areas as high crime.

Recall ("Did the model find all the high-crime areas?")
High Recall = Few False Negatives (FN) ‚Üí Model doesn‚Äôt miss actual high-crime areas.

A high F1 Score (0.937) means:
Few False Positives (Safe areas aren‚Äôt wrongly classified as high crime)
Few False Negatives (Actual high-crime areas are correctly identified)
The model is well-balanced and reliable.

### What Does the Highest AUC (0.9782) Mean?

ROC (Receiver Operating Characteristic) curve is a graph that shows how well a model can separate two classes (e.g., high-crime vs. low-crime areas).
AUC (Area Under the Curve) measures how well your model separates two classes of high-crime (1) vs. low-crime (0) areas.

A high AUC (0.9782) means:

Model correctly distinguishes between crime-prone and safe areas most of the time.
It rarely confuses high-crime neighborhoods with low-crime ones.
It performs better than random guessing (which has an AUC of 0.5).
In simple terms: AUC = 0.9782, meaning model is very effective at separating high-crime and low-crime areas! 




### Compare AIC and BIC

```{r}
cat("Model 1 AIC:", AIC(model1), " | BIC:", BIC(model1), "\n")
cat("Model 2 AIC:", AIC(model2), " | BIC:", BIC(model2), "\n")
cat("Model 3 AIC:", AIC(model3), " | BIC:", BIC(model3), "\n")

```
### Compute McFadden‚Äôs R¬≤ (using pscl package)

```{r}
# Install if needed: install.packages("pscl")
library(pscl)

cat("Model 1 McFadden's R¬≤:", pR2(model1)["McFadden"], "\n")
cat("Model 2 McFadden's R¬≤:", pR2(model2)["McFadden"], "\n")
cat("Model 3 McFadden's R¬≤:", pR2(model3)["McFadden"], "\n")

```
```{r}
# Correct way to extract AUC values
auc_model1 <- roc_model1$auc
auc_model2 <- roc_model2$auc
auc_model3 <- roc_model3$auc

model_metrics <- tibble(
  Model = c("Model 1", "Model 2", "Model 3"),
  AIC = c(AIC(model1), AIC(model2), AIC(model3)),
  BIC = c(BIC(model1), BIC(model2), BIC(model3)),
  McFadden_R2 = c(pR2(model1)["McFadden"], 
                  pR2(model2)["McFadden"], 
                  pR2(model3)["McFadden"]),
  Accuracy = c(eval_model1$Accuracy, eval_model2$Accuracy, eval_model3$Accuracy),
  F1_Score = c(eval_model1$F1_Score, eval_model2$F1_Score, eval_model3$F1_Score),
  AUC = c(auc_model1, auc_model2, auc_model3)
)

print(model_metrics)


```

### Key Metrics

AIC (Akaike Information Criterion) -> Model fit with penalty for complexity -> Lower is better

BIC (Bayesian Information Criterion) -> Like AIC but penalizes complexity more heavily -> Lower is better

McFadden's R¬≤ (a type of pseudo-R¬≤ for logistic regression) -> Pseudo R¬≤: measures improvement over null model -> Higher is better

Accuracy -> Overall correctness (TP+TN)/(Total) -> Higher is better

F1 Score -> Balance of Precision and Recall -> Higher is better

AUC (Area Under the ROC Curve) -> Classifier‚Äôs ability to distinguish classes (ROC) -> Closer to 1 is better

### Interpretation of Results

### Model 1
Best in predictive performance across the board: precision, recall, F1, AUC.
Slightly higher AIC/BIC ‚Üí a bit more complex (more variables/features).
Most accurate and reliable in identifying high-crime areas, which is critical.

### Model 2
Very close in performance to Model 1, just slightly lower on most metrics.
Lower AIC/BIC, meaning it's simpler and less prone to overfitting.
If model interpretability or computation is a concern, this is a great trade-off.

### Model 3
Clearly underperforms across all metrics.
Lower accuracy (0.85), lower precision and recall, and worse McFadden‚Äôs R¬≤.
Best to discard this model for this task.

### Recommended Model: Model 1

Highest F1 Score (0.9319): best balance of precision & recall.
Highest Recall (0.9381): means it catches nearly all crime areas.
Highest AUC (0.9787): best ability to distinguish between classes.
Best McFadden‚Äôs R¬≤ (0.730): indicates strong model fit.
Even though AIC/BIC is a bit higher, the gain in performance justifies the complexity, especially in a high-stakes domain like crime prediction.


Definitions:
Prediction Error: Usually calculated as the Misclassification Rate on a test set (or the same training set if test set isn't available):

Prediction Error= Number of Incorrect Predictions/Total Prediction


Cross-Validation Error (CV Error): Estimated using cv.glm() in R. The function returns:

delta[1]: raw cross-validation error estimate
delta[2]: adjusted cross-validation error (accounts for model bias, preferred for reporting)

### Calculate Cross-Validation Error and Pridiction Error for all 3 models:

```{r}
library(boot)


# Assuming these are your model objects:
# model1, model2, model3

cv_model1 <- cv.glm(data = crime_training_df, glmfit = model1, K = 10)
cv_model2 <- cv.glm(data = crime_training_df, glmfit = model2, K = 10)
cv_model3 <- cv.glm(data = crime_training_df, glmfit = model3, K = 10)

# Extract adjusted CV errors (delta[2] = bias-corrected estimate)
adjusted_cv_error_model1 <- cv_model1$delta[2]
adjusted_cv_error_model2 <- cv_model2$delta[2]
adjusted_cv_error_model3 <- cv_model3$delta[2]

adjusted_cv_error_model1
adjusted_cv_error_model2
adjusted_cv_error_model3

```



```{r}
# Model 1
total1 <- 223 + 17 + 14 + 212
incorrect1 <- 17 + 14
prediction_error_model1 <- incorrect1 / total1

# Model 2 (assuming similar structure, update numbers accordingly)
total2 <- 222 + 19 + 15 + 210
incorrect2 <- 19 + 15
prediction_error_model2 <- incorrect2 / total2

# Model 3
total3 <- 207 + 39 + 30 + 190
incorrect3 <- 39 + 30
prediction_error_model3 <- incorrect3 / total3

prediction_error_model1
prediction_error_model2
prediction_error_model3

```
### Best Model Selection

Model 1 is selected because:
‚Ä¢	Highest Accuracy ‚Üí Least classification errors overall.
‚Ä¢	Highest F1 Score ‚Üí Best balance between Precision & Recall.
‚Ä¢	Highest AUC ‚Üí Best at distinguishing high vs. low crime.
‚Ä¢	Highest McFadden‚Äôs R¬≤ ‚Üí Strongest model fit.
‚Ä¢	Low Prediction & CV Error ‚Üí Very reliable.

### Trade-off:

‚Ä¢	AIC and BIC slightly higher than Model 2 due to more features.
‚Ä¢	But performance gain justifies the added complexity, especially for critical domains like crime detection.

### Real-World Importance

‚Ä¢	False Negatives (FN): Predicting a low-crime area when it's actually high-crime.
‚Ä¢	Can lead to under-policing and increased crime.
‚Ä¢	Model 1 minimizes False Negatives best, making it ideal for crime prevention







### Fixing missing values
Luckily, there are no missing values in the training set.

### Transforming data by bucketing and combining variables

The variable `rad` contains an ordinal factor that represents an index of accessibility to radial highways with values ranging from 1-24. A count of the rad values reveals that the `rad` column contains only values 1-8 and 24. This data set does not include any rows with a `rad` value of 9-23. 

Since this column is contains values for an index value where 1 is assigned to neighborhoods with the poorest accessibility to a highway and 24 is assigned to neighborhoods with the most accessibility, we can simplify our variables by binning our rad values. Here we are using quantiles to bin the values into three buckets of nearly equal sizes for low, moderate and high accessibility. This method ensures a more balanced distribution of rows across the bins over using equal sized bins  (1-8, 9-16, 17-24). This especially useful when the data is not uniformly distributed across the range such as in our case where we do not have any rad values of 1-23.


```{r bin-rads}
rad_counts
quantile_breaks <- quantile(as.numeric(df_training$rad), probs = c(0, 1/3, 2/3, 1))

df_training$radq <- cut(as.numeric(df_training$rad),
                  breaks = quantile_breaks,  
                  labels = c('_low', '_mid', '_hi'),
                   include.lowest = TRUE, 
                   right = TRUE)  

table(df_training$radq)
```

While the `glm` function should automatically perform one-hot encoding to factors, we should consider one-hot encoding on the `rad_quantile` parameter to perform other operations, such as calculating correlation using the spearman test.

We will drop one of the one-hot encoded params as the presence of this additional param will result in correlation issues down the line. `radq_mid` was selected, as it seemed to have the most mixed results in our plots above.

```{r one-hot-encoding}
# one-hot encode rad values 
rad_one_hot <- model.matrix(~ radq - 1, data = df_training)

# combine new columns
df_training_one_hot <- cbind(df_training[ , !names(df_training) %in% "rad"], rad_one_hot) |>
  subset(select=-c(radq, radq_mid))

glimpse(df_training_one_hot)
```

We will use the one-hot encoded dataframe to diagnose a preliminary model with all of the predictors.

```{r full-model}
model_full <- glm(target ~., binomial(link = "logit"), data=df_training_one_hot)
summary(model_full)

```

Reviewing the summary statistics for full model indicates that the variable `indus`, `nox`, `dis`, and `radq_hi` has very strong statistically signification. Two additional variables, `medv`, `dis` and `radq_mid`, have high statistical significance while `zn` has weak statistical significance. `chas1`, `rm`, `age`, `tax`, `ptratio` and `lstat` have weak statistical significance values.

__Note: had we one-hot encoded all of the values for `rad` instead of binning them first, all `rad` params would have very weak statistical significance, as their p-values are nearly 1.0.__ 


### Multicollinearity

To test if correlation exists between the dependent and independent variables, we used a Pearson's Correlation test. The function below loops through each of our columns and prints out the correlation of the dependent variable `target` with each of the predictors. For predictors where Pearson's Correlation coefficient is close to zero, we can determine that collinearity does not exist.



```{r check-for-correlation}
# is above .7 would be too highly correlated
cor_results <- data.frame(name = character(0), value = numeric(0))
for (param in colnames(df_training_one_hot)) {
  cat("\nPearson Test score for", param, ":\n")
  x <- as.numeric(df_training_one_hot$target)
  y <- as.numeric(df_training_one_hot[[param]])
  pearsons <- cor.test(x, y, method = "pearson")
  print(pearsons)
  # calc pearson cor value only
  cor_object <-  data.frame(name = param, value = cor(x, y))
  assign("cor_results", rbind(cor_results, cor_object), envir = .GlobalEnv)
}

print(cor_results)
```

#### Correlation Clusters

Next we can visualize the correlations in clusters.

```{r check-for-correlation-clusters, echo=FALSE}

df_training_subset <- df_training_one_hot |>
  subset(select = -c(target)) |>
  mutate(chas = as.numeric(chas))

df_training_cor <- round(cor(df_training_subset, method = c("spearman")), 3)

par(mar = c(5, 4, 4, 2) + 0.1)
corrplot(df_training_cor, method="shade", order="hclust", addrect=4)

#df_training_cor |>  plot_correlation(type = "all")
```

Using "hclust", our corrplot shows four distinct groups, each with strong correlation between the parameters within each group. This suggests that we may want to select specific parameters from within these groups or conduct principal component analysis on each of these groups.


#### Variance Inflation Factor
```{r}
car::vif(model_full) |> sort()
```

A VIF test suggests that we should remove nox and medv.

### Presence of Outliers

We can examine our diagnostic plots to find potential outliers and leverage. First we will examine the Cook‚Äôs Distance and Cook‚Äôs Distance vs Leverage plots. Cook‚Äôs Distance measures the influence of an observation on the fitted values of the model.  

```{r cooks-d, echo=FALSE}

# use augment to get .cooksd and .str.resid values from our model
potential_outliers <- augment(model_full) |>
  mutate(
    index = 1:n(),
    target = if_else(target == 0, 'Lo', 'Hi')
  ) |>
  relocate(index, .before=target) |>
  top_n(5, .cooksd) |>
  arrange(desc(.cooksd))

glimpse(potential_outliers)
```

The calculation above shows that points 14, 457, 280, 338, and 54 have the highest Cook's distance values (ordered from highest to lowest) and should be investigated as potential outliers.

```{r cooksd-leverage}
par(mar = c(5, 4, 4, 2) + 0.1)
plot(model_full, which = c(4, 6), col=df_training_one_hot$target,  id.n = 5)
```

We see on the Cook's dist vs Leverage plot that points 280 and 338 may have very high leverage on our model, followed by point 14. Point 457 also stand out and should be investigated but appears to have less leverage.


```{r cooksd-influential-points}
# print influential points using cooks-distance
cooksd <- cooks.distance(model_full)
influential <- which(cooksd > (4 / length(cooksd)))
print(influential)

```
The formula above is used to idential influential points defined as points Cook's Distance value is greater than 4 / length of cooksd. This contains all three points (280, 338, and 14) as being influential.

```{r residuals-v-fitted, echo=FALSE}

plot(model_full, which = c(1, 2, 3, 5), col=df_training_one_hot$target,  id.n = 5)

```

Our residual vs fitted, QQ, Scale-Location and Residual vs Leverage plots all confirm that points 280, 338, and 14 should be investigated and could be outliers with high influence. Points 457 appear to have less leverage and does not stand out in these plots. 

Below is the output for the three points identified as potential outliers in our diagnostic plots. A quick review of the data doesn't reveal anything that stands out as being out of the ordinary.

_Partial residual plots_

```{r partial-residual-plots}

for (j in names(coef(model_full))[-1]) {
  if (j != 'chas1' && j != 'chas2'){
    plot(
      x = df_training_one_hot[,j],
      y = residuals(model_full, "partial")[,j],
    col = df_training_one_hot$target,
    main = paste0("Partial residuals by ", j),
    xlab = j,
    ylab = "partial residual"
    )
  }
} 
```

_Binned Residuals Plot_

Below is a Binned Residuals Plot. The binned residuals plot  divide the data into categories (bins) based on their fitted values, then plot the average residual versus the average fitted value for each bin.

```{r binned-plot}
binnedplot(
  x = predict(model_full, newdata=df_training_one_hot, type="response"),
  y = residuals(model_full, type="response")
)

```

In a Binned Residuals Plot, the gray lines indicate plus and minus 2 standard-error bounds. We would expect about 95% of the binned residuals to fall within these lines. Several points fall outside of the 95% interval, but three points are more obviously outside. 


### Linearity

To check this condition, I created a scatterplot with a loess line to check that there is a linear relationship between the logit of the dependent variable and the independent variables. 


```{r check-for-linearity-full-model, fig.width=7, fig.height=12, echo = FALSE}

# predict the probability of high crime rate
predicted_probs <- predict(model_full, newdata=df_training_one_hot, type = "response")

df_full_model_logit <- df_training_one_hot |>
  subset(select = -c(target)) |>
  mutate(
    chas = as.numeric(chas),
    logit = log(predicted_probs/(1-predicted_probs))
  ) |>
  gather(
    key = "predictor", 
    value = "predictor_value", 
    -logit
  )

ggplot(df_full_model_logit, aes(x = predictor_value, y = logit)) +
  geom_point(alpha = 0.5, color = "blue") + 
  geom_smooth(method = "loess", color = "red", se = FALSE) + 
  facet_wrap(~predictor, scales = "free") +
  theme_minimal() +
  labs(title = "Linearity of Logit Check for Binary Logistic Regression",
       x = "Predictor Variable",
       y = "Logit of Predicted Probability")



```

---


### Using mathematical transformations

To reduce the influence of outliers and better align the data with the assumptions of logistic regression, log-transformations were applied to tax, zn, dis, and lstat. This transformation helps normalize the data, reduce variance, and enhance model interpretability. A small constant was added to zn before the transformation to account for zero values.


```{r log-transform}
df_training_1h_log <- df_training_one_hot |>
  mutate(
    log_tax = log(tax),
    log_dis = log(dis),
    log_zn = log(zn + 1),
    log_lstat = log(lstat),
  ) |>
  subset(select = -c(tax, dis, zn, lstat))


model_full_log <- glm(target ~., binomial(link = "logit"), data=df_training_1h_log)
summary(model_full_log)
```

####  Outlier

Applying the log transformation didn't make too much of a difference with our questionable points (280, 338, and 14)

```{r cooksd-leverage-log}
par(mar = c(5, 4, 4, 2) + 0.1)
plot(model_full_log, which = c(4, 6, 1, 2, 3, 5), col=df_training_1h_log$target,  id.n = 5)
```



#### Linearity 

Applying the log transformation helped the linearity for some of the variables. It had less of an effect on medv, indus, and ptratio

```{r check-for-linearity-log-model, fig.width=7, fig.height=12, echo = FALSE}

# predict the probability of high crime rate
predicted_probs <- predict(model_full_log, newdata=df_training_1h_log, type = "response")

df_full_model_logit <- df_training_1h_log |>
  subset(select = -c(target)) |>
  mutate(
    chas = as.numeric(chas),
    logit = log(predicted_probs/(1-predicted_probs))
  ) |>
  gather(
    key = "predictor", 
    value = "predictor_value", 
    -logit
  )

ggplot(df_full_model_logit, aes(x = predictor_value, y = logit)) +
  geom_point(alpha = 0.5, color = "blue") + 
  geom_smooth(method = "loess", color = "red", se = FALSE) + 
  facet_wrap(~predictor, scales = "free") +
  theme_minimal() +
  labs(title = "Linearity of Logit Check for Binary Logistic Regression",
       x = "Predictor Variable",
       y = "Logit of Predicted Probability")


```

#### Colinearity

A Variance Inflation Factor test on our model with logged predictors shows that `nox` and `medv` should be considered for removal. `rm` and `log_dis` may also need to considered.

```{r}
car::vif(model_full_log) |> sort()
```


## MODEL BUILDING

Using a binomial (`target`) for our dependent variable  would violate the common assumptions for linear regression. Specifically:

* the observations will not be normally distributed as they are binary
* the variance of error may be heteroskedastic instead of homoskedastic
* R-squared may not a good fit

To account for these violations, we wil use a Generalized Linear Model (GLM) to conduct logistic regression. 



### Mubashira's Models

### Mubashira Model 1: Baseline (All variables)
```{r}
crime_training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-training-data_modified.csv")

crime_training_df <- crime_training_df %>%
  mutate(across(where(is.character), as.numeric))

# Log transformation
crime_training_df <- crime_training_df %>%
  mutate(
    log_medv = log(medv + 1),  # Avoid log(0)
    log_lstat = log(lstat + 1),
    log_dis = log(dis + 1)
  )

#Standardization (Normalize Continuous Variables)
#Standardization ensures that variables are on the same scale, improving model stability.
crime_training_df <- crime_training_df %>%
  mutate(
    zn_scaled = as.numeric(scale(zn)),
    indus_scaled = as.numeric(scale(indus)),
    nox_scaled = as.numeric(scale(nox)),
    rm_scaled = as.numeric(scale(rm)),
    age_scaled = as.numeric(scale(age)),
    dis_scaled = as.numeric(scale(dis)),
    rad_scaled = as.numeric(scale(rad)),
    tax_scaled = as.numeric(scale(tax)),
    ptratio_scaled = as.numeric(scale(ptratio))
  )


# Create categorical age groups bins
crime_training_df$age_group <- cut(crime_training_df$age, breaks=c(0, 30, 60, 100), labels=c("Young", "Middle-aged", "Old"))

# Create Interaction Terms
crime_training_df <- crime_training_df %>%
  mutate(
    lstat_medv_interact = log_lstat * log_medv,  # Income & housing price interaction
    tax_rad_interact = tax * rad                # Tax burden & highway accessibility
  )

```


```{r, warning = FALSE, message = FALSE}
# Load library

# Logistic Regression Model 1 (Baseline)
model1 <- glm(target ~ chas + lstat + medv + log_medv + log_lstat + log_dis +
                        zn_scaled + indus_scaled + nox_scaled + rm_scaled + age_scaled + dis_scaled +
                        rad_scaled + tax_scaled + ptratio_scaled +
                        lstat_medv_interact + tax_rad_interact + age_group,
              data = crime_training_df, family = binomial)

# Summary of models
summary(model1)

AIC(model1)  # Compare AIC
```


### Mubashira Model 2: Stepwise Selection

```{r}

# Logistic Regression Model 2 (Stepwise Selection)
model2 <- stepAIC(glm(target ~ ., data=crime_training_df, family=binomial), direction="both")

summary(model2)

AIC(model2)  # Compare AIC

```


### Mubashira Model 3: Transformations & Interactions

```{r}

# Logistic Regression Model 3 (With Transformations & Interactions)
model3 <- glm(target ~ log_medv + lstat + nox + ptratio + age_group, data=crime_training_df, family=binomial)

summary(model3)



```

### Puja Model 1: Baseline Logistic Regression

```{r}

### Puja Modified below:
pj_crime_training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-training-data_modified.csv")


### Data Preparation
# Log Transformations
pj_crime_training_df <- pj_crime_training_df %>%
  mutate(
    log_zn = log1p(zn),
    log_indus = log1p(indus),
    log_tax = log1p(tax),
    log_rad = log1p(rad),
    log_lstat = log1p(lstat),
    log_medv = log1p(medv)
  )

# Binning 'age' into Categories
pj_crime_training_df <- pj_crime_training_df %>%
  mutate(age_group = cut(age, breaks = c(0, 40, 70, 100), labels = c("Young", "Middle-aged", "Old")))

# Normalize 'dis'
pj_crime_training_df <- pj_crime_training_df %>%
  mutate(dis_scaled = scale(dis))

```


```{r}
### Model Building
# Model 1: Baseline Logistic Regression
pj_model1 <- glm(target ~ zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + lstat + medv, 
              data = pj_crime_training_df, family = binomial)
summary(pj_model1)
```


### Puja Model 2: Stepwise Logistic Regression

```{r}
#Model 2: Stepwise Logistic Regression
pj_model2 <- step(glm(target ~ ., data = pj_crime_training_df, family = binomial), direction = "both")
summary(pj_model2)
```

#### Puja  Model 3: Logistic Regression with Transformed Variables
```{r}
# Model 3: Logistic Regression with Transformed Variables
pj_model3 <- glm(target ~ log_zn + log_indus + chas + nox + rm + age_group + dis_scaled + log_rad + log_tax + ptratio + log_lstat + log_medv, 
              data = pj_crime_training_df, family = binomial)
summary(pj_model3)
``` 

###  Model using Principal Components

This section uses the correlation plot to perform Principal Component Analysis on the two large variable clusters shown in the plot. We will then substiture the variables in each of the two clusters with their respective PC scores in our model.

```{r principal-component-model}

# Create PCA from first cluster in our correlation plot
df_pca_subset1 <- df_training_one_hot |>
  subset(select = c(indus, tax, lstat, nox, age, ptratio, radq_hi))

# calculate PCA
df_training_pca1 <- prcomp(df_pca_subset1, scale=TRUE)

# use eigen vectors to plot % of data explained by PCA1
fviz_eig(df_training_pca1, addlabels=TRUE, ylim=c(0, 70))

# plot PCA biplot
fviz_pca_biplot(df_training_pca1, label="var", habillage =  df_training_one_hot$target)

# Create PCA from second cluster in our correlation plot
df_pca_subset2 <- df_training_one_hot |>
  subset(select = c(rm, medv, zn, dis))

# calculate PCA
df_training_pca2 <- prcomp(df_pca_subset2, scale=TRUE)

# use eigen vectors to plot % of data explained by PCA1
fviz_eig(df_training_pca2, addlabels=TRUE, ylim=c(0, 70))

# plot PCA biplot
fviz_pca_biplot(df_training_pca2, label="var", habillage =  df_training_one_hot$target)

# add pca's to our dataset
df_training_one_hot_pca <- df_training_one_hot |>
  subset(select = c(target, chas, radq_low)) |>
  mutate(
    group1_pc1 = df_training_pca1$x[,"PC1"],
    group1_pc2 = df_training_pca1$x[,"PC2"],
    group2_pc1 = df_training_pca2$x[,"PC1"],
    group2_pc2 = df_training_pca2$x[,"PC2"],
  ) 

#ggpairs(df_training_one_hot_pca |> subset(select = -c(target)))

model_pca <- glm(target ~., binomial(link = "logit"), data=df_training_one_hot_pca)
summary(model_pca)
```

Interestingly, only the primary principal component from group1 and the secondary principal component from group two have strong statistical significance. `radq_low` has a particularly high p-value and should be considered for removal.

```{r pca2}
model_pca2 <- update(model_pca, . ~ . - radq_low)
summary(model_pca2)

model_pca2 <- update(model_pca2, . ~ . - chas)
summary(model_pca2)

model_pca2 <- update(model_pca2, . ~ . - group1_pc2)
summary(model_pca2)

model_pca2 <- update(model_pca2, . ~ . - group2_pc1)
summary(model_pca2)
```



###  Model based on Variable Clustering

The dendogram is a variable clustering technique that shows how the parameters progressively come together at different levels of similarity. It offers another way to visualize correlations between our parameters. In this model, we will use the dedogram to prune parameters that are similar from the lower branches. In this model, we used the results from a T and Wilcox pairwise test to assist with the parameter selection.

```{r}

dist_one_hot = as.dist(m = 1 - abs(df_training_cor))
par(mar = c(5, 4, 4, 2) + 0.1)
plot(hclust(dist_one_hot))


sapply(numeric_cols, function(param) {
  pairwise.t.test(
    x = df_training_one_hot[, param],
    g = df_training_one_hot$target,
    pool.sd = FALSE,
    paired = FALSE,
    alternative = "two.sided"
  )$p.value
}) |> sort()

sapply(numeric_cols, function(param) {
  pairwise.wilcox.test(
    x = df_training_one_hot[, param],
    g = df_training_one_hot$target,
    pool.sd = FALSE,
    paired = FALSE,
    alternative = "two.sided"
  )$p.value
}) |> sort()

model_dendo <- glm(target ~ radq_hi + chas + lstat + indus + age, binomial(link = "logit"), data=df_training_one_hot)
summary(model_dendo)
```

### Model Using Quasi-Logit


### Model comparison
```{r full-model-lrstats}

library(vcdExtra)
library(pscl)
#models <- list(model_full, model_full_log, backward_model, backward_log_model, model_corr, model_pca, model_dendo)

stats <- LRstats(model_full, model_pca, model_dendo, model1, model2, model3, pj_model1, pj_model2, pj_model3)

stats$McFaddenR2 <- NA 
stats$Accuracy <- NA 
stats$Precision <- NA 
#stats$Recall <- NA 
stats$Sensitivity <- NA 
stats$Specificity <- NA 
stats$F1_score <- NA 
stats$AUC <- NA 
stats$CV_est_predict_err <- NA 
stats$CV_adj_est <- NA 

enhanceEvaluationMetrics <- function(df, model_name) {
  model <- get(model_name)
  
  if (model_name == "model_full_log" | model_name == "backward_log_model") {
    model_data <- df_training_1h_log
  } else if (model_name == "model_pca") {
    model_data <- df_training_one_hot_pca
  } else if (model_name == "model_full" | model_name == "backward_model" | model_name == "model_corr" | model_name == "model_dendo") {
    model_data <- df_training_one_hot
  } else if (model_name == "pj_model1" | model_name == "pj_model2" | model_name == "pj_model3") {
    model_data <- pj_crime_training_df
  } else {
    model_data <- crime_training_df
  }
  
  df[model_name, "McFaddenR2"] <- pR2(model)["McFadden"]
  
  pred_probs <- predict(model, type = "response")
  
  pred_probs_factor <- as.factor(ifelse(pred_probs > 0.5, 1, 0))
  conf_matrix <- confusionMatrix(pred_probs_factor, as.factor(model_data$target))
  df[model_name, "Accuracy"] <- conf_matrix$overall['Accuracy']
  df[model_name, "Precision"] <- conf_matrix$byClass['Precision']
  #df[model_name, "Recall"] <- conf_matrix$byClass['Recall']
  df[model_name, "F1_score"] <- conf_matrix$byClass['F1']
  df[model_name, "Sensitivity"] <- conf_matrix$byClass["Sensitivity"] 
  df[model_name, "Specificity"] <- conf_matrix$byClass["Specificity"]

  #roc_model <- roc(as.factor(model_data$target), pred_probs)
  #plot(roc_model, main = "ROC Curve using pROC", col = "red", lwd = 2)
  # roc_auc not working, so use MLmetrics
  df[model_name, "AUC"] <- MLmetrics::AUC(y_true = model_data$target, y_pred = pred_probs) 
  
  # Cross-Validation using 10 folsds
  cv_result <- boot::cv.glm(model_data, model, K= 10)
  df[model_name, "CV_est_predict_err"] <- cv_result$delta[1]
  df[model_name, "CV_adj_est"] <- cv_result$delta[2]
  
  return(df) 
}


# Loop through the list of models and update the dataframe for each
for (model_name in rownames(stats)) {
  
  stats <- enhanceEvaluationMetrics(stats, model_name)
}

stats 
```

For logistic regression, the "prediction error" is the mean squared error (difference between the predicted probabilities and the actual outcomes).

----

#### Checking the Model's Conditions

We will examine the following key conditions for fitting a logistic model:

1. dependent variable is binary 
2. large enough sample 
3. observations are independent, not matched
4. independent (predictor) variables do not correlate too strongly with each other 
5. linearity of independent variables and log odds
6. no outliers in data


##### Confidence Interval

```{r confint, echo=FALSE, warning=FALSE, message=FALSE}
confint(model_full)

```


##### Odds Ratio 
```{r odds-ration, echo=TRUE,  warning=FALSE, message=FALSE}
exp(coef(model_full))
```


#### ANOVA Test
```{r anova, echo =TRUE,  warning=FALSE, message=FALSE}
model_summary <- summary(model_full)
#param_pvalue <- model_summary$coefficients["target", "Pr(>|z|)"]
anova(model_full, test = "Chisq")
```


