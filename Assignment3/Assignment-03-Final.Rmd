---
title: "D621 - Assignment 3"
author: "Mubashira Qari, Marco Castro, Puja Roy, Zach Rose & Erick Hadi"
date: "2025-03-07"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(ggplot2)
library(miscTools)
library(GGally)
library(DataExplorer)
library(lmtest)
#library(car)
library(ggpubr)

library(arm)
library(regclass)
library(corrplot)
library(psych)
library(broom)
library(vcd)
library(vcdExtra)
library(MASS)
library(factoextra)

library(caret)
library(pROC)


# Load required packages
library(htmltools)
library(dplyr)
library(skimr)
require(miscTools)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 

# new 

library(factoextra)
library(gridExtra)

```

```{r import-data, echo=FALSE}



crime_training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-training-data_modified.csv")

crime_evaluation_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-evaluation-data_modified.csv")

df_training  <- crime_training_df
crime_training_zr <- crime_training_df

```

## DATA EXPLORATION

The training dataset contains 466 observations and 13 variables, including 12 predictor variables and one binary response variable (target). The target variable indicates whether a neighborhood’s crime rate is above the median (1) or not (0). The dataset includes a mix of continuous and categorical features, such as housing characteristics, pollution levels, property taxes, and proximity to employment centers. Understanding the distributions, relationships, and correlations between these features is an essential first step in building an effective predictive model. 

```{r}
dim(df_training)    
str(df_training)           

column_types <- sapply(df_training, class)
print(column_types)
```

The following three columns were imported as numerical but should be considered for converting to factors: 
- chas: binomial 
- rad: ordinal 
- target: binomial

```{r explore-summary}

# convert to factor
df_training <- df_training |>
  mutate(
    chas = as.factor(chas),
    rad = as.factor(rad),
    target = as.factor(target),
  )

numeric_cols <- c('zn', 'indus', 'nox', 'rm', 'age', 'dis', 'tax', 'ptratio', 'lstat', 'medv')

factor_cols <- c('chas', 'rad', 'target')

```

```{r bin-rads-hidden, include=FALSE}
# one hot encode rad, but explain later for readibility 
quantile_breaks <- quantile(as.numeric(df_training$rad), probs = c(0, 1/3, 2/3, 1))

df_training$radq <- cut(as.numeric(df_training$rad),
                  breaks = quantile_breaks,  
                  labels = c('_low', '_mid', '_hi'),
                   include.lowest = TRUE, 
                   right = TRUE)  

# one-hot encode rad values 
rad_one_hot <- model.matrix(~ radq - 1, data = df_training)

# combine new columns
df_training_one_hot <- cbind(df_training[ , !names(df_training) %in% "rad"], rad_one_hot) |>
  subset(select=-c(radq, radq_mid))

model_full <- glm(target ~., binomial(link = "logit"), data=df_training_one_hot)

```

A closer examination of the `rad` data shows that that our observations have a rad index value of 1-8 or 24 in this column. Below are the counts: 

```{r count-rads, echo=FALSE}

rad_counts <- table(df_training$rad)

# Print the result
print(rad_counts) 
```


### Summary Statistics

Summary statistics reveal substantial variability in several features. For instance, the variable tax (full-value property tax rate per $10,000) has a mean of 409.5 but a maximum value of 711, indicating a right-skewed distribution with significant outliers. Similar patterns are observed in zn (zoned residential land), age, and dis (distance to employment centers). These skewed distributions may require transformation to reduce leverage effects during modeling.

```{r explore-means-medians}

# only show summary stats for numeric values
for (param in numeric_cols) {
  cat("\nSummary for", param, ":\n")
  print(describe(df_training[[param]]))
}
```

#### Key Observations:


Crime Rate Target (target)

The median (p50) is 0, indicating that more than half of the data points fall in the low-crime category (target = 0).

Median Home Value (medv)
Mean = 22.59 ($22,590 in $1000s), Median = 21.2.
The range (p0 = 5, p75 = 25) suggests that most homes are valued between $5,000 and $25,000 (in $1000s).
The standard deviation (9.23) indicates a relatively high spread in home values.

Lower Status Population (lstat)
Mean = 12.63%, Median = 11.93%.
A positively skewed distribution (p0 = 1.73, p75 = 16.93), meaning some areas have much higher lower-status populations than others.

Property Tax Rate (tax)
High variance (Mean = 409.5, SD = 167.9).
Large difference between the 25th percentile (281) and 75th percentile (666), suggesting significant variability in tax rates among neighborhoods.

Average Number of Rooms (rm)
Mean = 6.29, Median = 6.21, with a relatively small spread (SD = 0.70).
Indicates most homes have around 6 rooms.

Distance to Employment Centers (dis)
Median = 3.19, but the 25th percentile is quite low (2.10), meaning some neighborhoods are much closer to employment centers than others.
Higher standard deviation (2.1) suggests some neighborhoods are much more remote.

Industrial Land Proportion (indus)
Mean = 11.10, Median = 9.69, and right-skewed distribution (p0 = 0.46, p75 = 18.1).
Some areas have much higher proportions of industrial land, potentially influencing crime.

Highway Accessibility (rad)
Highly right-skewed: The median is 5, but the 75th percentile is 24, meaning some neighborhoods have much greater access to highways than others.
This might be an important predictor for crime.

Potential Data Transformations
zn, indus, tax, rad, lstat, and medv show skewness, so applying a log transformation might improve normality.
age can be categorized into bins (e.g., young, middle-aged, old) since it ranges from 2.9 to 94.1.
dis has a wide range, so normalization might be needed.

### Missing data

No missing values were found in the training dataset. Therefore, no imputation or flagging was necessary at this stage. Several continuous variables demonstrated right-skewed distributions and a high number of extreme values. To reduce the influence of outliers and better align the data with the assumptions of logistic regression, log-transformations were applied to tax, zn, dis, and lstat. This transformation helps normalize the data, reduce variance, and enhance model interpretability. A small constant was added to zn before the transformation to account for zero values.

```{r explore-missing}

#introduce(df_training, echo=FALSE)
missing_values_count <- sapply(data, function(x) sum(is.na(x)))
print(missing_values_count)

```

### Plots of data

#### Boxplots

A boxplot of the numeric features further highlights the presence of skewness and outliers in variables such as tax, zn, and age. Many variables, such as chas (a binary indicator for bordering the Charles River), show relatively limited spread, while others, like nox and indus, display more variability across neighborhoods. These differences in scale and distribution suggest that transformation or normalization may be beneficial in the modeling stage. A correlation heatmap was constructed to examine multicollinearity and the relationships between predictors. Strong positive correlations were observed between nox, age, tax, indus, and rad, suggesting that these variables may capture related structural or geographic aspects of the neighborhoods. Several variables also show moderate to strong correlation with the target variable — in particular, nox, age, rad, and tax were positively correlated with higher crime risk, while dis and rm were negatively correlated.

Below is series of boxplots for all numeric parameters where target is our dependent variable.

```{r explore-boxplot}
plot_boxplot(df_training, by = "target", title="Boxplots of Target vs Param")
```

```{r iqrs}
df_training_hi_crime <- df_training |>
  filter(target == 1) |>
  subset(select = -c(chas, rad, target)) 

df_training_lo_crime <- df_training |>
  filter(target == 0) |>
  subset(select = -c(chas, rad, target)) 

cat("\nIQR for High Crime Neighborhoods\n")
summary(df_training_hi_crime)

cat("\nIQR for Low Crime Neighborhoods\n")
summary(df_training_lo_crime)
```

The boxplots show the distribution numerical parameters grouped by the dependent variable `target`. The plots are useful for getting a sense as to which parameters may be good predictors based on how different the parameter;s IQRs are. Conversely, similar IQRs may provide insight into which may not add much information to our model. Based on these box plots, we see that the IQR for `rm` are very similar where `target` is 0 and 1 and should be flagged for potential removal of our plot.  `ptratio` and `medv` have some overlap  All other variables appear somewhat 

Further, we see that param *zn* has a median value around zero, suggesting that few neighborhoods have residential areas zoned for large plots as shown below. We should also consider omitting this variable from our model down the line


```{r zn-zeros}
count_zeros <- sum(df_training_hi_crime$zn == 0)
cat("\nAbove Median Crime Rate Neighborhoods have ", count_zeros, " rows with a value of 0 for param zn out of ", nrow(df_training_hi_crime), "observations (",  
(count_zeros / nrow(df_training_hi_crime)), "%)\n")

count_zeros <- sum(df_training_lo_crime$zn == 0)
cat("\nBelow Median Crime Rate Neighborhoods have ", count_zeros, " rows with a value of 0 for param zn out of ", nrow(df_training_lo_crime), "observations (",  
(count_zeros / nrow(df_training_lo_crime)), "%)\n")

```


_Categorical Variables_

For our categorical variables, we can use barglaphs to get a sense of the parameter's impact on `target`. 


```{r chas-bargraphs, echo=FALSE}
chas_bargraph <- df_training |>
  group_by(
    target,chas
  ) |>
  dplyr::summarise(
    count = n()
  ) |>
  ungroup() |>
  group_by(chas) |>
  mutate(
    percent = 100 * count / sum(count),
    label = paste0(round(percent),"%")
  ) |>
  ggplot() +
    aes(x = chas, y = count, label = label, fill=target) +
    geom_col() +
    geom_text(position = position_stack(0.5)) +
    theme(legend.position = "bottom")

### rad
rad_bargraphs <- df_training |>
  group_by(
    target,rad
  ) |>
  dplyr::summarise(
    count = n()
  ) |>
  ungroup() |>
  group_by(rad) |>
  mutate(
    percent = 100 * count / sum(count),
    label = paste0(round(percent),"%")
  ) |>
  ggplot() +
    aes(x = rad, y = count, label = label, fill=target) +
    geom_col() +
    geom_text(size = 3, position = position_stack(0.5)) +
    theme(legend.position = "bottom")

grid.arrange(chas_bargraph, rad_bargraphs, ncol = 2, nrow = 1)

```

The bargraph for `chas` on the left shows fairly equal values for 0 and 1 accross the `chas` values. This suggests that the variable will may have low impact on our model and we should consider removing it.

The bargraphs for `rad` on the right are somewhat more revealing. They suggest a strong relationship between low `rad` index values of 1-3 and below median crime rate, while an index value of 24 (the highest rad index) has a strong relationship with above median crime rate.


### Checking Binary Logistic Regression Assumptions

Before interpreting results from a binary logistic regression, we must verify three key assumptions:

- Independence of Observations
- Linearity of the Logit
- No Multicollinearity

Spearman's correlation measures monotonic relationships between variables, making it suitable when we have a mix of ordinal and continuous predictors. Unlike the Pearson test, it does not assume normality. Additionally, it is more robust against outliers than Pearson.

#### Checking Independence Assumption

The independence assumption in binary logistic regression states that each observation (row) in the dataset should be independent of the others. This means:

No duplicated data points (e.g., same neighborhood appearing multiple times).
No clustered observations (e.g., observations grouped by region, time, or other factors).
No strong correlations between residuals of observations, meaning observations do not systematically affect each other.


```{r, warning = FALSE, message = FALSE}

# Exclude categorical columns before pivoting
crime_long <- crime_training_df %>%
  dplyr::select(where(is.numeric)) %>%  # Keep only numeric columns
  pivot_longer(cols = -target, names_to = "Variable", values_to = "Value")

# Create scatter plots for each predictor vs target with a fitted line
ggplot(crime_long, aes(x = Value, y = target)) +
  geom_point(alpha = 0.5, color = "blue") +  # Scatter plot
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear fit
  facet_wrap(~Variable, scales = "free") +  # Multiple plots for each variable
  theme_minimal() +
  labs(title = "Independence Check: Scatter Plots of Predictors vs Target",
       x = "Predictor Value",
       y = "Crime Rate (Binary Target)")


```


```{r}

# Initialize an empty data frame to store results
independence_results <- data.frame(Variable = character(),
                                   Correlation = numeric(),
                                   P_Value = numeric(),
                                   stringsAsFactors = FALSE)

# Loop through each predictor variable (excluding the target)
for (var in colnames(crime_training_df)[colnames(crime_training_df) != "target"]) {
  
  # Ensure the variable is numeric before computing Spearman correlation
  if (is.numeric(crime_training_df[[var]])) {
    
    # Perform Spearman correlation test
    test_result <- cor.test(crime_training_df[[var]], crime_training_df$target, method = "spearman")
    
    # Store results in a data frame
    independence_results <- rbind(independence_results, 
                                  data.frame(Variable = var, 
                                             Correlation = test_result$estimate, 
                                             P_Value = test_result$p.value))
  }
}

# View results in tabular format
print(independence_results)

```

##### Checking for Independence Using Spearman's Correlation
In Spearman’s method, we check the correlation between:

Each independent variable and the dependent variable (target)
If correlation is too low (|p| < 0.1) and p-value > 0.05, the variable might not be useful in predicting the target.

Variables to Consider Removing:

chas (p = 0.0800, p = 0.0843) → No meaningful correlation with crime.
Possibly rm (p = -0.1772, p = 0.00012) → Weak correlation but could check its importance in the model.

Variables to Keep (for now, but monitor multicollinearity):

nox (p = 0.7547)
age (p = 0.6457)
dis (p = -0.6591)
indus (p = 0.6192)
rad, tax, ptratio (moderate correlation)
Transform / Create Interaction Terms:

Log transform: dis (since it has a strong negative correlation)
Categorize: age into "Young", "Middle-aged", "Old"
Interaction term: tax * rad (both impact crime)


```{r}

crime_training_df <- crime_training_df %>%
  mutate(across(where(is.character), as.numeric))

crime_evaluation_df <- crime_evaluation_df %>%
  mutate(across(where(is.character), as.numeric))

# Verify again
str(crime_training_df)

###  Visualize Correlation Matrix

# Compute Spearman correlation matrix
numeric_data <- crime_training_df %>% dplyr::select(where(is.numeric), -target)
spearman_cor <- cor(numeric_data, method = "spearman", use = "pairwise.complete.obs")

# Plot the Spearman correlation matrix with labels
corrplot::corrplot(spearman_cor, 
         method = "color",         
         type = "upper",           
         tl.cex = 0.8,             
         addCoef.col = "black",    
         number.cex = 0.7,         
         col = colorRampPalette(c("blue", "white", "red"))(200)
)

```



#### Checking for Multicollinearity (High Correlation Between Predictors)


To test if correlation exists between the dependent and independent variables, we used a Pearson's Correlation test. The function below loops through each of our columns and prints out the correlation of the dependent variable `target` with each of the predictors. For predictors where Pearson's Correlation coefficient is close to zero, we can determine that collinearity does not exist.



```{r check-for-correlation}
# is above .7 would be too highly correlated
cor_results <- data.frame(name = character(0), value = numeric(0))
for (param in colnames(df_training_one_hot)) {
  cat("\nPearson Test score for", param, ":\n")
  x <- as.numeric(df_training_one_hot$target)
  y <- as.numeric(df_training_one_hot[[param]])
  pearsons <- cor.test(x, y, method = "pearson")
  print(pearsons)
  # calc pearson cor value only
  cor_object <-  data.frame(name = param, value = cor(x, y))
  assign("cor_results", rbind(cor_results, cor_object), envir = .GlobalEnv)
}

print(cor_results)
```

A general rule of thumb is that if |p| > 0.7, it indicates strong correlation between variables, which can lead to multicollinearity in the logistic regression model.

From the matrix:
indus & nox (p = 0.79) → Strong positive correlation, meaning they provide redundant information.
tax & rad (p = 0.70) → These variables are highly correlated, indicating one may be removed.
lstat & medv (p = -0.85) → Very strong negative correlation; keeping both might be problematic.
log_medv & medv (p = 1.00) → Perfect correlation (since log transformation was applied), meaning one must be removed to prevent redundancy.

Final Takeaways:

High correlation between predictors (|p| > 0.7) indicates potential multicollinearity. Consider removing indus, rad, or medv to improve model stability.
No evidence of entire rows/columns being highly correlated, suggesting no major independence violations.
Use VIF for confirmation and decide on variable selection accordingly.



```{r correlation-predictors, echo=FALSE}
layout(1)
par(mfrow = c(1, 1))
df_training |>
  subset(select=-c(target, chas)) |>
  plot_correlation(type = "all")
```


##### Variance Inflation Factor (VIF)

Variance Inflation Factor (VIF) helps quantify multicollinearity by measuring how much the variance of regression coefficients is inflated due to correlation among predictors. 
A VIF > 5 (or more conservatively, VIF > 10) suggests severe multicollinearity.

```{r}
car::vif(glm(target ~ nox + age + rad + tax + dis + zn + medv, family = binomial, data = crime_training_df))

```

Conclusion
 There is NO severe multicollinearity (all VIF values are below 5).
 No immediate need to drop variables based on VIF.
 The variable dis (VIF = 3.58) shows moderate correlation with other predictors, but it's not problematic.
 

Keep all predictors in the model.
If we suspect redundancy, check pairwise correlations again or test removing dis to see if model performance improves.

#### Checking the Assumption of Linearity of the Logit for Binary Logistic Regression

In logistic regression, we assume that each continuous predictor has a linear relationship with the log-odds (logit) of the target variable. 
If this assumption is violated, the model may be misleading or inaccurate.

```{r, warning = FALSE, message = FALSE}
# Load necessary libraries

# Fit the logistic regression model
model <- glm(target ~ nox + age + rad + tax + dis + zn + medv, 
             family = binomial, data = crime_training_df)

# Compute predicted probabilities
crime_training_df$predicted_prob <- predict(model, type = "response")

# Compute logit (log-odds) transformation
crime_training_df$logit <- log(crime_training_df$predicted_prob / 
                              (1 - crime_training_df$predicted_prob))

# Reshape data for faceting
plot_data <- crime_training_df %>%
  dplyr::select(logit, nox, age, rad, tax, dis, zn, medv) %>%
  pivot_longer(cols = -logit, names_to = "Predictor", values_to = "Value")

# Create faceted scatter plot
ggplot(plot_data, aes(x = Value, y = logit)) +
  geom_point(alpha = 0.5, color = "blue") + 
  geom_smooth(method = "loess", color = "red", se = FALSE) + 
  facet_wrap(~ Predictor, scales = "free") +
  theme_minimal() +
  labs(title = "Linearity of Logit Check for Binary Logistic Regression",
       x = "Predictor Variable",
       y = "Logit of Predicted Probability")

```


#### Checking for Possible Outliers

We can examine our diagnostic plots to find potential outliers and leverage. First we will examine the Cook’s Distance and Cook’s Distance vs Leverage plots. Cook’s Distance measures the influence of an observation on the fitted values of the model.  

```{r cooks-d, echo=FALSE}

# use augment to get .cooksd and .str.resid values from our model
potential_outliers <- augment(model_full) |>
  mutate(
    index = 1:n(),
    target = if_else(target == 0, 'Lo', 'Hi')
  ) |>
  relocate(index, .before=target) |>
  top_n(5, .cooksd) |>
  arrange(desc(.cooksd))

glimpse(potential_outliers)
```

The calculation above shows that points 14, 457, 280, 338, and 54 have the highest Cook's distance values (ordered from highest to lowest) and should be investigated as potential outliers.

```{r cooksd-leverage, fig.width=6, fig.height=8, echo=FALSE}
layout(matrix(1:6, nrow = 3, byrow = TRUE))
plot(model_full, which = c(4, 6, 1, 2, 3, 5), col=df_training_one_hot$target,  id.n = 5)
par(mfrow = c(1, 1))
```

We see on the Cook's dist vs Leverage plot that points 280 and 338 may have very high leverage on our model, followed by point 14. Point 457 also stand out and should be investigated but appears to have less leverage.


```{r cooksd-influential-points}
# print influential points using cooks-distance
cooksd <- cooks.distance(model_full)
influential <- which(cooksd > (4 / length(cooksd)))
print(influential)

```
The formula above is used to idential influential points defined as points Cook's Distance value is greater than 4 / length of cooksd. This contains all three points (280, 338, and 14) as being influential.

A quick review of the data doesn't reveal anything that stands out as being out of the ordinary.


##### Interpretation of the Linearity of Logit Check for Binary Logistic Regression

This faceted scatter plot assesses the assumption of linearity of the logit for binary logistic regression. 
The blue dots represent the relationship between each predictor variable and the logit of the predicted probability, 
while the red LOESS (Locally Estimated Scatterplot Smoothing) curve helps visualize patterns.

#### Overall Conclusion
Several predictors (e.g., dis, medv, tax, zn) violate the linearity of logit assumption.



## DATA PREPARATION

This step involves cleaning and transforming data to improve model performance.

### Fixing missing values
Luckily, there are no missing values in the training set.

### Binned Transformation 

To simplify the effects of age and distance while preserving interpretability, these two variables were transformed into categorical bins using quantiles. This can help reduce the influence of extreme values and better capture non-linear effects in logistic regression.

```{r}
# Create age and dis bins 
crime_training_zr$zr_age_bin <- cut(crime_training_zr$age, 
                                    breaks = quantile(crime_training_zr$age, probs = c(0, 0.33, 0.66, 1)), 
                                    labels = c("Young", "Middle-aged", "Old"),
                                    include.lowest = TRUE)

crime_training_zr$zr_dis_bin <- cut(crime_training_zr$dis, 
                                    breaks = quantile(crime_training_zr$dis, probs = c(0, 0.33, 0.66, 1)), 
                                    labels = c("Near", "Mid-range", "Far"),
                                    include.lowest = TRUE)

```


The variable `rad` contains an ordinal factor that represents an index of accessibility to radial highways with values ranging from 1-24. A count of the rad values reveals that the `rad` column contains only values 1-8 and 24. This data set does not include any rows with a `rad` value of 9-23. 

Since this column is contains values for an index value where 1 is assigned to neighborhoods with the poorest accessibility to a highway and 24 is assigned to neighborhoods with the most accessibility, we can simplify our variables by binning our rad values. Here we are using quantiles to bin the values into three buckets of nearly equal sizes for low, moderate and high accessibility. This method ensures a more balanced distribution of rows across the bins over using equal sized bins  (1-8, 9-16, 17-24). This especially useful when the data is not uniformly distributed across the range such as in our case where we do not have any rad values of 1-23.


```{r bin-rads}
rad_counts
quantile_breaks <- quantile(as.numeric(df_training$rad), probs = c(0, 1/3, 2/3, 1))

df_training$radq <- cut(as.numeric(df_training$rad),
                  breaks = quantile_breaks,  
                  labels = c('_low', '_mid', '_hi'),
                   include.lowest = TRUE, 
                   right = TRUE)  

table(df_training$radq)
```

While the `glm` function should automatically perform one-hot encoding to factors, we should consider one-hot encoding on the `rad_quantile` parameter to perform other operations, such as calculating correlation using the spearman test.

We will drop one of the one-hot encoded params as the presence of this additional param will result in correlation issues down the line. `radq_mid` was selected, as it seemed to have the most mixed results in our plots above.

```{r one-hot-encoding}
# one-hot encode rad values 
rad_one_hot <- model.matrix(~ radq - 1, data = df_training)

# combine new columns
df_training_one_hot <- cbind(df_training[ , !names(df_training) %in% "rad"], rad_one_hot) |>
  subset(select=-c(radq, radq_mid))

glimpse(df_training_one_hot)
```

We will use the one-hot encoded dataframe to diagnose a preliminary model with all of the predictors.

```{r full-model}
summary(model_full)

```

Reviewing the summary statistics for full model indicates that the variable `indus`, `nox`, `dis`, and `radq_hi` has very strong statistically signification. Two additional variables, `medv`, `dis` and `radq_mid`, have high statistical significance while `zn` has weak statistical significance. `chas1`, `rm`, `age`, `tax`, `ptratio` and `lstat` have weak statistical significance values.

__Note: had we one-hot encoded all of the values for `rad` instead of binning them first, all `rad` params would have very weak statistical significance, as their p-values are nearly 1.0.__ 



### Using mathematical transformations

To reduce the influence of outliers and better align the data with the assumptions of logistic regression, log-transformations were applied to tax, zn, dis, and lstat. This transformation helps normalize the data, reduce variance, and enhance model interpretability. A small constant was added to zn before the transformation to account for zero values.

#### Log Transformation

To address skewness and improve model interpretability, the following features were log transformed: tax, dis, zn, and lstat. These variables exhibited non normal distributions with long tails and outliers. Applying a log transformation reduces variance, brings extreme values closer to the center, and better satisfies the assumptions of logistic regression.

```{r}
crime_training_zr <- crime_training_zr %>%
  mutate(
    log_tax = log(tax),
    log_dis = log(dis),
    log_zn = log(zn + 1),
    log_lstat = log(lstat)
  ) 

model_full_log <- glm(target ~. - tax - dis - zn - lstat, binomial(link = "logit"), data=crime_training_zr)
summary(model_full_log)
```

#### Alternate Transformation
Log transformations: medv, lstat, and dis might benefit from log transformations to reduce skewness.
Binning: Convert age into categorical buckets (e.g., young, middle-aged, old).
Interactions: Create new features (e.g., lstat*medv to capture relationships between income and home values).
Standardization: Normalize numerical variables to bring them to the same scale.

```{r}

# Log transformation
crime_training_df <- crime_training_df %>%
  mutate(
    log_medv = log(medv + 1),  # Avoid log(0)
    log_lstat = log(lstat + 1),
    log_dis = log(dis + 1)
  )

#Standardization (Normalize Continuous Variables)
#Standardization ensures that variables are on the same scale, improving model stability.
crime_training_df <- crime_training_df %>%
  mutate(
    zn_scaled = as.numeric(scale(zn)),
    indus_scaled = as.numeric(scale(indus)),
    nox_scaled = as.numeric(scale(nox)),
    rm_scaled = as.numeric(scale(rm)),
    age_scaled = as.numeric(scale(age)),
    dis_scaled = as.numeric(scale(dis)),
    rad_scaled = as.numeric(scale(rad)),
    tax_scaled = as.numeric(scale(tax)),
    ptratio_scaled = as.numeric(scale(ptratio))
  )


# Create categorical age groups bins
crime_training_df$age_group <- cut(crime_training_df$age, breaks=c(0, 30, 60, 100), labels=c("Young", "Middle-aged", "Old"))

```


```{r}
# Create Interaction Terms
crime_training_df <- crime_training_df %>%
  mutate(
    lstat_medv_interact = log_lstat * log_medv,  # Income & housing price interaction
    tax_rad_interact = tax * rad                # Tax burden & highway accessibility
  )

colnames(crime_training_df)
```

x#### Alternate Transformation

```{r}

### Puja Modified below:
pj_crime_training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-training-data_modified.csv")


### Data Preparation
# Log Transformations
pj_crime_training_df <- pj_crime_training_df %>%
  mutate(
    log_zn = log1p(zn),
    log_indus = log1p(indus),
    log_tax = log1p(tax),
    log_rad = log1p(rad),
    log_lstat = log1p(lstat),
    log_medv = log1p(medv)
  )

# Binning 'age' into Categories
pj_crime_training_df <- pj_crime_training_df %>%
  mutate(age_group = cut(age, breaks = c(0, 40, 70, 100), labels = c("Young", "Middle-aged", "Old")))

# Normalize 'dis'
pj_crime_training_df <- pj_crime_training_df %>%
  mutate(dis_scaled = scale(dis))

```

####  Outliers

Applying the log transformation didn't make too much of a difference with our questionable points (280, 338, and 14)

```{r cooksd-leverage-log, fig.width=6, fig.height=8, echo=FALSE}
par(mfrow = c(3,2), mar = c(5, 4, 4, 2) + 0.1)
plot(model_full_log, which = c(4, 6, 1, 2, 3, 5), col=crime_training_zr$target,  id.n = 5)
par(mfrow = c(1, 1))
```


#### Linearity 

Applying the log transformation helped the linearity for some of the variables. It had less of an effect on medv, indus, and ptratio

```{r check-for-linearity-log-model, fig.width=7, fig.height=12, echo = FALSE}

# predict the probability of high crime rate
predicted_probs <- predict(model_full_log, newdata=crime_training_zr, type = "response")

df_full_model_logit <- crime_training_zr |>
  subset(select = -c(target)) |>
  mutate(
    chas = as.numeric(chas),
    logit = log(predicted_probs/(1-predicted_probs))
  ) |>
  gather(
    key = "predictor", 
    value = "predictor_value", 
    -logit
  )

ggplot(df_full_model_logit, aes(x = predictor_value, y = logit)) +
  geom_point(alpha = 0.5, color = "blue") + 
  geom_smooth(method = "loess", color = "red", se = FALSE) + 
  facet_wrap(~predictor, scales = "free") +
  theme_minimal() +
  labs(title = "Linearity of Logit Check for Binary Logistic Regression",
       x = "Predictor Variable",
       y = "Logit of Predicted Probability")


```

#### Colinearity

A Variance Inflation Factor test on our model with logged predictors shows that `nox` and `medv` should be considered for removal. `rm` and `log_dis` may also need to considered.

```{r}
car::vif(model_full_log) |> sort()
```


## MODEL BUILDING

Using a binomial (`target`) for our dependent variable  would violate the common assumptions for linear regression. Specifically:

* the observations will not be normally distributed as they are binary
* the variance of error may be heteroskedastic instead of homoskedastic
* R-squared may not a good fit

To account for these violations, we wil use a Generalized Linear Model (GLM) to conduct logistic regression. 



### Approach 1:

### Model 1: Baseline (All variables)

```{r, warning = FALSE, message = FALSE}
# Load library

# Logistic Regression Model 1 (Baseline)
model1 <- glm(target ~ chas + lstat + medv + log_medv + log_lstat + log_dis +
                        zn_scaled + indus_scaled + nox_scaled + rm_scaled + age_scaled + dis_scaled +
                        rad_scaled + tax_scaled + ptratio_scaled +
                        lstat_medv_interact + tax_rad_interact + age_group,
              data = crime_training_df, family = binomial)

# Summary of models
summary(model1)

AIC(model1)  # Compare AIC
```


### Model 2: Stepwise Selection

```{r}

# Logistic Regression Model 2 (Stepwise Selection)
model2 <- stepAIC(glm(target ~ ., data=crime_training_df, family=binomial), direction="both")

summary(model2)

AIC(model2)  # Compare AIC

```


### Model 3: Transformations & Interactions

```{r}

# Logistic Regression Model 3 (With Transformations & Interactions)
model3 <- glm(target ~ log_medv + lstat + nox + ptratio + age_group, data=crime_training_df, family=binomial)

summary(model3)



```

### Approach 2:

### Model 1: Baseline Logistic Regression


```{r}
### Model Building
# Model 1: Baseline Logistic Regression
pj_model1 <- glm(target ~ zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + lstat + medv, 
              data = pj_crime_training_df, family = binomial)
summary(pj_model1)
```


### Model 2: Stepwise Logistic Regression

```{r}
#Model 2: Stepwise Logistic Regression
pj_model2 <- step(glm(target ~ ., data = pj_crime_training_df, family = binomial), direction = "both")
summary(pj_model2)
```

#### Model 3: Logistic Regression with Transformed Variables

```{r}
# Model 3: Logistic Regression with Transformed Variables
pj_model3 <- glm(target ~ log_zn + log_indus + chas + nox + rm + age_group + dis_scaled + log_rad + log_tax + ptratio + log_lstat + log_medv, 
              data = pj_crime_training_df, family = binomial)
summary(pj_model3)
``` 

### Approach 3:

#### Model 1: Baseline Predictions

This model includes six variables identified through exploratory data analysis and correlation inspection:
nox, dis, tax, rad, ptratio, and lstat. These were selected based on their relatively strong correlation with the binary crime outcome and theoretical reasoning. For example, higher pollution (nox) and tax rates (tax) may be indicative of urban density, which could correlate with higher crime, while greater distance to employment centers (dis) might have a protective effect. This model provides a straightforward process using untransformed, raw features.

```{r}
zr_model_a <- glm(target ~ nox + dis + tax + rad + ptratio + lstat, 
                  data = crime_training_zr, family = binomial)
```


#### Model 2: Log-Transformed Predictors

This model includes a broader set of variables, with several of them log-transformed:
log_tax, log_dis, log_zn, log_lstat, as well as nox, rm, ptratio, rad, chas, and age.
This method retains the core features from Model 1 but adjusts for non-normality and skewness observed in variables like tax, zn, dis, and lstat. These features exhibited right-skewed distributions and extreme values, which could affect model stability. Applying log transformation helps to reduce the impact of outliers, normalize distributions, and potentially improve model performance.

```{r}
zr_model_b <- glm(target ~ log_tax + log_dis + log_zn + log_lstat + nox + rm + ptratio + rad + chas + age,
                  data = crime_training_zr, family = binomial)
```


#### Model 3: Binned Variables Logistic Regression

This model includes age and distance as categorical bins based on quantiles, plus tax, rad, ptratio, nox, and rm. These features were selected based on exploratory analysis and their correlation with the target. Binning helps reduce skewness and simplify interpretation.

```{r}
zr_model_c <- glm(target ~ zr_age_bin + zr_dis_bin + tax + rad + ptratio + nox + rm, 
                  data = crime_training_zr, family = binomial)
```


###  Model using Principal Components

This section uses the correlation plot to perform Principal Component Analysis on the two large variable clusters shown in the plot. We will then substitute the variables in each of the two clusters with their respective PC scores in our model.

First we use a correlation plot to visualize the possible clusters that could be used to select variables for our principle components.

```{r check-for-correlation-clusters, echo=FALSE}

df_training_subset <- df_training_one_hot |>
  subset(select = -c(target)) |>
  mutate(chas = as.numeric(chas))

df_training_cor <- round(cor(df_training_subset, method = c("spearman")), 3)

par(mar = c(5, 4, 4, 2) + 0.1)
corrplot(df_training_cor, method="shade", order="hclust", addrect=4)

#df_training_cor |>  plot_correlation(type = "all")
```

Using "hclust", our corrplot shows four distinct groups, each with strong correlation between the parameters within each group. This suggests that we may want to select indus, tax, lstat, ptratio, and radq_hi for one set of our principal component analysis and rm, medv, zn, dis.

```{r principal-component-analysis, echo=FALSE}

# Create PCA from first cluster in our correlation plot
df_pca_subset1 <- df_training_one_hot |>
  subset(select = c(indus, tax, lstat, nox, age, ptratio, radq_hi))

# calculate PCA
df_training_pca1 <- prcomp(df_pca_subset1, scale=TRUE)

# Create PCA from second cluster in our correlation plot
df_pca_subset2 <- df_training_one_hot |>
  subset(select = c(rm, medv, zn, dis))

# calculate PCA
df_training_pca2 <- prcomp(df_pca_subset2, scale=TRUE)

# use eigen vectors to plot % of data explained by PCA1
eig_plot1 <- fviz_eig(df_training_pca1, addlabels=TRUE, ylim=c(0, 70)) + 
  theme(text = element_text(size = 8),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7),
        plot.title = element_text(size = 10))

# plot PCA biplot
pca_biplot1 <- fviz_pca_biplot(df_training_pca1, label="var", habillage =  df_training_one_hot$target) + 
  theme(text = element_text(size = 8),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7),
        plot.title = element_text(size = 10))

# use eigen vectors to plot % of data explained by PCA2
eig_plot2 <- fviz_eig(df_training_pca2, addlabels=TRUE, ylim=c(0, 70)) + 
  theme(text = element_text(size = 8),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7),
        plot.title = element_text(size = 10))

# plot PCA biplot
pca_biplot2 <- fviz_pca_biplot(df_training_pca2, label="var", habillage =  df_training_one_hot$target) + 
  theme(text = element_text(size = 8),
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 7),
        plot.title = element_text(size = 10))

grid.arrange(eig_plot1, pca_biplot1, eig_plot2, pca_biplot2, ncol = 2, nrow = 2)

# add pca's to our dataset
df_training_one_hot_pca <- df_training_one_hot |>
  subset(select = c(target, chas, radq_low)) |>
  mutate(
    group1_pc1 = df_training_pca1$x[,"PC1"],
    group1_pc2 = df_training_pca1$x[,"PC2"],
    group2_pc1 = df_training_pca2$x[,"PC1"],
    group2_pc2 = df_training_pca2$x[,"PC2"],
  ) 

```

The Scree plots on the left use Eigenvalues to show percentage of explained variance for each principle component  from each of our selected groups, while the Biplot shows the positions of observations and how the original variables contribute to the first two principal components. Based on these plots, we see the first principal component makes up 61% of the variance for group 1 (indus, tax, lstat, ptratio, and radq_hi), while the second principal component makes up only 14.3%. The first principal component makes up 56.6% of the variance for group 2 (rm, medv, zn, dis) and the second 27.9% .

```{r principal-component-model}
model_pca <- glm(target ~., binomial(link = "logit"), data=df_training_one_hot_pca)
summary(model_pca)
```

Interestingly, only the primary principal component from group1 and the secondary principal component from group two have strong statistical significance. `radq_low` has a particularly high p-value and should be considered for removal.

```{r pca2}
model_pca2 <- update(model_pca, . ~ . - radq_low)

model_pca2 <- update(model_pca2, . ~ . - chas)

model_pca2 <- update(model_pca2, . ~ . - group1_pc2)

model_pca2 <- update(model_pca2, . ~ . - group2_pc1)
summary(model_pca2)
```

### Model comparison

```{r full-model-lrstats}

library(vcdExtra)
library(pscl)
#models <- list(model_full, model_full_log, backward_model, backward_log_model, model_corr, model_pca)

stats <- LRstats(model_full, model_pca, model1, model2, model3, pj_model1, pj_model2, pj_model3, zr_model_a, zr_model_b, zr_model_c)

stats$McFaddenR2 <- NA 
stats$Accuracy <- NA 
stats$Precision <- NA 
#stats$Recall <- NA 
stats$Sensitivity <- NA 
stats$Specificity <- NA 
stats$F1_score <- NA 
stats$AUC <- NA 
stats$CV_est_predict_err <- NA 
stats$CV_adj_est <- NA 

enhanceEvaluationMetrics <- function(df, model_name) {
  model <- get(model_name)
  
  if (model_name == "model_pca") {
    model_data <- df_training_one_hot_pca
  } else if (model_name == "model_full" | model_name == "backward_model") {
    model_data <- df_training_one_hot
  } else if (model_name == "pj_model1" | model_name == "pj_model2" | model_name == "pj_model3") {
    model_data <- pj_crime_training_df
  } else if (model_name == "zr_model_a" | model_name == "zr_model_b" | model_name == "zr_model_c") {
    model_data <- crime_training_zr
  } else {
    model_data <- crime_training_df
  }
  
  df[model_name, "McFaddenR2"] <- pR2(model)["McFadden"]
  
  pred_probs <- predict(model, type = "response")
  
  pred_probs_factor <- as.factor(ifelse(pred_probs > 0.5, 1, 0))
  conf_matrix <- confusionMatrix(pred_probs_factor, as.factor(model_data$target))
  df[model_name, "Accuracy"] <- conf_matrix$overall['Accuracy']
  df[model_name, "Precision"] <- conf_matrix$byClass['Precision']
  #df[model_name, "Recall"] <- conf_matrix$byClass['Recall']
  df[model_name, "F1_score"] <- conf_matrix$byClass['F1']
  df[model_name, "Sensitivity"] <- conf_matrix$byClass["Sensitivity"] 
  df[model_name, "Specificity"] <- conf_matrix$byClass["Specificity"]

  #roc_model <- roc(as.factor(model_data$target), pred_probs)
  #plot(roc_model, main = "ROC Curve using pROC", col = "red", lwd = 2)
  # roc_auc not working, so use MLmetrics
  df[model_name, "AUC"] <- MLmetrics::AUC(y_true = model_data$target, y_pred = pred_probs) 
  
  # Cross-Validation using 10 folsds
  cv_result <- boot::cv.glm(model_data, model, K= 10)
  df[model_name, "CV_est_predict_err"] <- cv_result$delta[1]
  df[model_name, "CV_adj_est"] <- cv_result$delta[2]
  
  return(df) 
}


# Loop through the list of models and update the dataframe for each
for (model_name in rownames(stats)) {
  
  stats <- enhanceEvaluationMetrics(stats, model_name)
}

stats 
```

### Model Evaluation:

Evaluate model performance and select the best model based on multiple criteria.

Evaluation Metrics:
Accuracy: (TP + TN) / (TP + TN + FP + FN)
Precision: TP / (TP + FP)
Recall (Sensitivity): TP / (TP + FN)
Specificity: TN / (TN + FP)
F1 Score: 2 * (Precision * Recall) / (Precision + Recall)
AUC-ROC Curve: Evaluate model discrimination.


For logistic regression, the "prediction error" is the mean squared error (difference between the predicted probabilities and the actual outcomes).

## MODEL SELECTION

### Checking the Model's Conditions

We will examine the following key conditions for fitting a logistic model:

1. dependent variable is binary 
2. large enough sample 
3. observations are independent, not matched
4. independent (predictor) variables do not correlate too strongly with each other 
5. linearity of independent variables and log odds
6. no outliers in data

As a result, Model 2: Stepwise Logistic Regression was selected as the best binary logistic regression model due to achieving a trade-off between model simplicity and performance. The optimal model not only has to perform excellently in prediction but also be interpretable and extendable to new data. Although a complicated model will provide marginally better performance, it will overfit if too many extraneous parameters are introduced. Therefore, we selected Model 2 since it achieves a balance between parsimony and performance as it retains the strongest predictors only. Stepwise logistic regression (direction = "both") reduced the model by selecting the optimal subset of features and hence giving a more efficient and stable model.

To compare Model 2, we employed a range of statistical measures that assess different aspects of performance. Akaike Information Criterion (AIC), on which the model fit will be judged, was 176.81, reflecting high performance compared to other models. Area Under the Curve (AUC) of 0.9843 reflects that the model was working very well to distinguish between the two classes. Accuracy (0.9372) also reflects that the model is correctly classifying the majority of the cases and classification error rate (0.0644) is low, reflecting high reliability.

Accuracy, recall or sensitivity, and specificity also act to define Model 2's performance. The accuracy of the model at 0.9451 means that whenever it is positive, it is correct 94.51% of the time. The sensitivity at 0.9356 means that it identifies 93.56% of real positive cases correctly, and the specificity at 0.9295 means 92.95% of non-positive cases are identified correctly. The 0.9372 F1 score as a compromise between the recall and precision measures how good the model is at predicting things correctly. The confusion matrix also confirms that the false positives and false negatives are zero, yet again proving correct.

Lastly, Model 2 was chosen since it is the optimal compromise among predictiveness, interpretability, and model fit. It has very high AUC value, good specificity and sensitivity, and very low classification error, and hence very dependable in binary classification. Stepwise selection of removing extraneous predictors avoids overfitting but not super-predictiveness. Due to its low AIC, good performance on a range of measures of evaluation, and relatively well-scaled set of predictors, Model 2 is optimal to be utilized in this analysis.


### Apply the Best Model to Evaluation Data

Once the best model is selected, we use it for prediction on crime_evaluation_df.


```{r}
# Apply same transformations as before 
# Apply the same transformations to the evaluation dataset as used in training
crime_evaluation_df$log_tax <- log(crime_evaluation_df$tax)
crime_evaluation_df$log_lstat <- log(crime_evaluation_df$lstat)
crime_evaluation_df$log_medv <- log(crime_evaluation_df$medv)

# If scaling or categorical transformations were applied, replicate them here
crime_evaluation_df$age_group <- cut(crime_evaluation_df$age, breaks = c(0, 35, 70, 100), labels = c("young", "middle-aged", "old"))
crime_evaluation_df$dis_scaled <- scale(crime_evaluation_df$dis)
```


```{r}

# Predict probabilities using model1
eval_pred_prob <- predict(pj_model2, newdata = crime_evaluation_df, type = "response")

# Convert probabilities to binary class (0 or 1) using a threshold of 0.5
eval_pred_class <- ifelse(eval_pred_prob > 0.5, 1, 0)

# Add predictions to the evaluation data
crime_evaluation_df$predicted_prob <- eval_pred_prob
crime_evaluation_df$predicted_class <- eval_pred_class


```


```{r}
# use the predicted_class and predicted_prob values as your final output

head(crime_evaluation_df[, c("predicted_prob", "predicted_class")])


```

This ensures that Model 2: Stepwise Logistic Regression is applied consistently and provides accurate predictions on the evaluation dataset.

## CONCLUSION

Applying the selected Stepwise Logistic Regression model (Model 2) to the test dataset allows us to experiment with its performance in real life. The model provides us with predicted probabilities and binary classification and allows us to determine the probability of an observation to belong to a particular class. Since Model 2 has been performing well on the training set—indicated by its high AUC (0.9843), high precision, and well-balanced sensitivity and specificity—we can anticipate the same in the case of the evaluation set.

However, despite the fact that the model generalized perfectly in training, it is of the utmost importance to check whether the model generalizes perfectly to novel data. In case the performance in testing has a high correlation with the performance in training, we could be certain that the model does an excellent job in identifying the patterns of data. In case we encounter a steep drop in accuracy or precision and other key statistics, it could be an indication of potential overfitting.

In general, if Model 2 shows high predictive accuracy on test data, then we are content with its performance. Further testing, e.g., investigation of miss-classified cases or adjusting the probability cut-off point, would most likely be performed in actual releases. The next thing to do is then to look at the confusion table and key measures on the test data to check whether this model still performs better.
