---
title: "Assignment3_Data621"
author: "Mubashira Qari"
date: "2025-03-09"
output: html_document
---

### Load Libraries

```{r global_options, include=FALSE}
# Load required packages
#install.packages("htmltools", dependencies = TRUE)
library(htmltools)
library(caret)
library(pROC)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(skimr)
require(DataExplorer)
require(miscTools)
#require(MASS)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 
```

### Loading Datasets

### Variables in the Dataset:
• zn: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)  
• indus: proportion of non-retail business acres per suburb (predictor variable)  
• chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)  
• nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)  
• rm: average number of rooms per dwelling (predictor variable)  
• age: proportion of owner-occupied units built prior to 1940 (predictor variable)  
• dis: weighted mean of distances to five Boston employment centers (predictor variable)  
• rad: index of accessibility to radial highways (predictor variable)  
• tax: full-value property-tax rate per $10,000 (predictor variable)  
• ptratio: pupil-teacher ratio by town (predictor variable)  
• lstat: lower status of the population (percent) (predictor variable)  
• medv: median value of owner-occupied homes in $1000s (predictor variable)  
• target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)  

###  load the dataset and understand its structure.

```{r}

crime_training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-training-data_modified.csv")

crime_evaluation_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-evaluation-data_modified.csv")

head(crime_evaluation_df)



```


### Exploratory Data Analysis

```{r}
#dim(crime_training_df)
skim(crime_training_df)

```

1. Missing Values & Completeness Rate
The n_missing column shows that there are no missing values (0) for any variable, meaning we don’t need to perform imputation.
The complete_rate column confirms this, as all variables have a completeness rate of 1, meaning every row has a value for these variables.

2. Descriptive Statistics

Mean (mean): The average value of each variable.
Standard Deviation (sd): Measures the spread of values.
Minimum (p0): The lowest observed value (0th percentile).
First Quartile (p25): The 25th percentile, where 25% of the values are below this number.
Median (p50): The 50th percentile (the middle value).
Third Quartile (p75): The 75th percentile, where 75% of the values are below this number.

Key Observations:

Crime Rate Target (target)

The median (p50) is 0, indicating that more than half of the data points fall in the low-crime category (target = 0).

Median Home Value (medv)
Mean = 22.59 ($22,590 in $1000s), Median = 21.2.
The range (p0 = 5, p75 = 25) suggests that most homes are valued between $5,000 and $25,000 (in $1000s).
The standard deviation (9.23) indicates a relatively high spread in home values.

Lower Status Population (lstat)
Mean = 12.63%, Median = 11.93%.
A positively skewed distribution (p0 = 1.73, p75 = 16.93), meaning some areas have much higher lower-status populations than others.

Property Tax Rate (tax)
High variance (Mean = 409.5, SD = 167.9).
Large difference between the 25th percentile (281) and 75th percentile (666), suggesting significant variability in tax rates among neighborhoods.

Average Number of Rooms (rm)
Mean = 6.29, Median = 6.21, with a relatively small spread (SD = 0.70).
Indicates most homes have around 6 rooms.

Distance to Employment Centers (dis)
Median = 3.19, but the 25th percentile is quite low (2.10), meaning some neighborhoods are much closer to employment centers than others.
Higher standard deviation (2.1) suggests some neighborhoods are much more remote.

Industrial Land Proportion (indus)
Mean = 11.10, Median = 9.69, and right-skewed distribution (p0 = 0.46, p75 = 18.1).
Some areas have much higher proportions of industrial land, potentially influencing crime.

Highway Accessibility (rad)
Highly right-skewed: The median is 5, but the 75th percentile is 24, meaning some neighborhoods have much greater access to highways than others.
This might be an important predictor for crime.

Potential Data Transformations
zn, indus, tax, rad, lstat, and medv show skewness, so applying a log transformation might improve normality.
age can be categorized into bins (e.g., young, middle-aged, old) since it ranges from 2.9 to 94.1.
dis has a wide range, so normalization might be needed.

### Checking Binary Logistic Regression Assumptions

Before interpreting results from a binary logistic regression, we must verify three key assumptions:

Independence of Observations
Linearity of the Logit
No Multicollinearity

Now, let's discuss how to check these assumptions and which correlation method is best for your data (which contains both ordinal and continuous variables).

Pearson Correlation, Spearman Rank Correlation, and Kendall’s Tau Rank Correlation are all methods used to measure the strength and direction of relationships between variables.

However, they differ in terms of their assumptions, use cases, and how they quantify relationships.

Pearson Correlation: Suitable for continuous data when you want to measure linear associations.

Spearman Rank Correlation: AAppropriate for both continuous and ordinal data. Particularly useful when the relationship is expected to be monotonic but not necessarily linear.

Kendall’s Tau Rank Correlation: Suitable for both continuous and ordinal (ranked) data. Useful when the data may not follow a linear relationship.

### Final Choice: Spearman's Rank Correlation
Since your data has both ordinal and continuous variables, Spearman's correlation is the best choice because:

It does not assume normality (unlike Pearson).
It can handle ordinal variables (like chas, rad).
It is more robust against outliers than Pearson.

### Checking Independence Assumption Using Spearman's Correlation in Binary Logistic Regression

#### What is the Independence Assumption?
The independence assumption in binary logistic regression states that each observation (row) in the dataset should be independent of the others. This means:

No duplicated data points (e.g., same neighborhood appearing multiple times).
No clustered observations (e.g., observations grouped by region, time, or other factors).
No strong correlations between residuals of observations, meaning observations do not systematically affect each other.

### Why Use Spearman’s Correlation?
Spearman's correlation measures monotonic relationships between variables, making it suitable when we have a mix of ordinal and continuous predictors.
If there is high correlation between observations, it suggests possible dependence (e.g., neighborhoods with similar crime rates).

### Compute Spearman's Correlation Between Observations
We calculate the Spearman correlation matrix for all numeric variables (excluding target), which tells us if some variables are highly dependent (correlated).

```{r}
# Load necessary library
library(corrplot)
library(dplyr)

crime_training_df <- crime_training_df %>%
  mutate(across(where(is.character), as.numeric))

# Verify again
str(crime_training_df)

###  Visualize Correlation Matrix

# Compute Spearman correlation matrix
numeric_data <- crime_training_df %>% select(where(is.numeric), -target)
spearman_cor <- cor(numeric_data, method = "spearman", use = "pairwise.complete.obs")

# Plot the Spearman correlation matrix with labels
corrplot(spearman_cor, 
         method = "color",         # Color-coded correlation plot
         type = "upper",           # Show only upper triangle
         tl.cex = 0.8,             # Adjust text size of axis labels
         addCoef.col = "black",    # Add correlation values in black
         number.cex = 0.7,         # Adjust correlation label size
         col = colorRampPalette(c("blue", "white", "red"))(200) # Color gradient
)



```
### Checking for Violations of Independence Assumption
Independence assumes that each observation is not influenced by another.
If entire rows or columns have high correlations, this suggests potentially grouped observations that might violate the independence assumption.
In the plot, we do not see entire rows or columns dominated by high correlations, which is a good sign that no groups are overly dependent.


### Checking for Multicollinearity (High Correlation Between Predictors)
A general rule of thumb is that if |𝜌| > 0.7, it indicates strong correlation between variables, which can lead to multicollinearity in the logistic regression model.

From the matrix:
indus & nox (𝜌 = 0.79) → Strong positive correlation, meaning they provide redundant information.
tax & rad (𝜌 = 0.70) → These variables are highly correlated, indicating one may be removed.
lstat & medv (𝜌 = -0.85) → Very strong negative correlation; keeping both might be problematic.
log_medv & medv (𝜌 = 1.00) → Perfect correlation (since log transformation was applied), meaning one must be removed to prevent redundancy.

Final Takeaways
High correlation between predictors (|𝜌| > 0.7) indicates potential multicollinearity. Consider removing indus, rad, or medv to improve model stability.
No evidence of entire rows/columns being highly correlated, suggesting no major independence violations.
Use VIF for confirmation and decide on variable selection accordingly.


### Checking Multicollinearity Using Variance Inflation Factor (VIF) in R
Variance Inflation Factor (VIF) helps quantify multicollinearity by measuring how much the variance of regression coefficients is inflated due to correlation among predictors. 
A VIF > 5 (or more conservatively, VIF > 10) suggests severe multicollinearity.

### Check VIF (Variance Inflation Factor)
VIF helps confirm whether these high correlations affect regression coefficients.

```{r}
library(car)
vif(glm(target ~ nox + age + rad + tax + dis + zn + medv, family = binomial, data = crime_training_df))

```
Conclusion
 There is NO severe multicollinearity (all VIF values are below 5).
 No immediate need to drop variables based on VIF.
 The variable dis (VIF = 3.58) shows moderate correlation with other predictors, but it's not problematic.
 
 Next Steps:

Keep all predictors in the model.
If we suspect redundancy, check pairwise correlations again or test removing dis to see if model performance improves.

### Checking the Assumption of Linearity of the Logit for Binary Logistic Regression

In logistic regression, we assume that each continuous predictor has a linear relationship with the log-odds (logit) of the target variable. 
If this assumption is violated, the model may be misleading or inaccurate.

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)

# Fit the logistic regression model
model <- glm(target ~ nox + age + rad + tax + dis + zn + medv, 
             family = binomial, data = crime_training_df)

# Compute predicted probabilities
crime_training_df$predicted_prob <- predict(model, type = "response")

# Compute logit (log-odds) transformation
crime_training_df$logit <- log(crime_training_df$predicted_prob / 
                              (1 - crime_training_df$predicted_prob))

# Reshape data for faceting
plot_data <- crime_training_df %>%
  select(logit, nox, age, rad, tax, dis, zn, medv) %>%
  pivot_longer(cols = -logit, names_to = "Predictor", values_to = "Value")

# Create faceted scatter plot
ggplot(plot_data, aes(x = Value, y = logit)) +
  geom_point(alpha = 0.5, color = "blue") + 
  geom_smooth(method = "loess", color = "red", se = FALSE) + 
  facet_wrap(~ Predictor, scales = "free") +
  theme_minimal() +
  labs(title = "Linearity of Logit Check for Binary Logistic Regression",
       x = "Predictor Variable",
       y = "Logit of Predicted Probability")

```
### Interpretation of the Linearity of Logit Check for Binary Logistic Regression
This faceted scatter plot assesses the assumption of linearity of the logit for binary logistic regression. 
The blue dots represent the relationship between each predictor variable and the logit of the predicted probability, while the red LOESS (Locally Estimated Scatterplot Smoothing) curve helps visualize patterns.

### Overall Conclusion
Several predictors (e.g., dis, medv, tax, zn) violate the linearity of logit assumption.

### Data Preparation

This step involves cleaning and transforming data to improve model performance.

Tasks:
Handle missing values:
If missing values exist, replace with the mean/median or use imputation methods.

Feature engineering:
Log transformations: medv, lstat, and dis might benefit from log transformations to reduce skewness.
Binning: Convert age into categorical buckets (e.g., young, middle-aged, old).
Interactions: Create new features (e.g., lstat*medv to capture relationships between income and home values).
Standardization: Normalize numerical variables to bring them to the same scale.

```{r}
# Handle missing values (if any)
#crime_training_df$medv[is.na(crime_training_df$medv)] <- median(crime_training_df$medv, na.rm = TRUE)

# Fill missing values with median
crime_training_df$medv[is.na(crime_training_df$medv)] <- median(crime_training_df$medv, na.rm = TRUE)
crime_training_df$lstat[is.na(crime_training_df$lstat)] <- median(crime_training_df$lstat, na.rm = TRUE)
crime_training_df$dis[is.na(crime_training_df$dis)] <- median(crime_training_df$dis, na.rm = TRUE)

# Log transformation
#crime_training_df$log_medv <- log(crime_training_df$medv + 1)

crime_training_df <- crime_training_df %>%
  mutate(
    log_medv = log(medv + 1),  # Avoid log(0)
    log_lstat = log(lstat + 1),
    log_dis = log(dis + 1)
  )


# Standardization
#crime_training_df$zn <- scale(crime_training_df$zn)
#crime_training_df$indus <- scale(crime_training_df$indus)

#Standardization (Normalize Continuous Variables)
#Standardization ensures that variables are on the same scale, improving model stability.
crime_training_df <- crime_training_df %>%
  mutate(
    zn_scaled = scale(zn),
    indus_scaled = scale(indus),
    nox_scaled = scale(nox),
    rm_scaled = scale(rm),
    age_scaled = scale(age),
    dis_scaled = scale(dis),
    rad_scaled = scale(rad),
    tax_scaled = scale(tax),
    ptratio_scaled = scale(ptratio)
  )


# Create categorical age groups bins
crime_training_df$age_group <- cut(crime_training_df$age, breaks=c(0, 30, 60, 100), labels=c("Young", "Middle-aged", "Old"))

```


```{r}
# Create Interaction Terms
crime_training_df <- crime_training_df %>%
  mutate(
    lstat_medv_interact = log_lstat * log_medv,  # Income & housing price interaction
    tax_rad_interact = tax * rad                # Tax burden & highway accessibility
  )


```


### Building Models

Applying three logistic regression models using different variable selections or transformations.

Approach:
Model 1 (Baseline Model): Use all predictors.
Model 2 (Stepwise Selection): Use stepAIC() to select important variables.
Model 3 (Transformed Variables): Use transformed and interaction variables.

```{r}
# Load library
library(MASS)

# Logistic Regression Model 1 (Baseline)
model1 <- glm(target ~ ., data=crime_training_df, family=binomial)

# Logistic Regression Model 2 (Stepwise Selection)
model2 <- stepAIC(glm(target ~ ., data=crime_training_df, family=binomial), direction="both")

# Logistic Regression Model 3 (With transformations)
model3 <- glm(target ~ log_medv + lstat + nox + ptratio + age_group, data=crime_training_df, family=binomial)

# Summary of models
#summary(model1)
summary(model2)
#summary(model3)

```


### Model Evaluation:
Evaluate model performance and select the best model based on multiple criteria.

Evaluation Metrics:
Accuracy: (TP + TN) / (TP + TN + FP + FN)
Precision: TP / (TP + FP)
Recall (Sensitivity): TP / (TP + FN)
Specificity: TN / (TN + FP)
F1 Score: 2 * (Precision * Recall) / (Precision + Recall)
AUC-ROC Curve: Evaluate model discrimination.


```{r}
# Load necessary library
library(caret)
library(pROC)

# Predict on training data
pred_probs <- predict(model2, type="response")
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)

# Confusion Matrix
conf_matrix <- table(Predicted=pred_classes, Actual=crime_training_df$target)
print(conf_matrix)

# Compute accuracy, precision, recall, F1-score
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- conf_matrix[2,2] / sum(conf_matrix[,2])
recall <- conf_matrix[2,2] / sum(conf_matrix[2,])
f1_score <- 2 * (precision * recall) / (precision + recall)
f1_score

# Print performance metrics
cat("Accuracy:", accuracy, "\nPrecision:", precision, "\nRecall:", recall, "\nF1 Score:", f1_score)

```











