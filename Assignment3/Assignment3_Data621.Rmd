---
title: "Assignment3_Data621"
author: "Mubashira Qari"
date: "2025-03-09"
output: html_document
---

### Load Libraries

```{r global_options, include=FALSE}
# Load required packages
#install.packages("htmltools", dependencies = TRUE)
library(htmltools)
library(caret)
library(pROC)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(skimr)
require(DataExplorer)
require(miscTools)
#require(MASS)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 
```

### Loading Datasets

### Variables in the Dataset:
• zn: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)  
• indus: proportion of non-retail business acres per suburb (predictor variable)  
• chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)  
• nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)  
• rm: average number of rooms per dwelling (predictor variable)  
• age: proportion of owner-occupied units built prior to 1940 (predictor variable)  
• dis: weighted mean of distances to five Boston employment centers (predictor variable)  
• rad: index of accessibility to radial highways (predictor variable)  
• tax: full-value property-tax rate per $10,000 (predictor variable)  
• ptratio: pupil-teacher ratio by town (predictor variable)  
• lstat: lower status of the population (percent) (predictor variable)  
• medv: median value of owner-occupied homes in $1000s (predictor variable)  
• target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)  

###  load the dataset and understand its structure.

```{r}

crime_training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-training-data_modified.csv")

crime_evaluation_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-evaluation-data_modified.csv")

head(crime_evaluation_df)



```


### Exploratory Data Analysis

```{r}
#dim(crime_training_df)
skim(crime_training_df)

```

1. Missing Values & Completeness Rate
The n_missing column shows that there are no missing values (0) for any variable, meaning we don’t need to perform imputation.
The complete_rate column confirms this, as all variables have a completeness rate of 1, meaning every row has a value for these variables.

2. Descriptive Statistics

Mean (mean): The average value of each variable.
Standard Deviation (sd): Measures the spread of values.
Minimum (p0): The lowest observed value (0th percentile).
First Quartile (p25): The 25th percentile, where 25% of the values are below this number.
Median (p50): The 50th percentile (the middle value).
Third Quartile (p75): The 75th percentile, where 75% of the values are below this number.

Key Observations:

Crime Rate Target (target)

The median (p50) is 0, indicating that more than half of the data points fall in the low-crime category (target = 0).

Median Home Value (medv)
Mean = 22.59 ($22,590 in $1000s), Median = 21.2.
The range (p0 = 5, p75 = 25) suggests that most homes are valued between $5,000 and $25,000 (in $1000s).
The standard deviation (9.23) indicates a relatively high spread in home values.

Lower Status Population (lstat)
Mean = 12.63%, Median = 11.93%.
A positively skewed distribution (p0 = 1.73, p75 = 16.93), meaning some areas have much higher lower-status populations than others.

Property Tax Rate (tax)
High variance (Mean = 409.5, SD = 167.9).
Large difference between the 25th percentile (281) and 75th percentile (666), suggesting significant variability in tax rates among neighborhoods.

Average Number of Rooms (rm)
Mean = 6.29, Median = 6.21, with a relatively small spread (SD = 0.70).
Indicates most homes have around 6 rooms.

Distance to Employment Centers (dis)
Median = 3.19, but the 25th percentile is quite low (2.10), meaning some neighborhoods are much closer to employment centers than others.
Higher standard deviation (2.1) suggests some neighborhoods are much more remote.

Industrial Land Proportion (indus)
Mean = 11.10, Median = 9.69, and right-skewed distribution (p0 = 0.46, p75 = 18.1).
Some areas have much higher proportions of industrial land, potentially influencing crime.

Highway Accessibility (rad)
Highly right-skewed: The median is 5, but the 75th percentile is 24, meaning some neighborhoods have much greater access to highways than others.
This might be an important predictor for crime.

Potential Data Transformations
zn, indus, tax, rad, lstat, and medv show skewness, so applying a log transformation might improve normality.
age can be categorized into bins (e.g., young, middle-aged, old) since it ranges from 2.9 to 94.1.
dis has a wide range, so normalization might be needed.


### Assumption Testing for Pearson Correlation on Crime Data

### Step 1: Determine if Variables are Continuous

To determine whether a variable is continuous, we check if it meets the criteria of interval or ratio-level measurement. 

Continuous variables have numerical values that can take any value within a range and have meaningful differences between values.

```{r}
length(unique(crime_training_df$medv))  # Count unique values
length(unique(crime_training_df$lstat))
length(unique(crime_training_df$tax))
length(unique(crime_training_df$rad))

```
```{r}
# Load necessary library
library(dplyr)

# Check data structure
str(crime_training_df)

# Select only numeric variables (excluding binary categorical ones like 'chas' and 'target')
continuous_vars <- crime_training_df %>% select(-chas, -target)

# Display column names of selected continuous variables
colnames(continuous_vars)

```
Interpretation:
All selected variables are numeric, we proceed with further assumption testing.
Some are categorical, we remove them before Pearson correlation.


### Interpretation of the Distributions

Highly Skewed Variables (Right-Skewed)
zn, indus, rad, tax → Log transformation may help normalize these.

Bimodal/Clustered Distributions
dis, age, ptratio → Suggests distinct groups within the dataset.

Normal-Looking Distributions
rm, lstat, medv → These may not require transformation.

All given variables are continuous except chas (binary).



### Step 2: Check Normality for All Variables (Histograms, Q-Q Plots, Shapiro-Wilk Test)

Since Pearson correlation requires normally distributed variables, we visualize their distributions.

Pearson correlation assumes that the variables are normally distributed. We check this using histograms, Q-Q plots, and the Shapiro-Wilk test.

### Variable Data Distribution Chart

```{r}
# Load necessary libraries
library(ggplot2)
library(gridExtra)  # For arranging multiple plots

# Select only predictor variables (excluding 'target')
predictors <- crime_training_df %>% select(-target)

# Create histograms for each predictor variable
plot_list <- lapply(names(predictors), function(var) {
  ggplot(crime_training_df, aes_string(x = var)) +
    geom_histogram(fill = "lightblue", color = "black", bins = 30) +
    theme_minimal() +
    ggtitle(var)
})

# Arrange plots in a grid layout
grid.arrange(grobs = plot_list, ncol = 4)

```
### Boxplots for Outlier Detection

```{r}
crime_training_df %>% 
  pivot_longer(cols = -target, names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Variable, y = Value)) +
  geom_boxplot(fill = "lightblue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

```{r}
# Perform Shapiro-Wilk test for normality on each variable
shapiro_results <- sapply(continuous_vars, function(x) shapiro.test(x)$p.value)

# Display results
shapiro_results
```
Interpretation:

Histogram:
Bell-shaped curve → Normally distributed.
Skewed (long tail) → Not normal (consider transformation).

Shapiro-Wilk Test (p-value > 0.05):
Variable is normally distributed.
Variable is not normal (consider log() or sqrt() transformation).

### Step 3: Check Linearity for All Variable Pairs (Scatter Plots)

Pearson correlation assumes a linear relationship between two variables. We plot scatter plots with regression lines.
```{r}
# Create scatter plots for key variable pairs
plot_list <- list(
  ggplot(crime_training_df, aes(x=lstat, y=medv)) + geom_point() + geom_smooth(method="lm", col="red") + ggtitle("lstat vs medv"),
  ggplot(crime_training_df, aes(x=tax, y=rad)) + geom_point() + geom_smooth(method="lm", col="red") + ggtitle("tax vs rad"),
  ggplot(crime_training_df, aes(x=rm, y=medv)) + geom_point() + geom_smooth(method="lm", col="red") + ggtitle("rm vs medv"),
  ggplot(crime_training_df, aes(x=indus, y=nox)) + geom_point() + geom_smooth(method="lm", col="red") + ggtitle("indus vs nox")
)

# Arrange scatter plots in a grid
grid.arrange(grobs = plot_list, ncol = 2)

```
Interpretation:
Points form a straight-line pattern, the assumption of linearity holds.
Since the relationship is not curved, we will not use Spearman correlation testing.

### Step 4: Check Homoscedasticity (Residual Plots)
To ensure constant variance, we plot residuals vs. fitted values for key regression models.

```{r}
# Fit linear models
model1 <- lm(medv ~ lstat, data=crime_training_df)
model2 <- lm(tax ~ rad, data=crime_training_df)

# Create residual plots
par(mfrow = c(1, 2))  # Arrange plots in 1 row, 2 columns
plot(model1$fitted.values, residuals(model1), main="Residual Plot (medv ~ lstat)")
abline(h=0, col="red")

plot(model2$fitted.values, residuals(model2), main="Residual Plot (tax ~ rad)")
abline(h=0, col="red")

```
Interpretation:
If residuals are evenly scattered, homoscedasticity holds.
If residuals form a pattern (fanning out or clustering), consider transformation.

Interpretation of Each Plot
Left Plot: medv ~ lstat (Median Home Value vs. Lower Status Population)
Observations:

Residuals are spread unevenly: More variation at higher fitted values.
Some funneling pattern at higher values → Potential heteroscedasticity.
Top-right outliers → A few extreme values influence the model.

Conclusion:

Mild heteroscedasticity (variance is not constant).
Potential fix: Apply log transformation (log(medv)) or check for influential points.
Right Plot: tax ~ rad (Property Tax vs. Highway Accessibility)
Observations:

Clustered residuals at low fitted values.
One extreme outlier (top-right) → Could be affecting the model.
Residuals not randomly scattered → Signs of heteroscedasticity.

Conclusion:

Clear heteroscedasticity (variance is inconsistent).
Potential fix: Try log transformation (log(tax)) or use robust regression.

```{r}
# Apply log transformation to medv and tax
crime_training_df$log_medv <- log(crime_training_df$medv)
crime_training_df$log_tax <- log(crime_training_df$tax)

# Re-run regression models with log-transformed variables
model1_log <- lm(log_medv ~ lstat, data = crime_training_df)
model2_log <- lm(log_tax ~ rad, data = crime_training_df)

# Plot new residuals
par(mfrow = c(1, 2))  # Arrange in one row, two columns
plot(model1_log$fitted.values, residuals(model1_log), main="Residual Plot (log_medv ~ lstat)")
abline(h=0, col="red")

plot(model2_log$fitted.values, residuals(model2_log), main="Residual Plot (log_tax ~ rad)")
abline(h=0, col="red")

```
### Perform Breusch-Pagan Test for Homoscedasticity

To confirm whether heteroscedasticity persists after transformation.

```{r}
library(lmtest)

# Test for model1 (medv ~ lstat)
bptest(model1_log)

# Test for model2 (tax ~ rad)
bptest(model2_log)

```

p < 0.05, variance is still an issue → we can proceed with robust regression.


### Check for Influential Points (Outliers)
We saw potential outliers in medv ~ lstat and tax ~ rad.
Outliers can distort regression results.

### Outlier Detection Using Cook’s Distance

```{r}
# Calculate Cook’s Distance for both models
cooksd1 <- cooks.distance(model1_log)
cooksd2 <- cooks.distance(model2_log)

# Identify influential points
influential1 <- which(cooksd1 > (4 / length(cooksd1)))
influential2 <- which(cooksd2 > (4 / length(cooksd2)))

# Print potential outliers
print(influential1)
print(influential2)

# Plot Cook's Distance to visualize
par(mfrow = c(1, 2))
plot(cooksd1, type="h", main="Cook's Distance (log_medv ~ lstat)")
abline(h = 4 / length(cooksd1), col = "red")

plot(cooksd2, type="h", main="Cook's Distance (log_tax ~ rad)")
abline(h = 4 / length(cooksd2), col = "red")

```
Left Plot: log_medv ~ lstat
Observations:

Most points have low Cook’s Distance, meaning they are not highly influential.
A few peaks (outliers) exceed the red line, suggesting some observations have a strong influence on the model.

Right Plot: log_tax ~ rad
Observations:

Almost all points are well below the red threshold, meaning no extreme influential points.
A few taller spikes, but they are not exceeding the red line.

### Compute Pearson Correlation (Final Step)

If all assumptions hold, we compute Pearson correlation coefficients (r)

### Correlation Heatmap b/w Variables

```{r}
# Load necessary libraries
library(ggplot2)
library(corrplot)

# Compute correlation matrix (excluding categorical variables like 'chas' and 'target')
crime_corr <- cor(crime_training_df %>% select(-chas, -target))

# Generate heatmap with color legend
corrplot(crime_corr, method="color", type="upper", 
         tl.col="black", tl.srt=45, 
         col=colorRampPalette(c("blue", "white", "red"))(200), # Color gradient from blue to red
         addCoef.col = "black", # Show correlation values
         number.cex = 0.7, # Adjust coefficient text size
         mar = c(1,1,1,1), # Adjust margins
         cl.lim = c(-1, 1) # Ensure legend covers -1 to 1
)


```


### Interpretation of the Variables Correlation:

Predictors with strong correlations may cause multicollinearity.

Example: tax and rad (0.91) → If both are included in a regression model, they might distort predictions.

Including both in the model confuses the algorithm → The model struggles to decide whether crime is caused by:
Highway accessibility (rad)
Property tax rate (tax)

Results in inflated standard errors → Making coefficient estimates unreliable.

Solution: Use only one of them or apply VIF (Variance Inflation Factor) analysis

medv and lstat are highly correlated (-0.74).
If medv predicts crime, lstat might not add much extra value.
We should check which one performs better in logistic regression.

Non-linearity in medv (home value).
medv is not linearly related to crime (from previous scatter plots).
Solution: Use a quadratic term (medv^2) to better capture trends.

```{r}
library(car)
vif(glm(target ~ tax + rad + lstat + medv + indus + nox + dis + age, data = crime_training_df, family = binomial))

```

### Interpretation:

All VIF values are below 5, meaning there is no serious multicollinearity issue in the model.

### Features Highly Related to Crime:

The results below will show which features are most related to crime.

```{r}
cor(crime_training_df$target, crime_training_df %>% select(-chas, -target)) 

```
### Visualizing Correlation with Crime Rate

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(corrr) # For correlation calculations

# Compute correlation of numeric predictors with `target`
correlations <- crime_training_df %>% 
  select(-chas) %>%  # Exclude categorical variable 'chas'
  mutate(target = as.numeric(target)) %>% # Ensure target is numeric
  correlate() %>% 
  focus(target) %>%  # Focus only on correlations with `target`
  arrange(desc(target)) # Sort in descending order

# Convert to dataframe for plotting
correlations_df <- as.data.frame(correlations)

# Plot correlation values as a bar chart
ggplot(correlations_df, aes(x = reorder(term, target), y = target, fill = target)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Flip for better readability
  theme_minimal() +
  labs(title = "Correlation of Each Variable with Crime Rate (target)",
       x = "Predictor Variable",
       y = "Correlation with Crime Rate") +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) # Color gradient


```
Applying the Logistic Regression Model Based on Correlation and VIF Analysis
Step 1: Interpretation of Correlation with Target
From the correlation output, the strongest relationships with the target variable (high crime rate = 1, low crime rate = 0) are:

Positive correlation (higher values → more crime)

nox (0.7261) → Higher nitrogen oxide levels are associated with higher crime.
indus (0.6048) → More industrial land is linked to higher crime.
age (0.6301) → Older houses are associated with higher crime.
rad (0.6281) → More access to highways correlates with higher crime.
tax (0.6111) → Higher property tax correlates with higher crime.
log_tax (0.6139) → The log-transformed tax rate also shows a strong correlation.

Negative correlation (higher values → less crime)

zn (-0.4317) → More residential zoning leads to lower crime.
dis (-0.6187) → Greater distance from employment centers reduces crime.
medv (-0.2706) & log_medv (-0.3586) → Higher median home values reduce crime.
rm (-0.1526) → More rooms per dwelling has a weak negative effect on crime.
Step 2: Selecting Variables Based on VIF and Correlation
Since VIF < 5 for all variables, we do not need to remove any due to multicollinearity.
We will choose the most relevant variables based on correlation strength and logical reasoning.

Final Variable Selection for Logistic Regression
Include positively correlated predictors (indicating higher crime):

nox, indus, age, rad, tax (or log_tax, if needed)
Include negatively correlated predictors (indicating lower crime):

zn, dis, medv (or log_medv, if needed)

```{r}
model1 <- glm(target ~ nox + indus + age + rad + tax + dis + zn + medv, 
             data = crime_training_df, 
             family = binomial)

summary(model1)

```
### Interpreting the Logistic Regression Model Output
After applying the logistic regression model, we analyze the estimated coefficients, standard errors, z-values, and p-values to determine the significance and impact of each predictor on predicting high-crime neighborhoods (target = 1).

Understanding Z-Score and P-Value in Logistic Regression
In logistic regression, we evaluate the significance of each predictor variable using Z-scores and P-values.

The Z-score (or "z value") measures how many standard deviations a coefficient is away from 0.

A higher absolute value of Z indicates that the variable has a stronger effect on the target variable.
If Z is close to 0, it suggests that the predictor has little impact.

The P-value tells us whether the coefficient is statistically significant. 
It answers:
A small P-value (typically < 0.05) means the variable is statistically significant, meaning it likely has a real effect on crime rate.
A high P-value (> 0.05) means the variable is likely not significant, suggesting no strong evidence that it affects crime risk.

Interpretation:
Variable	Estimate (Coefficient)	Std. Error	Z-Score	P-Value	

Intercept	-28.19	4.54	-6.205	5.48e-10	
The base log-odds of high crime in an area when all predictors are zero is very low.

nox	40.71	6.99	5.826	5.69e-09	
Strong positive effect: Higher pollution levels (nox) increase crime risk significantly.

indus	-0.038	0.044	-0.853	0.393	
Not significant: The proportion of non-retail business land has no meaningful effect on crime.

age	0.027	0.010	2.641	0.008	
Significant: Older buildings are associated with higher crime rates.

rad	0.578	0.140	4.116	3.86e-05	
Highly significant: Higher accessibility to highways increases crime risk.

tax	-0.0057	0.0027	-2.095	0.036	
Significant: Higher property taxes slightly decrease crime risk.

dis	0.631	0.217	2.904	0.0037	
Significant: Longer distances from employment centers increase crime risk.

zn	-0.092	0.036	-2.552	0.0107	
Significant: More residential zoning reduces crime risk.

medv	0.065	0.031	2.085	0.037	
Significant: Higher home values increase crime risk slightly.

Interpretation:
Non-Significant Predictor (p > 0.05)
Variable	Estimate (Coefficient)	p-value	
indus (-0.038)	p = 0.39	Not statistically significant. 
The proportion of non-retail business land does not meaningfully impact crime prediction.

AIC (Akaike Information Criterion)?
AIC is a metric used to compare different models and choose the best one. It helps balance model fit and complexity.

Lower AIC = Better Model

A lower AIC means a better balance between accuracy and simplicity.
If adding a variable reduces AIC, it improves the model.
If adding a variable increases AIC, the model is overfitting.

# Key Takeaways & Next Steps
Remove Non-Significant Variable (indus)

Since indus has p = 0.39 (not significant), remove it and re-run the model.
Check Model Performance Again

Lower AIC is better → removing an unnecessary variable might improve it.
Check for Non-Linear Effects

Some variables (like nox, age, medv) might have nonlinear effects. Try:
Log Transformation: log(nox), log(medv)
Polynomial Terms: age^2

### Next Steps: Improving the Model

### Step 1: Remove indus (Not Significant) and Re-run Model
Since indus had a high p-value (0.39), it’s likely not contributing to predictions. 
Let’s drop it and check if AIC improves.

```{r}
model2 <- glm(target ~ nox + age + rad + tax + dis + zn + medv, 
              family = binomial, data = crime_training_df)

summary(model2)  # Check new model summary
AIC(model2)  # Compare AIC with previous model

```
AIC decreases, this model seems better

### Step 2: Check for Nonlinear Effects

Some variables like nox, medv, and age may have nonlinear relationships with crime rate.
We can transform these variables using:

Log transformation: log(nox), log(medv), log(age)
Polynomial terms: age^2

### Log Transformation:

```{r}
crime_training_df$log_nox <- log(crime_training_df$nox)
crime_training_df$log_medv <- log(crime_training_df$medv)
crime_training_df$log_age <- log(crime_training_df$age + 1)  # Avoid log(0)

# Re-run model with transformed variables
model3 <- glm(target ~ log_nox + log_age + rad + tax + dis + zn + log_medv, 
              family = binomial, data = crime_training_df)

summary(model3)  # Check new coefficients & p-values
AIC(model3)  # Compare AIC

```
AIC worsens, we might need other techniques like interactions.

### Step 3: Test Polynomial Features
To check if the effect of age is curved (not linear), we can add age^2.

Polynomial Term for age:

```{r}
crime_training_df$age_sq <- crime_training_df$age^2

# Re-run model
model4 <- glm(target ~ nox + age + age_sq + rad + tax + dis + zn + medv, 
              family = binomial, data = crime_training_df)

summary(model4)  # Check p-values and significance of age_sq
AIC(model4)  # Compare AIC

```
### Model Performance
AIC = 216.85
This is lower than Model 3 (AIC = 231.03) and previous models, meaning Model 4 fits the data better.
Residual Deviance = 198.85 (on 457 degrees of freedom)
This is lower than Model 3 (215.03) → suggesting an improved fit.

### Step 4: Compare All Models & Pick the Best One
After running model2, model3, and model4, compare their AIC values:

```{r}
AIC(model1)  # Original model
AIC(model2)  # Without indus
AIC(model3)  # With log transformations
AIC(model4)  # With polynomial term

```

