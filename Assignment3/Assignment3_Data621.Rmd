---
title: "Assignment3_Data621"
author: "Mubashira Qari"
date: "2025-03-09"
output: html_document
---

### Load Libraries

```{r global_options, include=FALSE}
# Load required packages
#install.packages("htmltools", dependencies = TRUE)
library(htmltools)
library(caret)
library(pROC)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(skimr)
require(DataExplorer)
require(miscTools)
#require(MASS)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 
```

### Loading Datasets

### Variables in the Dataset:
• zn: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)  
• indus: proportion of non-retail business acres per suburb (predictor variable)  
• chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)  
• nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)  
• rm: average number of rooms per dwelling (predictor variable)  
• age: proportion of owner-occupied units built prior to 1940 (predictor variable)  
• dis: weighted mean of distances to five Boston employment centers (predictor variable)  
• rad: index of accessibility to radial highways (predictor variable)  
• tax: full-value property-tax rate per $10,000 (predictor variable)  
• ptratio: pupil-teacher ratio by town (predictor variable)  
• lstat: lower status of the population (percent) (predictor variable)  
• medv: median value of owner-occupied homes in $1000s (predictor variable)  
• target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)  

###  load the dataset and understand its structure.

```{r}

crime_training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-training-data_modified.csv")

crime_evaluation_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment3/crime-evaluation-data_modified.csv")

head(crime_evaluation_df)



```


### Exploratory Data Analysis

```{r}
#dim(crime_training_df)
skim(crime_training_df)

```

1. Missing Values & Completeness Rate
The n_missing column shows that there are no missing values (0) for any variable, meaning we don’t need to perform imputation.
The complete_rate column confirms this, as all variables have a completeness rate of 1, meaning every row has a value for these variables.

2. Descriptive Statistics

Mean (mean): The average value of each variable.
Standard Deviation (sd): Measures the spread of values.
Minimum (p0): The lowest observed value (0th percentile).
First Quartile (p25): The 25th percentile, where 25% of the values are below this number.
Median (p50): The 50th percentile (the middle value).
Third Quartile (p75): The 75th percentile, where 75% of the values are below this number.

Key Observations:

Crime Rate Target (target)

The median (p50) is 0, indicating that more than half of the data points fall in the low-crime category (target = 0).

Median Home Value (medv)
Mean = 22.59 ($22,590 in $1000s), Median = 21.2.
The range (p0 = 5, p75 = 25) suggests that most homes are valued between $5,000 and $25,000 (in $1000s).
The standard deviation (9.23) indicates a relatively high spread in home values.

Lower Status Population (lstat)
Mean = 12.63%, Median = 11.93%.
A positively skewed distribution (p0 = 1.73, p75 = 16.93), meaning some areas have much higher lower-status populations than others.

Property Tax Rate (tax)
High variance (Mean = 409.5, SD = 167.9).
Large difference between the 25th percentile (281) and 75th percentile (666), suggesting significant variability in tax rates among neighborhoods.

Average Number of Rooms (rm)
Mean = 6.29, Median = 6.21, with a relatively small spread (SD = 0.70).
Indicates most homes have around 6 rooms.

Distance to Employment Centers (dis)
Median = 3.19, but the 25th percentile is quite low (2.10), meaning some neighborhoods are much closer to employment centers than others.
Higher standard deviation (2.1) suggests some neighborhoods are much more remote.

Industrial Land Proportion (indus)
Mean = 11.10, Median = 9.69, and right-skewed distribution (p0 = 0.46, p75 = 18.1).
Some areas have much higher proportions of industrial land, potentially influencing crime.

Highway Accessibility (rad)
Highly right-skewed: The median is 5, but the 75th percentile is 24, meaning some neighborhoods have much greater access to highways than others.
This might be an important predictor for crime.

Potential Data Transformations
zn, indus, tax, rad, lstat, and medv show skewness, so applying a log transformation might improve normality.
age can be categorized into bins (e.g., young, middle-aged, old) since it ranges from 2.9 to 94.1.
dis has a wide range, so normalization might be needed.


### Assumption Testing for Pearson Correlation on Crime Data

### Step 1: Determine if Variables are Continuous

To determine whether a variable is continuous, we check if it meets the criteria of interval or ratio-level measurement. 

Continuous variables have numerical values that can take any value within a range and have meaningful differences between values.

```{r}
length(unique(crime_training_df$medv))  # Count unique values
length(unique(crime_training_df$lstat))
length(unique(crime_training_df$tax))
length(unique(crime_training_df$rad))

```
```{r}
# Load necessary library
library(dplyr)

# Check data structure
str(crime_training_df)

# Select only numeric variables (excluding binary categorical ones like 'chas' and 'target')
continuous_vars <- crime_training_df %>% select(-chas, -target)

# Display column names of selected continuous variables
colnames(continuous_vars)

```
Interpretation:
All selected variables are numeric, we proceed with further assumption testing.
Some are categorical, we remove them before Pearson correlation.




### Interpretation of the Distributions

Highly Skewed Variables (Right-Skewed)
zn, indus, rad, tax → Log transformation may help normalize these.

Bimodal/Clustered Distributions
dis, age, ptratio → Suggests distinct groups within the dataset.

Normal-Looking Distributions
rm, lstat, medv → These may not require transformation.

All given variables are continuous except chas (binary).



### Step 2: Check Normality for All Variables (Histograms, Q-Q Plots, Shapiro-Wilk Test)

Since Pearson correlation requires normally distributed variables, we visualize their distributions.

Pearson correlation assumes that the variables are normally distributed. We check this using histograms, Q-Q plots, and the Shapiro-Wilk test.

### Variable Data Distribution Chart

```{r}
# Load necessary libraries
library(ggplot2)
library(gridExtra)  # For arranging multiple plots

# Select only predictor variables (excluding 'target')
predictors <- crime_training_df %>% select(-target)

# Create histograms for each predictor variable
plot_list <- lapply(names(predictors), function(var) {
  ggplot(crime_training_df, aes_string(x = var)) +
    geom_histogram(fill = "lightblue", color = "black", bins = 30) +
    theme_minimal() +
    ggtitle(var)
})

# Arrange plots in a grid layout
grid.arrange(grobs = plot_list, ncol = 4)

```
### Boxplots for Outlier Detection

```{r}
crime_training_df %>% 
  pivot_longer(cols = -target, names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Variable, y = Value)) +
  geom_boxplot(fill = "lightblue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

```{r}
# Perform Shapiro-Wilk test for normality on each variable
shapiro_results <- sapply(continuous_vars, function(x) shapiro.test(x)$p.value)

# Display results
shapiro_results
```
Interpretation:

Histogram:
Bell-shaped curve → Normally distributed.
Skewed (long tail) → Not normal (consider transformation).

Shapiro-Wilk Test (p-value > 0.05):
Variable is normally distributed.
Variable is not normal (consider log() or sqrt() transformation).

### Step 3: Check Linearity for All Variable Pairs (Scatter Plots)

Pearson correlation assumes a linear relationship between two variables. We plot scatter plots with regression lines.
```{r}
# Create scatter plots for key variable pairs
plot_list <- list(
  ggplot(crime_training_df, aes(x=lstat, y=medv)) + geom_point() + geom_smooth(method="lm", col="red") + ggtitle("lstat vs medv"),
  ggplot(crime_training_df, aes(x=tax, y=rad)) + geom_point() + geom_smooth(method="lm", col="red") + ggtitle("tax vs rad"),
  ggplot(crime_training_df, aes(x=rm, y=medv)) + geom_point() + geom_smooth(method="lm", col="red") + ggtitle("rm vs medv"),
  ggplot(crime_training_df, aes(x=indus, y=nox)) + geom_point() + geom_smooth(method="lm", col="red") + ggtitle("indus vs nox")
)

# Arrange scatter plots in a grid
grid.arrange(grobs = plot_list, ncol = 2)

```
Interpretation:
Points form a straight-line pattern, the assumption of linearity holds.
Since the relationship is not curved, we will not use Spearman correlation testing.

### Step 4: Check Homoscedasticity (Residual Plots)
To ensure constant variance, we plot residuals vs. fitted values for key regression models.

```{r}
# Fit linear models
model1 <- lm(medv ~ lstat, data=crime_training_df)
model2 <- lm(tax ~ rad, data=crime_training_df)

# Create residual plots
par(mfrow = c(1, 2))  # Arrange plots in 1 row, 2 columns
plot(model1$fitted.values, residuals(model1), main="Residual Plot (medv ~ lstat)")
abline(h=0, col="red")

plot(model2$fitted.values, residuals(model2), main="Residual Plot (tax ~ rad)")
abline(h=0, col="red")

```
Interpretation:
If residuals are evenly scattered, homoscedasticity holds.
If residuals form a pattern (fanning out or clustering), consider transformation.

Interpretation of Each Plot
Left Plot: medv ~ lstat (Median Home Value vs. Lower Status Population)
Observations:

Residuals are spread unevenly: More variation at higher fitted values.
Some funneling pattern at higher values → Potential heteroscedasticity.
Top-right outliers → A few extreme values influence the model.

Conclusion:

Mild heteroscedasticity (variance is not constant).
Potential fix: Apply log transformation (log(medv)) or check for influential points.
Right Plot: tax ~ rad (Property Tax vs. Highway Accessibility)
Observations:

Clustered residuals at low fitted values.
One extreme outlier (top-right) → Could be affecting the model.
Residuals not randomly scattered → Signs of heteroscedasticity.

Conclusion:

Clear heteroscedasticity (variance is inconsistent).
Potential fix: Try log transformation (log(tax)) or use robust regression.

```{r}
# Apply log transformation to medv and tax
crime_training_df$log_medv <- log(crime_training_df$medv)
crime_training_df$log_tax <- log(crime_training_df$tax)

# Re-run regression models with log-transformed variables
model1_log <- lm(log_medv ~ lstat, data = crime_training_df)
model2_log <- lm(log_tax ~ rad, data = crime_training_df)

# Plot new residuals
par(mfrow = c(1, 2))  # Arrange in one row, two columns
plot(model1_log$fitted.values, residuals(model1_log), main="Residual Plot (log_medv ~ lstat)")
abline(h=0, col="red")

plot(model2_log$fitted.values, residuals(model2_log), main="Residual Plot (log_tax ~ rad)")
abline(h=0, col="red")

```
### Perform Breusch-Pagan Test for Homoscedasticity

To confirm whether heteroscedasticity persists after transformation.

```{r}
library(lmtest)

# Test for model1 (medv ~ lstat)
bptest(model1_log)

# Test for model2 (tax ~ rad)
bptest(model2_log)

```

p < 0.05, variance is still an issue → we can proceed with robust regression.


### Check for Influential Points (Outliers)
We saw potential outliers in medv ~ lstat and tax ~ rad.
Outliers can distort regression results.

### Outlier Detection Using Cook’s Distance

```{r}
# Calculate Cook’s Distance for both models
cooksd1 <- cooks.distance(model1_log)
cooksd2 <- cooks.distance(model2_log)

# Identify influential points
influential1 <- which(cooksd1 > (4 / length(cooksd1)))
influential2 <- which(cooksd2 > (4 / length(cooksd2)))

# Print potential outliers
print(influential1)
print(influential2)

# Plot Cook's Distance to visualize
par(mfrow = c(1, 2))
plot(cooksd1, type="h", main="Cook's Distance (log_medv ~ lstat)")
abline(h = 4 / length(cooksd1), col = "red")

plot(cooksd2, type="h", main="Cook's Distance (log_tax ~ rad)")
abline(h = 4 / length(cooksd2), col = "red")

```
Left Plot: log_medv ~ lstat
Observations:

Most points have low Cook’s Distance, meaning they are not highly influential.
A few peaks (outliers) exceed the red line, suggesting some observations have a strong influence on the model.

Right Plot: log_tax ~ rad
Observations:

Almost all points are well below the red threshold, meaning no extreme influential points.
A few taller spikes, but they are not exceeding the red line.

### Compute Pearson Correlation (Final Step)

If all assumptions hold, we compute Pearson correlation coefficients (r)

### Correlation Heatmap b/w Variables

```{r}
# Load necessary libraries
library(ggplot2)
library(corrplot)

# Compute correlation matrix (excluding categorical variables like 'chas' and 'target')
crime_corr <- cor(crime_training_df %>% select(-chas, -target))

# Generate heatmap with color legend
corrplot(crime_corr, method="color", type="upper", 
         tl.col="black", tl.srt=45, 
         col=colorRampPalette(c("blue", "white", "red"))(200), # Color gradient from blue to red
         addCoef.col = "black", # Show correlation values
         number.cex = 0.7, # Adjust coefficient text size
         mar = c(1,1,1,1), # Adjust margins
         cl.lim = c(-1, 1) # Ensure legend covers -1 to 1
)


```


### Interpretation of the Variables Correlation:

Predictors with strong correlations may cause multicollinearity.

Example: tax and rad (0.91) → If both are included in a regression model, they might distort predictions.

Including both in the model confuses the algorithm → The model struggles to decide whether crime is caused by:
Highway accessibility (rad)
Property tax rate (tax)

Results in inflated standard errors → Making coefficient estimates unreliable.

Solution: Use only one of them or apply VIF (Variance Inflation Factor) analysis

medv and lstat are highly correlated (-0.74).
If medv predicts crime, lstat might not add much extra value.
We should check which one performs better in logistic regression.

Non-linearity in medv (home value).
medv is not linearly related to crime (from previous scatter plots).
Solution: Use a quadratic term (medv^2) to better capture trends.


###  logistic regression model including both tax and rad

Let's say we run a logistic regression model including both tax and rad:
```{r}
model <- glm(target ~ tax + rad + lstat + medv, data=crime_training_df, family=binomial)
summary(model)

```
```{r}
vif(model)
```
### Interpretation:

All VIF values are below 5, meaning there is no serious multicollinearity issue in the model.
Both lstat and medv have very small p-values (< 0.001), meaning they are both statistically significant predictors of crime.
lstat has a lower p-value (3.86e-08) than medv (0.00012), meaning lstat has a stronger relationship with crime compared to medv.
Both should be kept in the model unless multicollinearity causes problems.

### Features Highly Related to Crime:

The results below will show which features are most related to crime.

```{r}
cor(crime_training_df$target, crime_training_df %>% select(-chas, -target)) 

```
### Visualizing Correlation with Crime Rate

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(corrr) # For correlation calculations

# Compute correlation of numeric predictors with `target`
correlations <- crime_training_df %>% 
  select(-chas) %>%  # Exclude categorical variable 'chas'
  mutate(target = as.numeric(target)) %>% # Ensure target is numeric
  correlate() %>% 
  focus(target) %>%  # Focus only on correlations with `target`
  arrange(desc(target)) # Sort in descending order

# Convert to dataframe for plotting
correlations_df <- as.data.frame(correlations)

# Plot correlation values as a bar chart
ggplot(correlations_df, aes(x = reorder(term, target), y = target, fill = target)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Flip for better readability
  theme_minimal() +
  labs(title = "Correlation of Each Variable with Crime Rate (target)",
       x = "Predictor Variable",
       y = "Correlation with Crime Rate") +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) # Color gradient


```

