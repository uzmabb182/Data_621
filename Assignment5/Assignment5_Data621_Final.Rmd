---
title: "Assignment5_Data621"
author: "Mubashira Qari, Marco Castro"
date: "2025-04-13"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r, warning = FALSE, message = FALSE, echo=FALSE, include=FALSE}
# Load required packages
library(tidyverse)
library(dplyr)
library(ggplot2)
library(htmltools)
library(knitr)
library(RColorBrewer)
library(DataExplorer)
#library(skimr)
library(recipes)
library(corrplot)
library(MASS)
library(caret)
library(pROC)
library(miscTools)
library(performance)
library(lmtest)
library(mice)
library(glmnet)
library(Metrics) 
library(patchwork)  
library(e1071)
#library(car)
```

## Data Exploration

```{r, warning = FALSE, message = FALSE, echo=FALSE, include=FALSE }

# load the dataset and understand its structure

training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment5/wine-training-data.csv")

evaluation_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment5/wine-evaluation-data.csv")

```

The training dataset has 12,795 observations across 15 columns and an additional INDEX column. For the purposes of our analysis, we will drop the INDEX column. The parameters STARS, AcidIndex, LabelAppeal appear to be categorical, while the remaining 12 variables, including our TARGET variable, appear numerical. The number of cases purchased (TARGET) ranges from 0-8. Roughly 21.4% (2734) of our observations had a TARGET value of zero.

```{r exploration, echo=FALSE}

wine_training_df <- training_df %>%
  subset(select =-c(INDEX)) %>%
  dplyr::select(
    all_of(c("TARGET")),
    sort(setdiff(names(.), c("TARGET")))
  ) %>%
  mutate(
    STARS = as.factor(STARS),
    AcidIndex = as.factor(AcidIndex),
    LabelAppeal = as.factor(LabelAppeal)
  )
 
glimpse(wine_training_df)


numeric_df <- wine_training_df %>%
  dplyr::select(where(is.numeric))  

numeric_cols <- sapply(wine_training_df, is.numeric)
factor_cols <- sapply(wine_training_df, is.factor)

table(wine_training_df$TARGET)
```

### Missing Values

Additionally, eight parameters had missing values ranging from 395 missing values (pH) to 3359 missing values (STARS). Below is the full list of variables with missing values:

```{r missing-values, echo=FALSE}
# Count missing values
missing <- colSums(is.na(wine_training_df))

missing[missing > 0]
```

### Examining Numerical Variables

A review of the summary statistics reveals issues with our data. In particular, nine of the 11 numeric variables show minimum values below zero.  Table 1 shows number of missing values.


```{r data-summaries, echo=FALSE}

summary(wine_training_df[numeric_cols])

```


When put in the context of our specific properties of wine they represent, these negative values appear to be erroneous. For example, we expected `Alcohol Content` to have a minimum value of zero instead of a negative value. The same can be said of the other parameters with negative values (Chlorides, Citric Acid, Fixed Acidity, Free Sulfur Dioxide, Residual Sugar, Sulphates, Total Sulfur Dioxie, and Volatile Acidity). This suggests possible data entry errors or normalization that shifted our actual values to the left. 

```{r negative-values, echo=FALSE}

wine_properties <- tibble::tibble(
  Variable = c(
    "TARGET",
    "Alcohol",
    "Chlorides",
    "CitricAcid",
    "Density",
    "FixedAcidity",
    "FreeSulfurDioxide",
    "ResidualSugar",
    "Sulphates",
    "TotalSulfurDioxide",
    "VolatileAcidity",
    "pH"
  ),
  NormalRange = c(
    "0 or higher",
    "8% – 15% ABV",
    "0.01 – 0.10 g/L",
    "0 – 1.0 g/L",
    "0.990 – 1.005 g/cm³",
    "4 – 9 g/L",
    "10 – 70 mg/L",
    "0 – 45 g/L",
    "0.3 – 1.0 g/L",
    "30 – 150 mg/L",
    "0.2 – 0.8 g/L",
    "2.9 – 4.0"
  )
)

below_zero_counts <-  numeric_df %>%
  summarise(across(everything(), ~ sum(. < 0, na.rm = TRUE)))  %>%
  pivot_longer(
    cols = everything(),      
    names_to = "Variable",    
    values_to = "Rows Below Zero"       
  )


below_zero_counts %>%
  left_join(wine_properties, by = "Variable") %>%
  kable(caption="Number of negative values") 

```

A look at our boxplots shows the IQR's for each parameter are centered around a similar x-axis for each of our case counts. The boxplots confirm the presence of  extreme values at lower as well as on the upper ranges.  It should be noted the IQRs for the affected variables are in line with their corresponding typical ranges according to [VineEnology.com](https://www.vinoenology.com/wine-composition/) as shown in Table 1. 

```{r explore-boxplot-p2, fig.width=7, fig.height=9, echo=FALSE, message = FALSE, warning=FALSE}

numeric_df %>% 
  mutate(TARGET = as.factor(TARGET)) %>% 
  drop_na() %>%
  plot_boxplot(by = "TARGET", title="Boxplots of Target vs Param", ncol = 2)

```


### Examining Categorical Variables

Visualizing the distributions of out categorical variables helps ensure variables are treated as discrete categories, not continuous numbers. Our AcidIndex variable shows that majority of values fall between 6 and 11 with 342 observations collectively making up the remaining values. We may want to bin values for this parameter. Label Appeal has a normal distribution ranging from -2 to 2 and centered around 0. The our Wine rative variable STARS has the most missing values (3359) of any variable; of missing value, 61% of rows had 0 cases purchased. As would be expected, the majority of observations have a low STARS value (1/2), while few observations have a perfect value of 4.

```{r factor-charts, fig.width=7, fig.height=6, echo=FALSE}

# Show bar charts for factors

summary(wine_training_df[factor_cols])

plots <- list()  
i <- 1

for (colname in names(wine_training_df)[factor_cols]) {
  
  if (colname != 'TARGET') {
  
  plot_data <- wine_training_df %>%
    group_by(TARGET, group_var = .data[[colname]]) %>%
    dplyr::summarise(count = n(), .groups = "drop") %>%
    group_by(group_var) %>%
    mutate(
      percent = 100 * count / sum(count),
      label = paste0(round(percent), "%")
    ) %>%
    mutate(TARGET = factor(TARGET, levels = 8:0)) 

factorColors <-  c(
    "0" = "gray",
    "1" = "pink",
    "2" = "cyan",
    "3" = "yellow",
    "4" = "orange",
    "5" = "purple",
    "6" = "green",
    "7" = "blue",
    "8" = "red"
    )

  # Plot   
 plots[[i]]  <- ggplot(plot_data, aes(x = group_var, y = count, fill = TARGET, label = label)) +
    geom_col(position = "stack") +
    geom_text(position = position_stack(0.5), size = 3) +
    labs(
      title = paste("Distribution of", colname, "by TARGET"),
      x = colname,
      y = "Count",
      fill = "# Cases" 
    ) +
   scale_fill_manual(values = factorColors) +
    theme_minimal() +
    theme(
      plot.title = element_blank(),
      axis.title.x = element_text(size = 6),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
    
    i <- i + 1
  }
}

( wrap_plots(plots[1], ncol = 1, guides = "collect") /
wrap_plots(plots[2:3], ncol = 2, guides = "collect") ) & theme(legend.position = "bottom")


layout(1)
par(mfrow = c(1, 1))
```




\newpage
### Visualizing Distributions

Next we will visualize the distributions for our numeric variables. Using the histograms, we can quickly spot skewness, check distribution and value ranges, and identify variables with spikes or unusual spread. The histograms show symmetric unimodal distributions strongly peaked with thin tails across our numeric variables. 

```{r distributions1, fig.width=7, fig.height=7, echo=FALSE, message = FALSE, warning=FALSE}

plots <- list()  
i <- 1

for (colname in names(numeric_df)[numeric_cols]) {
  
  if (!is.na(colname) && colname != "TARGET") {
     plots[[i]]  <- ggplot(numeric_df, aes(x = .data[[colname]])) +
       geom_histogram(fill = "#ffbf00", bins = 40) 
     i <- i + 1
  }
}


wrap_plots(plots, ncol = 3, nrow = 4, guides = "collect")
```

Our skewness test confirms that our numerical variables are nearly symmetrical or almost symmetrical.

```{r skewness}

# Calculate skewness for each numeric variable (using original values)
skew_vals <- sapply(numeric_df |> subset(select=-c(TARGET)), function(x) skewness(x, na.rm = TRUE, type = 3))

# Create a dataframe with skewness
skew_df <- data.frame(
  Variable = names(skew_vals),
  Skewness = skew_vals
)

# Sort by highest absolute skewness
skew_df <- skew_df[order(-abs(skew_df$Skewness)), ]

# Show top 10 most skewed variables (untransformed)
head(skew_df, 10)

```

### Visualizing Relationships Among Variables

Correlation plots help us understand variable relationships and potential multicollinearity. A correlation plot for all variables shows moderate correlation between our dependent variable TARGET and the variables STARS and LabelAppeal and weak correlation between TARGET and AcidIndex.


```{r corrplot1, echo=FALSE}
layout(1)
par(mfrow = c(1, 1))
# Correlation matrix (exclude non-numeric or identifier variables)
cor_matrix <- cor( training_df %>%
  subset(select =-c(INDEX))  %>% select_if(is.numeric), use = "complete.obs")
corrplot(cor_matrix, method = "color", tl.cex = 0.8)

```

STARS, LabelAppeal, and AcidIndex are also three parameters that we identified to be ordinal. The following correlation plot shows the correlation between the remaining numerical parameters. This view also groups the parameters into four clusters that appear to have a relationship with each other. 

```{r corrplot2, echo=FALSE }

par(mfrow = c(1, 1))
cor_matrix <- cor(numeric_df |> subset(select=-c(TARGET)), use = "pairwise.complete.obs", method = "pearson")

corrplot(cor_matrix, method="shade", order="hclust", addrect=4)

```

A Variance Inflation Factor (VIF) test confirms that no major multicollinearity present in between our variables.


```{r vif-scores, echo=FALSE}

# Fit linear model using all remaining features to predict TARGET
full_lm_model <- lm(TARGET ~ ., data = wine_training_df)

car::vif(full_lm_model)

```


\newpage
## Data Preparation

### Handling Negative Values

In the Data Exploration phase, we discovered that nine out of 11 numerical variables had negative values. While Poisson and Negative Binomial Regression will allow negative predictor values, we know that these values are impossible in the read world. Thus, ignoring these values could lead to biased coefficient estimates that could introduce highly misleading relationships in our models. We will assume that these erronous values may be the result of data entry or normalization errors and attempt to address them.

 As we do not know what transformations may have been applied if normalization occurred, we will need to address the negative values through another method. From our earlier observations, we noted that nearly all of the affected parameters had thousands of affected records, with Chlorides having the most affects rows (3197). This means that nearly 1/4 of our 12,795 may be affected one way or another and we would loose too much data if we were to drop the affected records. We will instead only drop the 118 records with for Alcohol Content as it is a low percentage of the dataset since some of these records may also have negative values; we will then set the remaining negative values to N/A, allowing the values to be imputed if desired. Imputing may introduce some bias into our results, but will retain much of our data. 

```{r handle-negative, echo=FALSE}

wine_training_non_neg <- wine_training_df |>
  filter(Alcohol > 0)
  
for (colname in names(wine_training_non_neg)[numeric_cols]) {
  
  if (!is.na(colname) && colname != "TARGET") {
    wine_training_non_neg[[colname]] <- ifelse(wine_training_non_neg[[colname]] < 0, NA, wine_training_non_neg[[colname]])
  }
}




```


### Handling Missing Values

```{r impute-missing, echo=FALSE}

wine_training_imputed <- wine_training_non_neg

# Impute missing values in those columns using median
for (col in names(wine_training_imputed[missing > 0])) {
  median_val <- median(as.numeric(wine_training_imputed[[col]]), na.rm = TRUE)
  wine_training_imputed[[col]][is.na(wine_training_imputed[[col]])] <- median_val
}

kable(
  data.frame(
    'Original Missing Count' = colSums(is.na(wine_training_df)),
    'New Missing Count' = colSums(is.na(wine_training_non_neg)),
    'After Imputation' = colSums(is.na(wine_training_imputed))
  ), caption="Number of missing values"
)

```

### Binned Transformation 

To simplify the effects of AcidIndex, this variable was transformed into categorical bins. This can help reduce the influence of extreme values and better capture non-linear effects in logistic regression.


```{r binning, echo=FALSE}
# Create bins 

wine_training_df$AcidIdx <- cut(as.numeric(wine_training_df$AcidIndex),
                  breaks = c(1, 3, 4, 5, 6, Inf),  
                  labels = c("1-6", "7","8","9", "10+"),
                  include.lowest = TRUE, 
                  right = TRUE)

wine_training_non_neg$AcidIdx <- cut(as.numeric(wine_training_non_neg$AcidIndex),
                  breaks = c(1, 3, 4, 5, 6, Inf),  
                  labels = c("1-6", "7","8","9", "10+"),
                  include.lowest = TRUE, 
                  right = TRUE)

wine_training_imputed$AcidIdx <- cut(as.numeric(wine_training_imputed$AcidIndex),
                  breaks = c(1, 3, 4, 5, 6, Inf),  
                  labels = c("1-6", "7","8","9", "10+"),
                  include.lowest = TRUE, 
                  right = TRUE)


ggplot(wine_training_df, aes(x=AcidIdx, fill=factor(TARGET, levels = 8:0))) +
  geom_bar() +
   scale_fill_manual(values = factorColors) +
    theme_minimal() +
    labs(
      fill = "# Cases" 
    )
```


### Transformations

```{r base_model-diagnostic-plots, fig.width=6, fig.height=3, echo=FALSE}
layout(matrix(1:2, nrow = 1, byrow = TRUE))
plot(full_lm_model, which = c(4, 5), col=wine_training_df$TARGET,  id.n = 5)
par(mfrow = c(1, 1))

```




## Model Building




## Model Selection

