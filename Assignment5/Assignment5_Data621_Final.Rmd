---
title: "Assignment5_Data621_MQ"
author: "Mubashira Qari"
date: "2025-04-13"
output: html_document
---

### Objective:

The goal is to build a count regression model to predict the number of wine cases sold (TARGET), based on wine characteristics.

### Load Libraries

```{r, warning = FALSE, message = FALSE}
# Load required packages
library(htmltools)
library(caret)
library(pROC)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(skimr)
require(DataExplorer)
require(miscTools)
require(MASS)
require(performance)
require(lmtest)
require(mice)
require(glmnet)
require(Metrics) 
library(patchwork)  # for combining ggplots
library(e1071)
library(car)

```

###  load the dataset and understand its structure.

```{r, warning = FALSE, message = FALSE}
# echo=FALSE, include=FALSE

wine_training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment5/wine-training-data.csv")

wine_evaluation_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment5/wine-evaluation-data.csv")

head(wine_training_df)
#head(wine_evaluation_df)

```
```{r}
# Count missing values
any(is.na(wine_training_df))
sum(is.na(wine_training_df))
colSums(is.na(wine_training_df))
```


```{r}

colSums(is.na(wine_training_df))
```
### Identifying and Handling missing Values:

```{r}
# Identify numeric columns with missing values
numeric_cols <- sapply(wine_training_df, is.numeric)
cols_with_na <- colnames(wine_training_df)[numeric_cols & colSums(is.na(wine_training_df)) > 0]

# Impute missing values in those columns using median
for (col in cols_with_na) {
  median_val <- median(wine_training_df[[col]], na.rm = TRUE)
  wine_training_df[[col]][is.na(wine_training_df[[col]])] <- median_val
}
colSums(is.na(wine_training_df))

```
### Visualizing Relationships among Variables:

Correlation plots help us understand variable relationships and potential multicollinearity.


```{r}
# Correlation matrix (exclude non-numeric or identifier variables)
cor_matrix <- cor(wine_training_df %>% select_if(is.numeric), use = "complete.obs")
corrplot(cor_matrix, method = "color", tl.cex = 0.8)


```
```{r}
# Load necessary libraries
library(corrplot)
library(dplyr)

# 1. Select only numeric columns
numeric_df <- wine_training_df %>%
  dplyr::select(where(is.numeric))

# 2. Remove columns with all NA or constant values (optional but recommended)
numeric_df <- numeric_df[, sapply(numeric_df, function(x) length(unique(x[!is.na(x)])) > 1)]

# 3. Compute correlation matrix
cor_matrix <- cor(numeric_df, use = "pairwise.complete.obs", method = "pearson")

# 4. Visualize the correlation matrix
corrplot(cor_matrix,
         method = "color",           # Use colored squares
         type = "upper",             # Show only upper triangle
         tl.cex = 0.8,               # Text label size
         addCoef.col = "black",      # Show correlation values
         number.cex = 0.7,           # Size of the coefficients
         tl.col = "black",           # Text label color
         diag = FALSE)               # Hide diagonal

```
```{r}

library(dplyr)

wine_features <- dplyr::select(wine_training_df, -INDEX)
#wine_features
# Load required package
library(car)

# Fit linear model using all remaining features to predict TARGET
vif_model <- lm(TARGET ~ ., data = wine_features)

# Compute VIF values
vif_values <- vif(vif_model)

# View all VIF values
print(vif_values)

# Optional: Filter features with VIF > 5 (multicollinearity concern)
high_vif <- vif_values[vif_values > 5]
print(high_vif)

# Column positions of high VIF variables in wine_features
which(names(vif_values) %in% names(high_vif))

```
All very close to 1, ranging from ~1.00 to ~1.10

No variable has a VIF > 5

Final filtered output named numeric(0) and integer(0) means:

No variables exceeded the multicollinearity threshold you set (probably VIF > 5)

### Detect Skewed Variables

```{r}
# Load e1071 for skewness
library(e1071)

# 1. Keep only numeric columns
numeric_df <- wine_training_df[sapply(wine_training_df, is.numeric)]

# 2. Remove identifier and target columns if present
numeric_df <- numeric_df[ , !(names(numeric_df) %in% c("INDEX", "TARGET"))]

# 3. Calculate skewness for each numeric variable (using original values)
skew_vals <- sapply(numeric_df, function(x) skewness(x, na.rm = TRUE))

# 4. Create a dataframe with skewness
skew_df <- data.frame(
  Variable = names(skew_vals),
  Skewness = skew_vals
)

# 5. Sort by highest absolute skewness
skew_df <- skew_df[order(-abs(skew_df$Skewness)), ]

# 6. Show top 10 most skewed variables (untransformed)
head(skew_df, 10)

```

AcidIndex	1.65	Highly right-skewed	- consider log() or sqrt()
STARS	0.56	Moderately right-skewed	- Possibly bin or treat as ordinal/factor


```{r}
# Load libraries
library(ggplot2)
library(patchwork)

# Example: If your top skewed vars aren't defined, use dummy fallback
# Replace with actual top skewed variables as needed
top_skewed_vars <- c("ResidualSugar", "TotalSulfurDioxide", "Chlorides")

# Create 3 ggplot histograms
p1 <- ggplot(wine_training_df, aes_string(x = top_skewed_vars[1])) +
  geom_histogram(bins = 30, fill = "#2c7fb8", color = "white") +
  labs(title = paste("Histogram of", top_skewed_vars[1])) +
  theme_minimal()

p2 <- ggplot(wine_training_df, aes_string(x = top_skewed_vars[2])) +
  geom_histogram(bins = 30, fill = "#4daf4a", color = "white") +
  labs(title = paste("Histogram of", top_skewed_vars[2])) +
  theme_minimal()

p3 <- ggplot(wine_training_df, aes_string(x = top_skewed_vars[3])) +
  geom_histogram(bins = 30, fill = "#984ea3", color = "white") +
  labs(title = paste("Histogram of", top_skewed_vars[3])) +
  theme_minimal()

# Display all three plots in one view
(p1 / p2 / p3)  # stacked vertically
# Or side-by-side with: (p1 | p2 | p3)




```
### Feature Engineering:

Some features may benefit from transformation (e.g., reducing skew with log1p), or discretization (e.g., bucketing pH). These transformations help linear models fit better.

### Log Transform on AcidIndex

log1p(x) is equivalent to log(x + 1) and works better with small or zero values.

This helps reduce right skew and stabilize the variance of AcidIndex.

### To visualize the improvement:

```{r}
library(e1071)

# Original skewness
skewness(wine_training_df$AcidIndex, na.rm = TRUE)

# After log
wine_training_df$log_AcidIndex <- log1p(wine_training_df$AcidIndex)
skewness(wine_training_df$log_AcidIndex, na.rm = TRUE)

```
The log transformation working reducing skewness from 1.65 → 0.88 means the distribution became more symmetric and closer to normal, which is ideal for many models (like linear regression, Poisson, etc.).

### Convert STARS to a Factor

```{r}
wine_training_df$STARS_factor <- as.factor(wine_training_df$STARS)

wine_training_df$STARS_ord <- factor(wine_training_df$STARS,
                                      levels = c(1, 2, 3, 4),
                                      ordered = TRUE)
head(wine_training_df)

```

All three display the same numbers, but only STARS_factor and STARS_ord tell R to treat them as categories, not numbers.


### Build Multiple Models

### Poisson Regression – Basic

log_AcidIndex	- Reduced skew, transformed acidity
Alcohol	- Typically positively correlated with wine quality/sales
Density	- Impacts texture; important physical feature
LabelAppeal	- Reflects marketing impact
STARS_factor - Captures perceived expert quality (categorical)

```{r}
# Fit Poisson regression model
poisson_model <- glm(
  TARGET ~ log_AcidIndex + Alcohol + Density + LabelAppeal + STARS_factor,
  family = poisson(link = "log"),
  data = wine_training_df
)

# Show model summary
summary(poisson_model)


```

### Poisson Regression – With interaction

```{r}
# Poisson model with interaction between LabelAppeal and STARS
poisson_model_interact <- glm(
  TARGET ~ log_AcidIndex + Alcohol + Density + LabelAppeal * STARS_factor,
  family = poisson(link = "log"),
  data = wine_training_df
)

# Model summary
summary(poisson_model_interact)

```

### Negative Binomial Regression

Why Apply Negative Binomial Regression?
Poisson Assumes:
The mean = variance of the response (TARGET).

That is, the number of wine cases sold has a similar average and variability.

But in reality:
Count data often shows overdispersion — where the variance > mean.

If we use Poisson under overdispersion:

We get underestimated standard errors

This leads to inflated significance and unreliable inferences

```{r}
# Fit NB model with interaction
nb_model <- glm.nb(
  TARGET ~ log_AcidIndex + Alcohol + Density + LabelAppeal * STARS_factor,
  data = wine_training_df
)

# View summary
summary(nb_model)

```

### Negative Binomial – With interactions

wine sample cases sold (TARGET), which is count data:

Overdispersion (variance > mean) — so Negative Binomial is more appropriate than Poisson.

Some variables (like LabelAppeal) likely behave differently at different star ratings (STARS).

So combining these two ideas:

 We use Negative Binomial + interaction terms to handle both:

The overdispersion in count data

The conditional effect (e.g., the impact of LabelAppeal might depend on STARS)



```{r}

nb_interact_model <- glm.nb(
  TARGET ~ log_AcidIndex + Alcohol + Density + LabelAppeal * STARS_factor,
  data = wine_training_df
)

# View summary
summary(nb_interact_model)


```

### Multiple Linear Regression

Even though TARGET is count-based, linear regression can still be a useful benchmark to compare against Poisson and Negative Binomial models. 
It’s often easier to interpret, and can perform decently if the residuals are not heavily skewed.

```{r}
# Fit Multiple Linear Regression
mlr_model <- lm(
  TARGET ~ log_AcidIndex + Alcohol + Density + LabelAppeal + STARS_factor,
  data = wine_training_df
)

# View summary
summary(mlr_model)

```

### MLR – All variables

Multiple Linear Regression (MLR) model using all available predictors from your wine_training_df. 
This includes all numeric features, and categorical variables like STARS should be properly encoded (as a factor).

```{r}
# Ensure STARS is a factor
wine_training_df$STARS_factor <- as.factor(wine_training_df$STARS)

# Drop unnecessary columns
vars_to_exclude <- c("INDEX", "TARGET", "STARS", "STARS_ord")  # if exists
all_vars <- setdiff(names(wine_training_df), vars_to_exclude)

# Formula for linear regression
mlr_formula <- as.formula(
  paste("TARGET ~", paste(all_vars, collapse = " + "))
)

# Fit the full model
mlr_full_model <- lm(mlr_formula, data = wine_training_df)

# Model summary
summary(mlr_full_model)

```

### Evaluate Models: Compare AIC (for Poisson/NB only)

Poisson, Poisson with interaction, Negative Binomial, Negative Binomial with interaction, and Multiple Linear Regression (MLR with selected and all variables).
Lower AIC = better model among count models.

```{r}

AIC(poisson_model, poisson_model_interact, nb_model, nb_interact_model)


```

### Compare RMSE and MAE (all models)

```{r}
# Calculate RMSE and MAE
rmse_df <- data.frame(
  Model = c("Poisson", "Poisson + Interact", "NB", "NB + Interact", "MLR", "MLR (All Vars)"),
  RMSE = c(
    rmse(wine_training_df$TARGET, predict(poisson_model, type = "response")),
    rmse(wine_training_df$TARGET, predict(poisson_model_interact, type = "response")),
    rmse(wine_training_df$TARGET, predict(nb_model, type = "response")),
    rmse(wine_training_df$TARGET, predict(nb_interact_model, type = "response")),
    rmse(wine_training_df$TARGET, predict(mlr_model)),
    rmse(wine_training_df$TARGET, predict(mlr_full_model))
  ),
  MAE = c(
    mae(wine_training_df$TARGET, predict(poisson_model, type = "response")),
    mae(wine_training_df$TARGET, predict(poisson_model_interact, type = "response")),
    mae(wine_training_df$TARGET, predict(nb_model, type = "response")),
    mae(wine_training_df$TARGET, predict(nb_interact_model, type = "response")),
    mae(wine_training_df$TARGET, predict(mlr_model)),
    mae(wine_training_df$TARGET, predict(mlr_full_model))
  )
)

# View comparison
print(rmse_df)
```

### Interpretation:

AIC (Akaike Information Criterion) balances model fit and complexity — lower AIC is better.

The Poisson model with interaction has the lowest AIC → suggesting best balance between complexity and goodness of fit.

Interestingly, Negative Binomial (NB) models do not significantly outperform Poisson here, suggesting low overdispersion in data.

### Visualization – RMSE and MAE

```{r}
library(tidyr)
library(ggplot2)

# Convert for plotting
rmse_long <- pivot_longer(rmse_df, cols = c("RMSE", "MAE"), names_to = "Metric", values_to = "Value")

# Plot
ggplot(rmse_long, aes(x = reorder(Model, Value), y = Value, fill = Metric)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(
    title = "Model Comparison: RMSE & MAE",
    x = "Model", y = "Error Value",
    fill = "Metric"
  ) +
  theme_minimal(base_size = 13)

```
### Interpretation:

MLR with All Variables has the lowest RMSE and MAE — indicating best predictive accuracy on the training data.

However, MLR assumes normally distributed residuals, which may not be appropriate for count target variable (TARGET).

MLR models also may overfit or underestimate variance for low-count observations.

### Preprocess wine_evaluation_df the same way

```{r}
### Step 1: Preprocess Evaluation Data the Same Way

# Impute missing values using training medians
for (col in names(wine_evaluation_df)) {
  if (is.numeric(wine_evaluation_df[[col]])) {
    missing <- is.na(wine_evaluation_df[[col]])
    if (any(missing)) {
      # Use training median
      train_median <- median(wine_training_df[[col]], na.rm = TRUE)
      wine_evaluation_df[[col]][missing] <- train_median
    }
  }
}

# Step 2: Apply same transformations
wine_evaluation_df$log_AcidIndex <- log1p(wine_evaluation_df$AcidIndex)
wine_evaluation_df$STARS_factor <- as.factor(wine_evaluation_df$STARS)

# Confirm everything matches structure of training data
str(wine_evaluation_df[, c("log_AcidIndex", "Alcohol", "Density", "LabelAppeal", "STARS_factor")])

```

### Apply Model to Evaluation Set

```{r}
# Predict using the best count model — Poisson with interaction
wine_evaluation_df$Predicted_TARGET <- predict(poisson_model_interact,
                                                newdata = wine_evaluation_df,
                                                type = "response")

# Round to whole numbers (optional)
wine_evaluation_df$Predicted_TARGET <- round(wine_evaluation_df$Predicted_TARGET)

```

Distribution of predicted sample case orders (your TARGET variable) for the evaluation dataset, based on your Poisson Regression model with interaction.

```{r}
ggplot(wine_evaluation_df, aes(x = Predicted_TARGET)) +
  geom_histogram(bins = 30, fill = "#2c7fb8", color = "white") +
  labs(
    title = "Distribution of Predicted TARGET for Evaluation Set",
    x = "Predicted Number of Sample Cases",
    y = "Frequency"
  ) +
  theme_minimal(base_size = 13)

```
Interpretation Summary:

Insight	Explanation:
Most wines receive 2–4 predicted cases	These are likely average wines with decent characteristics.
Some wines go above 5–7 predicted cases	Likely top-tier wines (high appeal and expert scores).
Very few wines are predicted < 1 case	Indicates limited perceived demand — maybe weaker features.

