---
title: "Assignment5_Data621"
author: "Mubashira Qari, Marco Castro"
date: "2025-04-13"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r, warning = FALSE, message = FALSE, echo=FALSE, include=FALSE}
# Load required packages
library(tidyverse)
library(dplyr)
library(ggplot2)
library(htmltools)
library(knitr)
library(RColorBrewer)
library(DataExplorer)
#library(skimr)
library(recipes)
library(corrplot)
library(MASS)
library(caret)
library(pROC)
library(miscTools)
library(performance)
library(lmtest)
library(mice)
library(glmnet)
library(Metrics) 
library(patchwork)  
library(e1071)
#library(car)
```

## Data Exploration

```{r, warning = FALSE, message = FALSE, echo=FALSE, include=FALSE }

# load the dataset and understand its structure

training_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment5/wine-training-data.csv")

evaluation_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_621/refs/heads/main/Assignment5/wine-evaluation-data.csv")

```

The training dataset has 12,795 observations across 15 columns and an additional INDEX column. For the purposes of our analysis, we will drop the INDEX column. The parameters STARS, AcidIndex, LabelAppeal appear to be categorical, while the remaining 12 variables, including our TARGET variable, appear numerical. The number of cases purchased (TARGET) ranges from 0-8. Roughly 21.4% (2734) of our observations had a TARGET value of zero.

```{r exploration, echo=FALSE}

wine_training_df <- training_df %>%
  subset(select =-c(INDEX)) %>%
  dplyr::select(
    all_of(c("TARGET")),
    sort(setdiff(names(.), c("TARGET")))
  ) %>%
  mutate(
    STARS = as.factor(STARS),
    AcidIndex = as.factor(AcidIndex),
    LabelAppeal = as.factor(LabelAppeal)
  )
 
glimpse(wine_training_df)


numeric_df <- wine_training_df %>%
  dplyr::select(where(is.numeric))  

numeric_cols <- sapply(wine_training_df, is.numeric)
factor_cols <- sapply(wine_training_df, is.factor)

table(wine_training_df$TARGET)
```

### Missing Values

Additionally, eight parameters had missing values ranging from 395 missing values (pH) to 3359 missing values (STARS). Below is the full list of variables with missing values:

```{r missing-values, echo=FALSE}
# Count missing values
missing <- colSums(is.na(wine_training_df))

missing[missing > 0]

```

### Examining Numerical Variables

A review of the summary statistics reveals issues with our data. In particular, nine of the 11 numeric variables show minimum values below zero.  Table 1 shows number of negative values.


```{r data-summaries, echo=FALSE}

summary(wine_training_df[numeric_cols])

```


When put in the context of our specific properties of wine they represent, these negative values appear to be erroneous. For example, we expected `Alcohol Content` to have a minimum value of zero instead of a negative value. The same can be said of the other parameters with negative values (Chlorides, Citric Acid, Fixed Acidity, Free Sulfur Dioxide, Residual Sugar, Sulphates, Total Sulfur Dioxie, and Volatile Acidity). This suggests possible data entry errors or normalization that shifted our actual values to the left. 

```{r negative-values, echo=FALSE}

wine_properties <- tibble::tibble(
  Variable = c(
    "TARGET",
    "Alcohol",
    "Chlorides",
    "CitricAcid",
    "Density",
    "FixedAcidity",
    "FreeSulfurDioxide",
    "ResidualSugar",
    "Sulphates",
    "TotalSulfurDioxide",
    "VolatileAcidity",
    "pH"
  ),
  CommonRange = c(
    "0 or higher",
    "8% – 15% ABV",
    "0.01 – 0.10 g/L",
    "0 – 1.0 g/L",
    "0.990 – 1.005 g/cm³",
    "4 – 9 g/L",
    "10 – 70 mg/L",
    "0 – 45 g/L",
    "0.3 – 1.0 g/L",
    "30 – 150 mg/L",
    "0.2 – 0.8 g/L",
    "2.9 – 4.0"
  )
)

below_zero_counts <-  numeric_df %>%
  summarise(across(everything(), ~ sum(. < 0, na.rm = TRUE)))  %>%
  pivot_longer(
    cols = everything(),      
    names_to = "Variable",    
    values_to = "Rows Below Zero"       
  )


summary_stats <- lapply(numeric_df, function(x) {
  if (is.numeric(x)) {
    list(
      Q25 = round(quantile(x, 0.25, na.rm = TRUE), 3),
      Median = round(median(x, na.rm = TRUE), 3),
      Q75 = round(quantile(x, 0.75, na.rm = TRUE), 3)
    )
  } else {
    list(Q25 = NA, Median = NA, Q75 = NA)
  }
})

tibble(
  Variable = names(numeric_df),
  Q25 = sapply(summary_stats, function(x) x$Q25),
  Median = sapply(summary_stats, function(x) x$Median),
  Q75 = sapply(summary_stats, function(x) x$Q75)
) %>%
  left_join(wine_properties, by = "Variable") %>%
  left_join(below_zero_counts, by = "Variable") %>%
  kable(caption="Number of negative values") 

```

A look at our boxplots shows the IQR's for each parameter are centered around a similar x-axis for each of our case counts. The boxplots confirm the presence of  extreme values at lower as well as on the upper ranges.  It should be noted the IQRs for the affected variables are in line with their corresponding typical ranges according to [VineEnology.com](https://www.vinoenology.com/wine-composition/) as shown in Table 1. 

```{r explore-boxplot-p2, fig.width=7, fig.height=9, echo=FALSE, message = FALSE, warning=FALSE}

numeric_df %>% 
  mutate(TARGET = as.factor(TARGET)) %>% 
  drop_na() %>%
  plot_boxplot(by = "TARGET", title="Boxplots of Target vs Param", ncol = 2)

```


### Examining Categorical Variables

Visualizing the distributions of out categorical variables helps ensure variables are treated as discrete categories, not continuous numbers. Our AcidIndex variable shows that majority of values fall between 6 and 11 with 342 observations collectively making up the remaining values. We may want to bin values for this parameter. Label Appeal has a normal distribution ranging from -2 to 2 and centered around 0. The  Wine rative variable STARS has the most missing values (3359) of any variable; of missing value, 61% of rows had 0 cases purchased. As would be expected, the majority of observations have a low STARS value (1/2), while few observations have a perfect value of 4.

```{r factor-charts, fig.width=7, fig.height=8, echo=FALSE}

# Show bar charts for factors

plots <- list()  
i <- 1

for (colname in names(wine_training_df)[factor_cols]) {
  
  if (colname != 'TARGET') {
  
  plot_data <- wine_training_df %>%
    group_by(TARGET, group_var = .data[[colname]]) %>%
    dplyr::summarise(count = n(), .groups = "drop") %>%
    group_by(group_var) %>%
    mutate(
      percent = 100 * count / sum(count),
      label = paste0(round(percent), "%")
    ) %>%
    mutate(TARGET = factor(TARGET, levels = 8:0)) 

factorColors <-  c(
    "0" = "gray",
    "1" = "pink",
    "2" = "cyan",
    "3" = "yellow",
    "4" = "orange",
    "5" = "purple",
    "6" = "green",
    "7" = "blue",
    "8" = "red"
    )

  # Plot   
 plots[[i]]  <- ggplot(plot_data, aes(x = group_var, y = count, fill = TARGET, label = label)) +
    geom_col(position = "stack") +
    geom_text(position = position_stack(0.5), size = 2) +
    labs(
      title = paste("Distribution of", colname, "by TARGET"),
      x = colname,
      y = "Count",
      fill = "# Cases" 
    ) +
   scale_fill_manual(values = factorColors) +
    theme_minimal() +
    theme(
      plot.title = element_blank(),
      axis.title.x = element_text(size =5),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
    
    i <- i + 1
  }
}

( wrap_plots(plots[1], ncol = 1, guides = "collect") /
wrap_plots(plots[2:3], ncol = 2, guides = "collect") ) & theme(legend.position = "bottom")


layout(1)
par(mfrow = c(1, 1))


summary(wine_training_df[factor_cols])
```




### Visualizing Distributions

Next we will visualize the distributions for our numeric variables. Using the histograms, we can quickly spot skewness, check distribution and value ranges, and identify variables with spikes or unusual spread. The histograms show symmetric unimodal distributions strongly peaked with thin tails across our numeric variables. However, as noted earlier, negative values for many of these parameters would be impossible.

```{r distributions1, fig.width=7, fig.height=7, echo=FALSE, message = FALSE, warning=FALSE}

plots <- list()  
i <- 1

for (colname in names(numeric_df)[numeric_cols]) {
  
  if (!is.na(colname) && colname != "TARGET") {
     plots[[i]]  <- ggplot(numeric_df, aes(x = .data[[colname]])) +
       geom_histogram(fill = "#ffbf00", bins = 40) 
     i <- i + 1
  }
}


wrap_plots(plots, ncol = 3, nrow = 4, guides = "collect")
```

\newpage

Our skewness test confirms that our numerical variables are nearly symmetrical or almost symmetrical.

```{r skewness, echo=FALSE}

# Calculate skewness for each numeric variable (using original values)
skew_vals <- sapply(numeric_df |> subset(select=-c(TARGET)), function(x) skewness(x, na.rm = TRUE, type = 3))

# Create a dataframe with skewness
skew_df <- data.frame(
  Variable = names(skew_vals),
  Skewness = skew_vals
)

# Sort by highest absolute skewness
skew_df <- skew_df[order(-abs(skew_df$Skewness)), ]

# Show top 10 most skewed variables (untransformed)
kable(skew_df, caption = "Skewness")

```

### Visualizing Relationships Among Variables

Correlation plots help us understand variable relationships and potential multicollinearity. A correlation plot for all variables shows moderate correlation between our dependent variable TARGET and the variables STARS and LabelAppeal and weak correlation between TARGET and AcidIndex. STARS, LabelAppeal, and AcidIndex are also three parameters that we identified to be ordinal.


```{r corrplot1, echo=FALSE}
layout(1)
par(mfrow = c(1, 1))
# Correlation matrix (exclude non-numeric or identifier variables)
cor_matrix <- cor( training_df %>%
  subset(select =-c(INDEX))  %>% select_if(is.numeric), use = "complete.obs")
corrplot(cor_matrix, method = "color", tl.cex = 0.8)

```

\newpage
The following correlation plot shows the correlation between only the numerical parameters. This view also groups the parameters into four clusters that appear to have a relationship with each other. 

```{r corrplot2, echo=FALSE }

par(mfrow = c(1, 1))
cor_matrix <- cor(numeric_df |> subset(select=-c(TARGET)), use = "pairwise.complete.obs", method = "pearson")

corrplot(cor_matrix, method="shade", order="hclust", addrect=4)

```

A Variance Inflation Factor (VIF) test confirms that no major multicollinearity present in between our variables.


```{r vif-scores, echo=FALSE}

# Fit linear model using all remaining features to predict TARGET
full_lm_model <- lm(TARGET ~ ., data = wine_training_df)

car::vif(full_lm_model)

```


\newpage
## Data Preparation

### Handling Negative Values

In the Data Exploration phase, we discovered that nine out of 11 numerical variables had negative values. While Poisson and Negative Binomial Regression will allow negative predictor values, we know that these values are impossible in the read world. Ignoring these values could lead to biased coefficient estimates that could introduce highly misleading relationships in our models. We will assume that these erronous values may be the result of data entry or normalization errors and attempt to address them.

 As we do not know what transformations may have been applied if normalization occurred, we will need to address the negative values through another method. From our earlier observations, we noted that nearly all of the affected parameters had thousands of affected records, with Chlorides having the most affects rows (3197). This means that nearly 1/4 of our 12,795 may be affected one way or another and we would loose too much data if we were to drop the affected records. We will instead only drop the 118 records with for Alcohol Content as it is a small fraction of the total rows in our  dataset. We will then set the remaining negative values to N/A, allowing the values to be imputed if desired. Imputing may introduce some bias into our results, but will retain much of our data; dropping these 118 rows may somewhat help reduce this bias. 

```{r handle-negative, echo=FALSE}

wine_training_flagged <- wine_training_df |>
  mutate(
    across(all_of(names(wine_training_df)[numeric_cols]),
      .fns = list(
        flag_neg = ~ ifelse(. < 0, 1, 0)
      ),
      .names = "{.col}_{.fn}"
    )
  ) |>
  mutate(
    STARS_na = ifelse(is.na(STARS), 1, 0)
  ) 

wine_training_non_neg <- wine_training_df |>
  filter(Alcohol > 0 | is.na(Alcohol))
  
for (colname in names(wine_training_non_neg)[numeric_cols]) {
  
  if (!is.na(colname) && colname != "TARGET") {
    wine_training_non_neg[[colname]] <- ifelse(wine_training_non_neg[[colname]] < 0, NA, wine_training_non_neg[[colname]])
  }
}




```


### Handling Missing Values

Given that we don't know if our data was previously transformed, we will apply median imputation to our variables with missing data. This method was preferred over multiple imputation or other techniques as it is easier to understand how we have altered the data and simpler to . Additionally, Multiple-imputation may magnify additional bias that may have been introduced if the data was previously transformed. We won't impute values for STARS since this appears to be an indication that a rating was not given to these observations. When exploring our data, we noticed that 61% of the wines without a STAR rating had zero (0) cases purchased. As such, a NA value may represent useful data for our model. We will convert this variable to -1 for interpretability by our model.

```{r impute-missing, echo=FALSE}

wine_training_imputed <- wine_training_non_neg |>
  mutate(
    STARS = as.factor(ifelse(is.na(as.numeric(STARS)), -1, as.numeric(STARS)))
  )

missing <- colSums(is.na(wine_training_imputed))

# Impute missing values in those columns using median
for (col in names(wine_training_imputed[missing > 0])) {
  median_val <- median(as.numeric(wine_training_imputed[[col]]), na.rm = TRUE)
  wine_training_imputed[[col]][is.na(wine_training_imputed[[col]])] <- median_val
}



kable(
  data.frame(
    'Original Missing Count' = colSums(is.na(wine_training_df)),
    'New Missing Count' = colSums(is.na(wine_training_non_neg)),
    'After Imputation' = colSums(is.na(wine_training_imputed))
  ), caption="Number of missing values"
)

```


### Transformations

Dropping our negative values and imputing our missing values introduced skewness into most of the numerical variables. All variables but Alcohol, Density and pH have are right—skewed  --though the right tail for FixedAcidity and TotalSulfurDioxide are relatively moderate compared the heavy right tails of the other variables. We can apply log transformation to these variables to help address skewness.


```{r distributions2, fig.width=7, fig.height=5, echo=FALSE, message = FALSE, warning=FALSE}

plots <- list()  
i <- 1

for (colname in names(wine_training_imputed)[numeric_cols]) {
  
   if (colname == "pH" | colname == "Alcohol"  | colname == "Density" ) {
     plots[[i]]  <- ggplot(wine_training_imputed, aes(x = .data[[colname]])) +
       geom_histogram(fill = "#bbbf00", bins = 40) 
     i <- i + 1
  }
}

for (colname in names(wine_training_imputed)[numeric_cols]) {
  
   if (is.numeric(wine_training_imputed[[colname]]) && !is.na(colname) && colname != "TARGET" && colname != "Alcohol"  && colname != "Density"  && colname != "pH") {
     plots[[i]]  <- ggplot(wine_training_imputed, aes(x = .data[[colname]])) +
       geom_histogram(fill = "#ffbf00", bins = 40) 
     i <- i + 1
  }
}


wrap_plots(plots, ncol = 3, nrow = 4, guides = "collect") +
  plot_annotation(
    title = "Numerical Variables - Before Transformation",
    theme = theme(plot.title = element_text(size = 10, hjust = 0.5))
  )
```

---


```{r log-transformations, echo=FALSE}

wine_training_clean <- wine_training_imputed |>
  mutate(
    Chlorides = log1p(Chlorides),
    CitricAcid = log1p(CitricAcid),
    FixedAcidity = log1p(FixedAcidity),
    FreeSulfurDioxide = log1p(FreeSulfurDioxide),
    ResidualSugar = log1p(ResidualSugar),
    Sulphates = log1p(Sulphates),
    TotalSulfurDioxide = log1p(TotalSulfurDioxide),
    VolatileAcidity = log1p(VolatileAcidity)
  )

```


```{r distributions3, fig.width=7, fig.height=4, echo=FALSE, message = FALSE, warning=FALSE}

plots <- list()  
i <- 1

for (colname in names(wine_training_clean)[numeric_cols]) {
  
  if (is.numeric(wine_training_clean[[colname]]) && !is.na(colname) && colname != "TARGET" && colname != "Alcohol"  && colname != "Density"  && colname != "pH") {
     plots[[i]]  <- ggplot(wine_training_clean, aes(x = .data[[colname]])) +
       geom_histogram(fill = "#ffb999", bins = 40) 
     i <- i + 1
  }
}

wrap_plots(plots, ncol = 3, nrow = 3, guides = "collect") +
  plot_annotation(
    title = "Numerical Variables - Post Transformation",
    theme = theme(plot.title = element_text(size = 10, hjust = 0.5))
  )
```


\newpage 
### Binned Transformation 

To simplify the effects of AcidIndex, this variable was transformed into categorical bins. This can help reduce the influence of extreme values and better capture non-linear effects in logistic regression.


```{r binning, fig.width=4, fig.height=3, echo=FALSE}
# Create bins 

wine_training_df$AcidIdx <- cut(as.numeric(wine_training_df$AcidIndex),
                  breaks = c(1, 3, 4, 5, 6, Inf),  
                  labels = c("1-6", "7","8","9", "10+"),
                  include.lowest = TRUE, 
                  right = TRUE)

wine_training_non_neg$AcidIdx <- cut(as.numeric(wine_training_non_neg$AcidIndex),
                  breaks = c(1, 3, 4, 5, 6, Inf),  
                  labels = c("1-6", "7","8","9", "10+"),
                  include.lowest = TRUE, 
                  right = TRUE)

wine_training_imputed$AcidIdx <- cut(as.numeric(wine_training_imputed$AcidIndex),
                  breaks = c(1, 3, 4, 5, 6, Inf),  
                  labels = c("1-6", "7","8","9", "10+"),
                  include.lowest = TRUE, 
                  right = TRUE)

wine_training_clean$AcidIdx <- cut(as.numeric(wine_training_clean$AcidIndex),
                  breaks = c(1, 3, 4, 5, 6, Inf),  
                  labels = c("1-6", "7","8","9", "10+"),
                  include.lowest = TRUE, 
                  right = TRUE)

ggplot(wine_training_clean, aes(x=AcidIdx, fill=factor(TARGET, levels = 8:0))) +
  geom_bar() +
   scale_fill_manual(values = factorColors) +
    theme_minimal() +
    labs(
      fill = "# Cases" 
    )
```


### Outliers

Diagnostic plots show several points with relatively high Cook's distance values when compared to rest of our data. 

```{r base_model-diagnostic-plots, fig.width=6, fig.height=3, echo=FALSE}
layout(matrix(1:2, ncol=2, nrow = 2, byrow = TRUE))
full_lm_model <- lm(TARGET ~ ., data = wine_training_clean)
plot(full_lm_model, which = c(4, 6),  id.n = 5)
par(mfrow = c(1, 1))


```


## Model Building




## Model Selection

